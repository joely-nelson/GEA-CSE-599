03/11/2022 01:54:52 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
03/11/2022 01:54:52 - INFO - transformers.configuration_utils -   loading configuration file /home/mexposit/cg/gea/dnabert/model/pretrained/6-new-12w-0/config.json
03/11/2022 01:54:52 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "do_sample": false,
  "eos_token_ids": 0,
  "finetuning_task": "dnaprom",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "num_return_sequences": 1,
  "num_rnn_layer": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "rnn": "lstm",
  "rnn_dropout": 0.0,
  "rnn_hidden": 768,
  "split": 10,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 4101
}

03/11/2022 01:54:52 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/vocab.txt from cache at /home/mexposit/.cache/torch/transformers/ea1474aad40c1c8ed4e1cb7c11345ddda6df27a857fb29e1d4c901d9b900d32d.26f8bd5a32e49c2a8271a46950754a4a767726709b7741c68723bc1db840a87e
03/11/2022 01:54:52 - INFO - transformers.modeling_utils -   loading weights file /home/mexposit/cg/gea/dnabert/model/pretrained/6-new-12w-0/pytorch_model.bin
03/11/2022 01:54:55 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
03/11/2022 01:54:55 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
03/11/2022 01:54:55 - INFO - __main__ -   finish loading model
03/11/2022 01:54:55 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, attention_probs_dropout_prob=0.1, beta1=0.9, beta2=0.999, cache_dir='', config_name='', data_dir='/home/mexposit/cg/gea/transformers/2_geainit/in_data', device=device(type='cpu'), do_ensemble_pred=False, do_eval=True, do_lower_case=False, do_predict=False, do_train=True, do_visualize=False, early_stop=0, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, hidden_dropout_prob=0.1, learning_rate=0.0002, local_rank=-1, logging_steps=10, max_grad_norm=1.0, max_seq_length=100, max_steps=-1, model_name_or_path='/home/mexposit/cg/gea/dnabert/model/pretrained/6-new-12w-0', model_type='dna', n_gpu=0, n_process=8, no_cuda=False, num_rnn_layer=2, num_train_epochs=5.0, output_dir='/home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=32, per_gpu_pred_batch_size=8, per_gpu_train_batch_size=32, predict_dir=None, predict_scan_size=1, result_dir=None, rnn='lstm', rnn_dropout=0.0, rnn_hidden=768, save_steps=4000, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, task_name='dnaprom', tokenizer_name='dna6', visualize_data_dir=None, visualize_models=None, visualize_train=False, warmup_percent=0.1, warmup_steps=0, weight_decay=0.01)
03/11/2022 01:54:55 - INFO - __main__ -   Creating features from dataset file at /home/mexposit/cg/gea/transformers/2_geainit/in_data
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   LOOKING AT /home/mexposit/cg/gea/transformers/2_geainit/in_data/train.tsv
============================================================
<class 'transformers.tokenization_dna.DNATokenizer'>
finish loading examples
number of processes for converting feature: 8
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   Writing example 0/549
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-1
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 258 1020 4066 3962 3548 1891 3454 1515 1950 3691 2461 1637 2438 1548 2081 118 458 1818 3163 349 1382 1418 1561 2133 326 1292 1060 130 506 2011 3933 3429 1414 1546 2074 90 346 1369 1365 1350 1290 1050 91 350 1386 1434 1626 2395 1374 1386 1435 1630 2412 1444 1666 2554 2010 3929 3413 1349 1285 1030 10 27 93 357 1416 1554 2106 218 857 3413 1349 1288 1042 60 225 885 3525 1800 3090 60 226 889 3541 1862 3337 1046 76 291 1149 487 1935 3631 2221 678 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-2
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 4016 3761 2744 2771 2878 3306 921 3669 2376 1297 1079 206 812 3236 643 2559 2032 4018 3771 2784 2930 3514 1754 2906 3417 1367 1357 1317 1159 528 2098 187 736 2930 3516 1761 2935 3534 1836 3236 644 2561 2037 4037 3845 3079 13 38 139 543 2157 422 1676 2595 2174 489 1944 3667 2368 1266 954 3801 2901 3399 1296 1075 192 753 3000 3793 2872 3282 825 3288 852 3396 1281 1013 4039 3854 3116 163 639 2541 1960 3732 2627 2301 998 3979 3613 2149 389 1542 2057 21 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-3
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 1435 1630 2409 1432 1618 2364 1250 889 3544 1875 3391 1264 946 3769 2776 2898 3386 1241 856 3412 1347 1279 1005 4007 3727 2605 2215 654 2602 2203 605 2405 1416 1553 2101 199 782 3115 158 620 2466 1657 2520 1875 3389 1255 911 3632 2227 703 2798 2985 3735 2637 2342 1161 535 2127 302 1195 672 2675 2494 1771 2974 3692 2467 1662 2537 1941 3654 2315 1055 110 428 1698 2682 2521 1879 3407 1325 1192 658 2620 2276 899 3582 2028 4003 3710 2540 1955 3711 2541 1960 3730 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-4
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 245 967 3853 3109 133 520 2067 62 235 927 3694 2473 1686 2634 2331 1119 367 1453 1703 2702 2604 2211 638 2539 1950 3690 2458 1626 2394 1370 1371 1374 1387 1438 1643 2462 1644 2467 1661 2535 1935 3629 2215 654 2603 2206 618 2459 1630 2411 1438 1642 2458 1628 2403 1407 1518 1962 3740 2660 2434 1532 2020 3972 3586 2044 4067 3966 3561 1943 3662 2347 1183 622 2473 1685 2630 2316 1060 130 506 2011 3933 3429 1414 1546 2074 90 346 1369 1367 1358 1321 1175 590 2346 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-5
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 2298 987 3933 3430 1419 1565 2150 396 1571 2175 496 1970 3770 2778 2908 3426 1404 1505 1910 3532 1828 3203 510 2026 3995 3679 2413 1446 1676 2594 2171 480 1908 3523 1789 3048 3985 3637 2246 780 3107 126 490 1945 3669 2374 1292 1057 117 454 1802 3097 87 333 1317 1159 525 2088 146 569 2263 846 3372 1187 640 2545 1974 3788 2849 3192 466 1852 3300 899 3581 2024 3988 3652 2307 1024 4084 4036 3844 3075 4096 4082 4025 3797 2885 3336 1041 54 203 798 3180 420 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   Writing example 0/549
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-550
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 3701 2504 1811 3134 236 929 3701 2504 1812 3137 247 976 3892 3267 768 3060 4035 3839 3053 4007 3726 2601 2199 592 2355 1215 752 2995 3775 2800 2993 3768 2772 2882 3323 989 3941 3464 1553 2103 207 813 3239 655 2606 2217 663 2637 2341 1160 532 2115 255 1005 4005 3720 2577 2101 200 787 3135 239 944 3762 2748 2787 2941 3560 1939 3646 2284 931 3711 2543 1968 3764 2755 2816 3059 4031 3822 2985 3735 2637 2342 1163 544 2162 443 1760 2929 3511 1741 2854 3211 541 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-551
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 3387 1248 884 3524 1796 3073 4087 4045 3879 3213 552 2194 572 2276 900 3588 2051 4095 4080 4020 3778 2810 3036 3940 3459 1535 2030 4012 3748 2690 2555 2015 3949 3495 1680 2609 2230 716 2849 3189 456 1812 3137 248 980 3906 3324 995 3967 3567 1966 3755 2719 2669 2472 1684 2628 2308 1028 4099 4096 4084 4036 3843 3072 4083 4029 3816 2961 3639 2255 813 3239 653 2599 2189 552 2196 578 2300 993 3957 3528 1811 3133 232 914 3644 2275 896 3572 1988 3842 3068 4065 3958 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-552
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 2597 2183 528 2097 184 724 2881 3319 974 3881 3223 589 2343 1167 557 2214 651 2592 2162 444 1764 2945 3573 1991 3853 3112 146 569 2263 848 3377 1205 711 2832 3123 192 755 3007 3824 2993 3768 2772 2884 3331 1023 4080 4019 3775 2797 2983 3726 2603 2207 621 2471 1679 2608 2228 707 2816 3060 4035 3837 3046 3980 3620 2177 503 2000 3889 3256 723 2878 3308 930 3705 2519 1869 3365 1160 530 2107 222 875 3488 1649 2488 1747 2878 3305 918 3660 2340 1153 502 1996 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-553
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 255 1005 4005 3720 2577 2101 200 787 3135 239 944 3762 2748 2787 2941 3560 1939 3646 2284 931 3711 2543 1968 3764 2755 2816 3059 4031 3822 2985 3735 2637 2341 1159 528 2098 187 733 2917 3463 1549 2086 139 541 2149 392 1554 2106 220 868 3457 1527 1997 3878 3211 541 2151 399 1582 2219 671 2671 2477 1703 2701 2597 2183 528 2097 184 724 2881 3319 974 3881 3223 589 2343 1167 557 2214 651 2592 2162 444 1764 2945 3573 1991 3853 3112 146 569 2263 848 3377 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-554
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 3324 993 3958 3532 1826 3195 479 1901 3493 1671 2574 2090 156 609 2422 1484 1826 3194 476 1889 3447 1488 1842 3258 732 2914 3449 1496 1876 3395 1280 1011 4031 3824 2996 3780 2819 3069 4072 3987 3646 2284 931 3709 2535 1936 3636 2244 771 3070 4074 3995 3678 2410 1436 1636 2435 1535 2030 4010 3740 2658 2425 1496 1876 3394 1276 996 3970 3579 2014 3946 3484 1633 2423 1487 1838 3243 669 2664 2451 1600 2290 955 3808 2930 3513 1752 2898 3388 1252 899 3583 2032 4019 3775 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   Writing example 0/549
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-1099
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 2744 2770 2875 3296 883 3518 1772 2979 3709 2534 1932 3617 2167 463 1838 3243 669 2662 2444 1570 2172 483 1919 3565 1960 3730 2620 2275 894 3564 1956 3715 2560 2036 4033 3829 3016 3860 3137 247 975 3887 3246 684 2721 2680 2516 1857 3320 979 3904 3316 963 3839 3055 4013 3751 2702 2602 2202 602 2396 1377 1400 1490 1849 3287 847 3374 1196 675 2685 2536 1940 3651 2303 1006 4010 3739 2655 2414 1452 1700 2689 2552 2004 3905 3319 974 3881 3223 590 2346 1179 605 2407 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-1100
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 56 209 823 3279 814 3241 663 2640 2354 1211 736 2929 3512 1747 2877 3304 915 3645 2279 912 3633 2232 724 2882 3324 996 3971 3582 2028 4002 3708 2532 1923 3583 2029 4008 3729 2614 2249 791 3150 300 1187 640 2545 1975 3791 2862 3243 671 2671 2478 1705 2712 2643 2365 1253 901 3591 2062 44 164 644 2564 2051 4093 4071 3981 3624 2193 566 2252 801 3192 467 1856 3316 963 3839 3056 4019 3776 2801 2999 3790 2859 3230 617 2456 1617 2358 1227 797 3174 393 1557 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   Writing example 0/549
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-1101
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 642 2555 2013 3943 3469 1576 2193 566 2252 802 3196 484 1921 3576 2002 3897 3287 845 3367 1165 550 2186 540 2145 375 1488 1844 3267 767 3053 4006 3724 2596 2178 508 2020 3971 3581 2024 3986 3643 2272 881 3511 1742 2860 3235 637 2536 1937 3637 2246 778 3099 96 369 1461 1736 2835 3134 234 924 3681 2424 1491 1854 3307 928 3697 2488 1745 2870 3275 798 3180 417 1656 2514 1851 3295 880 3508 1730 2809 3032 3923 3392 1267 958 3817 2968 3667 2368 1268 961 3830 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-1648
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 3209 536 2131 320 1266 954 3804 2916 3459 1534 2025 3991 3663 2351 1200 690 2748 2785 2934 3529 1814 3146 284 1123 382 1516 1953 3701 2504 1809 3128 211 830 3306 924 3684 2435 1536 2036 4035 3840 3057 4021 3782 2828 3108 132 515 2046 4076 4001 3703 2511 1840 3251 702 2794 2971 3679 2414 1451 1696 2674 2492 1763 2942 3562 1946 3673 2391 1360 1332 1218 761 3030 3915 3360 1139 447 1776 2995 3774 2795 2975 3695 2480 1713 2742 2762 2843 3168 371 1469 1768 2963 3648 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-1649
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 576 2292 963 3838 3050 3996 3684 2434 1529 2008 3921 3381 1221 776 3090 60 225 885 3528 1809 3127 207 814 3244 674 2684 2529 1912 3537 1845 3270 778 3099 94 364 1443 1661 2536 1938 3643 2272 881 3511 1744 2868 3266 761 3031 3919 3376 1203 704 2804 3012 3843 3071 4079 4016 3764 2756 2817 3062 4043 3871 3181 423 1679 2608 2228 705 2806 3019 3870 3177 408 1617 2358 1225 789 3143 270 1068 161 630 2507 1821 3174 393 1557 2118 267 1053 104 403 1599 2285 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-1102
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 483 1917 3559 1934 3626 2204 609 2421 1480 1810 3132 228 898 3580 2020 3970 3578 2012 3938 3450 1499 1885 3431 1424 1588 2242 764 3043 3967 3567 1966 3755 2719 2669 2470 1676 2594 2169 471 1869 3368 1171 574 2282 923 3677 2406 1420 1570 2172 483 1917 3558 1932 3618 2170 475 1886 3435 1439 1646 2474 1689 2645 2374 1291 1053 104 403 1598 2283 928 3699 2494 1771 2973 3687 2447 1581 2214 652 2596 2178 508 2020 3971 3582 2025 3991 3663 2352 1204 706 2809 3032 3923 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-1650
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 579 2302 1002 3995 3678 2411 1439 1646 2476 1700 2689 2549 1991 3854 3116 161 629 2502 1804 3107 126 492 1954 3706 2523 1885 3429 1415 1549 2086 137 533 2118 267 1055 111 431 1711 2735 2733 2728 2706 2620 2276 899 3582 2026 3995 3678 2411 1437 1640 2451 1597 2280 913 3638 2252 801 3189 456 1810 3131 221 872 3475 1597 2280 913 3637 2248 787 3135 239 941 3750 2699 2589 2151 398 1580 2211 639 2543 1965 3749 2694 2572 2082 124 481 1911 3535 1840 3252 705 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-1103
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 847 3376 1201 696 2771 2879 3310 940 3748 2689 2549 1991 3855 3119 175 686 2730 2715 2655 2416 1459 1726 2795 2975 3693 2469 1672 2579 2110 234 924 3683 2429 1512 1939 3645 2280 915 3645 2277 902 3595 2079 110 428 1700 2692 2562 2044 4068 3970 3580 2018 3964 3556 1921 3575 1999 3885 3240 657 2614 2251 799 3181 423 1677 2598 2187 541 2149 389 1543 2063 48 180 708 2820 3076 4099 4094 4073 3989 3656 2324 1090 250 987 3934 3435 1438 1641 2455 1615 2350 1195 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-1651
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 239 941 3752 2705 2615 2255 814 3244 673 2680 2514 1850 3289 855 3407 1327 1199 686 2730 2714 2651 2399 1389 1445 1670 2572 2082 124 481 1910 3531 1822 3179 415 1646 2475 1693 2662 2441 1559 2126 299 1183 621 2470 1675 2590 2155 415 1647 2479 1711 2734 2731 2718 2668 2466 1658 2523 1887 3437 1446 1674 2587 2141 359 1423 1582 2220 675 2685 2533 1927 3598 2091 157 614 2441 1558 2123 287 1134 428 1700 2692 2564 2051 4094 4074 3995 3678 2409 1432 1620 2372 1282 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   Writing example 0/549
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-2197
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 206 809 3223 589 2341 1160 529 2104 210 825 3286 842 3356 1123 381 1510 1929 3605 2120 273 1079 206 809 3223 589 2342 1162 537 2133 328 1299 1086 234 924 3683 2429 1512 1939 3646 2283 927 3693 2472 1683 2622 2282 922 3674 2396 1378 1402 1499 1887 3439 1454 1706 2714 2649 2392 1362 1340 1249 888 3540 1860 3330 1018 4057 3925 3398 1290 1052 99 384 1523 1984 3827 3006 3818 2972 3684 2435 1536 2034 4025 3797 2886 3339 1053 102 396 1572 2178 507 2013 3942 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-2198
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 3468 1569 2168 465 1846 3274 793 3158 331 1309 1125 389 1541 2053 8 20 65 246 971 3870 3178 411 1629 2407 1423 1582 2217 664 2641 2358 1227 799 3182 426 1690 2650 2393 1365 1349 1286 1034 25 85 325 1285 1029 6 12 33 117 456 1810 3130 218 858 3417 1365 1349 1286 1035 29 101 390 1547 2078 105 405 1605 2312 1042 57 214 841 3350 1097 278 1100 289 1144 466 1849 3285 837 3335 1038 42 156 612 2434 1531 2014 3948 3489 1655 2509 1832 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-2199
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 1409 1528 2004 3907 3327 1006 4009 3736 2644 2372 1281 1015 4048 3890 3257 727 2895 3375 1197 677 2694 2570 2075 96 371 1471 1775 2990 3753 2710 2633 2328 1106 316 1249 888 3538 1851 3296 882 3513 1750 2890 3353 1111 336 1331 1216 755 3008 3827 3006 3819 2973 3687 2446 1580 2212 643 2559 2032 4018 3771 2784 2930 3514 1754 2906 3417 1367 1357 1317 1159 528 2098 187 736 2930 3516 1761 2935 3534 1836 3236 644 2561 2037 4037 3845 3079 15 47 174 684 2724 2691 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-2200
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 815 3246 682 2714 2650 2393 1368 1362 1339 1245 872 3474 1596 2274 892 3556 1921 3573 1989 3845 3078 11 30 107 414 1641 2456 1619 2365 1256 914 3644 2276 899 3584 2035 4031 3823 2992 3761 2741 2759 2829 3112 148 580 2305 1015 4046 3882 3228 609 2421 1477 1800 3091 64 241 949 3781 2824 3092 68 257 1013 4037 3847 3087 45 168 657 2616 2260 833 3320 979 3902 3307 926 3691 2462 1643 2464 1649 2487 1744 2867 3261 744 2964 3649 2295 974 3883 3232 628 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-2201
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 156 609 2421 1477 1800 3090 57 214 842 3354 1115 352 1393 1462 1738 2842 3163 350 1386 1436 1636 2435 1534 2026 3994 3673 2390 1353 1302 1097 278 1099 286 1130 412 1634 2428 1508 1921 3573 1989 3848 3092 65 247 976 3889 3253 709 2823 3085 39 143 560 2227 701 2789 2949 3592 2065 55 205 805 3207 525 2087 143 558 2217 663 2637 2341 1158 524 2084 130 506 2010 3930 3417 1368 1361 1336 1235 830 3305 920 3665 2357 1221 774 3081 24 83 317 1253 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   Writing example 0/549
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-2746
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 1137 440 1747 2878 3305 920 3668 2371 1277 999 3982 3626 2201 597 2373 1286 1033 23 77 293 1158 521 2070 75 286 1131 414 1644 2467 1661 2533 1926 3596 2083 128 500 1987 3837 3045 3974 3594 2075 93 360 1426 1596 2276 898 3578 2011 3936 3442 1467 1759 2925 3493 1670 2571 2079 109 422 1676 2594 2171 477 1896 3473 1591 2255 815 3248 690 2747 2782 2924 3490 1658 2524 1891 3455 1518 1962 3739 2655 2414 1449 1685 2630 2313 1045 72 276 1091 253 999 3984 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-1652
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 3198 489 1942 3658 2332 1123 382 1514 1946 3673 2390 1354 1306 1116 354 1401 1493 1863 3343 1069 166 650 2585 2134 329 1301 1096 275 1086 236 931 3709 2533 1926 3593 2069 69 263 1037 37 136 530 2106 217 853 3399 1293 1061 135 525 2085 135 525 2085 134 522 2076 99 381 1510 1930 3611 2141 358 1418 1562 2138 345 1366 1356 1314 1146 474 1883 3421 1384 1428 1602 2298 987 3933 3432 1428 1604 2308 1028 4097 4088 4052 3906 3324 994 3964 3556 1924 3585 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   Writing example 0/549
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-3295
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 3546 1884 3427 1406 1516 1953 3703 2512 1843 3261 741 2951 3599 2095 175 687 2733 2727 2702 2604 2212 642 2554 2012 3940 3460 1540 2051 4093 4070 3978 3612 2147 383 1517 1959 3727 2605 2215 655 2606 2220 674 2683 2525 1896 3475 1598 2283 927 3694 2474 1690 2651 2399 1392 1460 1732 2817 3063 4046 3882 3226 603 2400 1395 1470 1770 2970 3675 2399 1391 1455 1711 2734 2731 2719 2671 2478 1705 2710 2634 2332 1123 383 1517 1959 3728 2612 2243 768 3060 4033 3829 3015 3854 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-3296
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 3056 4019 3774 2796 2977 3702 2507 1822 3178 411 1629 2408 1425 1591 2255 814 3244 676 2689 2552 2004 3905 3320 980 3905 3320 977 3894 3273 790 3148 289 1144 468 1860 3329 1015 4045 3877 3206 522 2076 100 385 1528 2001 3893 3272 786 3132 225 885 3526 1802 3097 86 329 1302 1097 277 1093 262 1033 22 73 277 1093 264 1042 57 216 850 3385 1237 837 3333 1029 6 10 28 97 373 1479 1807 3117 166 650 2585 2136 340 1345 1272 978 3897 3288 851 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-2747
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 2839 3149 295 1168 564 2243 768 3060 4035 3840 3059 4032 3825 3000 3796 2883 3327 1005 4008 3732 2627 2301 999 3983 3629 2216 657 2616 2259 829 3304 916 3651 2303 1008 4020 3779 2815 3053 4008 3731 2622 2282 924 3681 2424 1489 1847 3278 809 3223 591 2351 1199 687 2736 2738 2747 2783 2928 3505 1718 2762 2843 3166 363 1440 1652 2498 1788 3044 3971 3583 2032 4019 3776 2803 3006 3819 2976 3699 2493 1768 2964 3651 2303 1007 4015 3760 2739 2751 2798 2987 3744 2675 2495 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   Writing example 0/549
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-2748
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 2092 162 634 2524 1892 3460 1537 2037 4040 3860 3140 259 1024 4081 4022 3787 2848 3188 450 1788 3043 3968 3572 1988 3843 3071 4078 4011 3742 2666 2459 1632 2419 1470 1769 2966 3658 2329 1111 336 1331 1215 749 2984 3731 2622 2284 932 3715 2560 2033 4021 3781 2824 3092 68 260 1028 4097 4086 4044 3874 3196 483 1918 3564 1955 3709 2533 1928 3604 2115 256 1009 4022 3786 2841 3157 328 1298 1082 220 868 3460 1538 2041 4053 3911 3344 1075 191 749 2984 3732 2628 2306 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-3844
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 4010 3738 2650 2396 1379 1406 1516 1956 3715 2559 2030 4010 3738 2650 2396 1379 1406 1515 1949 3687 2445 1574 2188 546 2170 475 1886 3434 1434 1627 2399 1390 1452 1699 2688 2546 1978 3801 2902 3403 1311 1135 431 1710 2732 2721 2678 2506 1819 3166 364 1442 1660 2532 1921 3574 1993 3861 3143 271 1072 178 697 2774 2890 3353 1111 335 1328 1203 703 2798 2986 3738 2652 2401 1400 1490 1852 3297 888 3539 1854 3308 929 3702 2505 1815 3151 304 1203 702 2795 2976 3699 2495 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-3845
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 3101 103 397 1575 2192 561 2231 719 2862 3244 676 2689 2550 1996 3876 3201 504 2002 3900 3300 900 3585 2039 4045 3880 3217 568 2257 821 3269 774 3082 25 85 327 1293 1061 134 522 2073 87 333 1319 1165 549 2184 531 2110 234 921 3669 2374 1289 1047 77 295 1166 555 2207 622 2474 1689 2645 2374 1290 1052 97 373 1480 1809 3125 198 779 3104 115 445 1765 2949 3589 2055 15 45 168 659 2621 2277 904 3601 2101 197 773 3080 17 53 198 780 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-2749
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 3750 2697 2583 2126 297 1174 585 2326 1100 290 1146 474 1882 3417 1365 1350 1290 1050 89 342 1353 1302 1098 282 1114 346 1370 1371 1374 1386 1434 1627 2399 1391 1455 1711 2734 2732 2724 2691 2559 2030 4010 3737 2645 2375 1295 1072 177 693 2758 2826 3098 90 346 1370 1371 1375 1391 1453 1702 2699 2592 2163 448 1777 2998 3787 2846 3177 405 1606 2314 1051 94 363 1439 1647 2479 1711 2736 2739 2750 2794 2969 3669 2374 1289 1047 78 300 1185 631 2512 1843 3262 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-3846
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 3001 3797 2886 3339 1054 108 419 1662 2540 1955 3710 2538 1948 3683 2429 1509 1925 3591 2061 37 133 517 2053 5 5 7 15 45 167 655 2608 2227 702 2793 2967 3663 2349 1192 659 2624 2292 962 3836 3044 3970 3578 2010 3932 3426 1402 1498 1884 3427 1407 1520 1972 3777 2806 3019 3869 3173 392 1553 2104 211 830 3305 919 3663 2349 1189 647 2574 2091 158 618 2458 1626 2394 1371 1375 1392 1457 1717 2760 2836 3138 249 981 3911 3342 1068 164 643 2558 2026 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-3297
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 2115 256 1009 4024 3794 2874 3289 855 3405 1318 1164 545 2166 459 1823 3183 431 1711 2733 2726 2700 2594 2170 476 1890 3452 1507 1917 3557 1925 3589 2053 5 8 19 64 244 962 3834 3033 3928 3411 1342 1259 927 3694 2474 1691 2656 2420 1474 1787 3039 3950 3499 1695 2672 2481 1718 2763 2848 3186 442 1756 2914 3451 1501 1896 3473 1589 2248 786 3129 213 840 3346 1082 220 868 3459 1535 2032 4019 3773 2792 2962 3644 2274 890 3545 1878 3403 1309 1127 398 1579 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-3847
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 4065 3960 3538 1851 3293 871 3471 1583 2221 679 2701 2599 2189 549 2181 520 2068 65 245 965 3845 3080 20 68 259 1023 4078 4010 3738 2651 2399 1392 1458 1723 2783 2926 3499 1693 2664 2451 1599 2288 946 3771 2784 2931 3518 1770 2971 3677 2406 1420 1570 2172 481 1911 3534 1835 3231 621 2471 1680 2612 2241 760 3026 3897 3287 847 3376 1204 708 2819 3072 4083 4031 3824 2994 3771 2783 2925 3496 1684 2627 2301 999 3983 3630 2219 672 2673 2486 1738 2841 3160 338 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-3848
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 824 3281 823 3279 815 3245 680 2706 2617 2263 845 3365 1160 531 2109 229 901 3589 2053 8 19 61 232 913 3638 2251 798 3178 412 1634 2427 1502 1898 3483 1632 2418 1466 1756 2916 3460 1537 2040 4050 3900 3297 885 3526 1802 3097 88 339 1343 1263 942 3754 2715 2655 2413 1448 1682 2619 2271 879 3503 1711 2735 2734 2730 2714 2650 2395 1374 1386 1434 1626 2393 1365 1349 1285 1029 8 18 60 228 899 3582 2025 3989 3656 2321 1078 203 798 3177 407 1613 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-3298
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 496 1971 3773 2789 2949 3589 2053 5 8 20 68 257 1013 4038 3849 3093 72 276 1092 259 1024 4081 4023 3789 2855 3216 564 2241 757 3013 3846 3084 34 122 476 1889 3445 1478 1801 3095 78 299 1181 614 2441 1559 2126 299 1182 618 2459 1631 2414 1450 1690 2650 2394 1371 1373 1381 1414 1545 2070 74 281 1110 330 1308 1121 373 1480 1811 3133 230 906 3610 2137 342 1355 1309 1128 404 1604 2306 1018 4057 3926 3402 1308 1122 379 1502 1899 3485 1638 2444 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-3299
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 1115 350 1385 1432 1619 2366 1259 926 3689 2453 1605 2309 1031 15 48 180 706 2810 3033 3927 3408 1330 1210 732 2914 3449 1496 1874 3386 1243 861 3431 1424 1587 2240 756 3010 3836 3042 3962 3546 1883 3424 1394 1467 1759 2926 3498 1690 2651 2399 1389 1447 1677 2597 2184 529 2102 201 790 3145 278 1097 277 1093 264 1043 63 237 933 3720 2577 2101 197 774 3083 32 113 437 1733 2822 3081 23 78 298 1178 603 2397 1381 1416 1554 2106 217 855 3408 1332 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   guid: train-2750
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   input_ids: 2 1146 473 1877 3397 1287 1039 45 165 646 2570 2075 95 365 1447 1677 2597 2181 519 2062 42 156 611 2431 1519 1965 3750 2698 2586 2137 342 1355 1310 1129 405 1606 2314 1051 95 365 1445 1670 2569 2069 70 266 1051 94 362 1436 1634 2426 1499 1885 3430 1418 1563 2142 362 1434 1626 2395 1374 1386 1436 1635 2430 1516 1956 3714 2554 2010 3930 3420 1379 1408 1521 1974 3786 2843 3166 362 1435 1629 2405 1414 1546 2073 85 328 1300 1089 248 978 3900 3298 889 3 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:54:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:54:58 - INFO - __main__ -   Saving features into cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_train_6-new-12w-0_100_dnaprom
03/11/2022 01:54:59 - INFO - __main__ -   ***** Running training *****
03/11/2022 01:54:59 - INFO - __main__ -     Num examples = 4392
03/11/2022 01:54:59 - INFO - __main__ -     Num Epochs = 5
03/11/2022 01:54:59 - INFO - __main__ -     Instantaneous batch size per GPU = 32
03/11/2022 01:54:59 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32
03/11/2022 01:54:59 - INFO - __main__ -     Gradient Accumulation steps = 1
03/11/2022 01:54:59 - INFO - __main__ -     Total optimization steps = 690
03/11/2022 01:54:59 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step
03/11/2022 01:54:59 - INFO - __main__ -     Continuing training from epoch 0
03/11/2022 01:54:59 - INFO - __main__ -     Continuing training from global step 0
03/11/2022 01:54:59 - INFO - __main__ -     Will skip the first 0 steps in the first epoch
1 processor started !
2 processor started !
3 processor started !
4 processor started !
5 processor started !
6 processor started !
7 processor started !
8 processor started !
Epoch:   0%|          | 0/5 [00:00<?, ?it/s]
Iteration:   0%|          | 0/138 [00:00<?, ?it/s][A/home/mexposit/cg/gea/dnabert/src/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811701593/work/torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)

Iteration:   1%|          | 1/138 [00:22<51:18, 22.47s/it][A
Iteration:   1%|         | 2/138 [00:42<47:14, 20.84s/it][A
Iteration:   2%|         | 3/138 [01:02<46:48, 20.81s/it][A
Iteration:   3%|         | 4/138 [01:23<46:21, 20.76s/it][A
Iteration:   4%|         | 5/138 [01:44<45:50, 20.68s/it][A
Iteration:   4%|         | 6/138 [02:04<45:28, 20.67s/it][A
Iteration:   5%|         | 7/138 [02:25<45:29, 20.83s/it][A
Iteration:   6%|         | 8/138 [02:46<44:51, 20.70s/it][A
Iteration:   7%|         | 9/138 [03:06<44:09, 20.54s/it][A03/11/2022 01:58:26 - INFO - __main__ -   Creating features from dataset file at /home/mexposit/cg/gea/transformers/2_geainit/in_data
finish loading examples
number of processes for converting feature: 2
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   Writing example 0/500
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   guid: dev-1
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   Writing example 0/500
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   input_ids: 2 2973 3687 2446 1580 2212 643 2559 2032 4018 3771 2784 2930 3514 1754 2906 3417 1367 1357 1317 1159 528 2098 187 736 2930 3516 1761 2935 3534 1836 3236 644 2561 2037 4037 3845 3079 15 47 174 684 2724 2691 2560 2034 4026 3801 2903 3407 1327 1197 677 2695 2574 2090 153 597 2374 1291 1056 115 447 1774 2986 3740 2659 2429 1512 1939 3645 2279 909 3622 2187 543 2159 431 1711 2734 2730 2714 2651 2400 1395 1471 1773 2984 3731 2622 2284 932 3715 2560 2034 4025 3797 3 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   guid: dev-501
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   input_ids: 2 1416 1554 2106 219 861 3432 1425 1591 2253 806 3211 541 2151 398 1580 2209 632 2513 1845 3269 774 3083 29 104 401 1590 2252 804 3201 503 2000 3890 3257 725 2888 3348 1091 253 1000 3987 3646 2283 925 3686 2444 1569 2168 465 1845 3269 775 3085 40 147 573 2278 905 3605 2119 269 1062 138 538 2138 346 1371 1374 1386 1436 1633 2422 1483 1821 3176 401 1590 2252 801 3191 462 1833 3222 586 2332 1121 374 1484 1825 3189 456 1809 3126 201 789 3141 262 3 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   guid: dev-2
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   input_ids: 2 3837 3045 3976 3604 2115 254 1004 4004 3715 2559 2031 4016 3764 2754 2811 3040 3956 3523 1789 3047 3983 3629 2216 658 2618 2268 867 3456 1522 1980 3809 2936 3539 1856 3316 961 3829 3013 3848 3089 54 204 804 3203 511 2032 4019 3774 2794 2971 3679 2415 1456 1716 2755 2815 3055 4014 3756 2723 2686 2540 1955 3709 2536 1940 3652 2305 1016 4051 3902 3307 925 3685 2437 1541 2054 12 36 129 504 2004 3905 3319 976 3891 3264 756 3011 3840 3059 4030 3819 2976 3700 2500 3 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   guid: dev-502
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   input_ids: 2 6 10 27 96 372 1474 1786 3033 3925 3400 1300 1091 255 1005 4008 3732 2628 2308 1028 4097 4085 4037 3848 3089 53 197 773 3077 5 6 9 22 73 277 1093 262 1034 25 85 325 1285 1031 13 38 137 534 2121 280 1106 313 1238 844 3364 1156 515 2045 4069 3976 3603 2109 232 916 3652 2305 1016 4051 3902 3305 920 3665 2357 1223 784 3121 182 714 2843 3168 371 1469 1768 2962 3642 2265 853 3398 1291 1055 110 428 1700 2691 2559 2030 4012 3746 3 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   guid: dev-3
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   input_ids: 2 2809 3029 3912 3345 1079 205 807 3216 561 2231 718 2858 3225 598 2379 1312 1139 447 1773 2983 3726 2604 2212 643 2557 2024 3987 3645 2280 915 3647 2285 935 3726 2604 2212 642 2553 2005 3911 3341 1064 148 577 2294 970 3865 3160 339 1341 1256 913 3640 2259 832 3313 952 3796 2882 3321 982 3916 3362 1145 472 1876 3395 1280 1012 4034 3836 3043 3966 3561 1943 3661 2344 1169 568 2258 826 3291 862 3434 1436 1633 2421 1480 1810 3132 228 898 3580 2020 3971 3583 3 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   guid: dev-503
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   input_ids: 2 3478 1612 2337 1142 457 1814 3147 285 1127 399 1581 2216 659 2623 2288 945 3768 2769 2869 3269 775 3087 46 169 663 2637 2341 1158 524 2083 128 499 1982 3818 2969 3671 2382 1321 1173 582 2314 1052 100 386 1530 2009 3925 3399 1293 1064 145 566 2252 803 3199 495 1968 3761 2741 2758 2828 3107 126 492 1956 3715 2560 2033 4024 3795 2879 3309 936 3729 2613 2247 781 3110 138 540 2146 380 1505 1910 3531 1821 3174 395 1567 2158 426 1690 2652 2402 1404 1508 3 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   guid: dev-4
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   input_ids: 2 2690 2556 2020 3970 3579 2016 3956 3524 1795 3072 4081 4024 3793 2871 3278 811 3231 624 2481 1719 2768 2868 3266 764 3044 3971 3583 2029 4008 3732 2625 2293 967 3855 3117 167 656 2611 2240 756 3012 3843 3070 4075 3999 3694 2474 1692 2660 2436 1539 2047 4080 4020 3778 2812 3043 3968 3572 1987 3840 3059 4031 3821 2984 3732 2625 2296 980 3907 3327 1006 4010 3739 2655 2413 1446 1675 2590 2156 418 1658 2524 1891 3454 1516 1955 3712 2547 1984 3828 3011 3839 3053 4008 3731 3 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   guid: dev-504
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   input_ids: 2 2825 3095 79 303 1197 680 2705 2613 2248 785 3125 197 774 3081 24 82 313 1240 849 3383 1230 809 3223 590 2346 1179 605 2405 1416 1555 2109 232 914 3643 2272 882 3514 1754 2907 3423 1390 1452 1697 2680 2516 1860 3330 1018 4057 3926 3402 1307 1118 362 1436 1636 2436 1537 2037 4039 3856 3121 181 712 2835 3133 230 908 3617 2168 466 1850 3289 855 3408 1329 1205 712 2833 3127 208 820 3268 772 3076 4097 4085 4038 3850 3098 92 355 1407 1518 1963 3744 3 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   guid: dev-5
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   input_ids: 2 2640 2356 1220 771 3070 4076 4001 3702 2508 1827 3198 489 1942 3660 2340 1155 511 2029 4005 3720 2580 2113 247 975 3888 3251 701 2792 2962 3644 2273 888 3540 1857 3320 980 3905 3317 967 3856 3122 185 727 2895 3373 1191 654 2604 2209 632 2516 1859 3325 1000 3985 3640 2257 821 3272 785 3125 198 780 3105 120 467 1856 3314 956 3810 2940 3555 1917 3560 1937 3637 2248 787 3133 231 911 3630 2220 673 2677 2504 1812 3139 255 1007 4014 3755 2717 2663 2446 1578 3 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   *** Example ***
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   guid: dev-505
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   input_ids: 2 3824 2996 3780 2820 3074 4092 4067 3967 3567 1968 3763 2751 2798 2986 3739 2655 2414 1452 1700 2689 2552 2001 3895 3279 814 3243 671 2672 2483 1728 2803 3007 3823 2991 3760 2739 2749 2789 2951 3599 2094 171 671 2671 2479 1710 2730 2715 2654 2409 1431 1616 2353 1208 723 2880 3316 963 3838 3051 4000 3700 2499 1790 3050 3995 3677 2407 1423 1584 2226 699 2781 2919 3471 1584 2227 703 2800 2993 3767 2768 2866 3259 736 2929 3512 1748 2882 3324 995 3967 3567 1968 3761 2741 3 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/11/2022 01:58:26 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)
03/11/2022 01:58:27 - INFO - __main__ -   Saving features into cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 01:58:27 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 01:58:27 - INFO - __main__ -     Num examples = 1000
03/11/2022 01:58:27 - INFO - __main__ -     Batch size = 32
1 processor started !
2 processor started !


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:17,  6.36s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:11,  6.40s/it][A[A

Evaluating:   9%|         | 3/32 [00:19<03:06,  6.43s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<03:00,  6.44s/it][A[A

Evaluating:  16%|        | 5/32 [00:32<02:53,  6.44s/it][A[A

Evaluating:  19%|        | 6/32 [00:38<02:47,  6.45s/it][A[A

Evaluating:  22%|       | 7/32 [00:45<02:41,  6.44s/it][A[A

Evaluating:  25%|       | 8/32 [00:51<02:34,  6.44s/it][A[A

Evaluating:  28%|       | 9/32 [00:57<02:28,  6.44s/it][A[A

Evaluating:  31%|      | 10/32 [01:04<02:22,  6.46s/it][A[A

Evaluating:  34%|      | 11/32 [01:10<02:14,  6.42s/it][A[A

Evaluating:  38%|      | 12/32 [01:17<02:07,  6.38s/it][A[A

Evaluating:  41%|      | 13/32 [01:23<02:00,  6.35s/it][A[A

Evaluating:  44%|     | 14/32 [01:29<01:53,  6.33s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:47,  6.32s/it][A[A

Evaluating:  50%|     | 16/32 [01:42<01:41,  6.32s/it][A[A

Evaluating:  53%|    | 17/32 [01:48<01:34,  6.31s/it][A[A

Evaluating:  56%|    | 18/32 [01:54<01:28,  6.30s/it][A[A

Evaluating:  59%|    | 19/32 [02:01<01:21,  6.30s/it][A[A

Evaluating:  62%|   | 20/32 [02:07<01:15,  6.31s/it][A[A

Evaluating:  66%|   | 21/32 [02:13<01:09,  6.31s/it][A[A

Evaluating:  69%|   | 22/32 [02:20<01:03,  6.30s/it][A[A

Evaluating:  72%|  | 23/32 [02:26<00:56,  6.32s/it][A[A

Evaluating:  75%|  | 24/32 [02:32<00:50,  6.32s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:44,  6.30s/it][A[A

Evaluating:  81%| | 26/32 [02:45<00:38,  6.39s/it][A[A

Evaluating:  84%| | 27/32 [02:52<00:32,  6.43s/it][A[A

Evaluating:  88%| | 28/32 [02:58<00:25,  6.41s/it][A[A

Evaluating:  91%| | 29/32 [03:04<00:19,  6.39s/it][A[A

Evaluating:  94%|| 30/32 [03:11<00:12,  6.37s/it][A[A

Evaluating:  97%|| 31/32 [03:17<00:06,  6.36s/it][A[A

Evaluating: 100%|| 32/32 [03:19<00:00,  4.94s/it][A[AEvaluating: 100%|| 32/32 [03:19<00:00,  6.22s/it]
03/11/2022 02:01:46 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 02:01:46 - INFO - __main__ -     acc = 0.575
03/11/2022 02:01:46 - INFO - __main__ -     auc = 0.609359434449152
03/11/2022 02:01:46 - INFO - __main__ -     f1 = 0.5748771394933136
03/11/2022 02:01:46 - INFO - __main__ -     mcc = 0.15086065972631862
03/11/2022 02:01:46 - INFO - __main__ -     precision = 0.5755099749823593
03/11/2022 02:01:46 - INFO - __main__ -     recall = 0.5753507687506751
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:   7%|         | 10/138 [06:47<2:55:46, 82.39s/it][A
Iteration:   8%|         | 11/138 [07:08<2:14:45, 63.67s/it][A
Iteration:   9%|         | 12/138 [07:29<1:46:06, 50.53s/it][A
Iteration:   9%|         | 13/138 [07:49<1:26:11, 41.37s/it][A
Iteration:  10%|         | 14/138 [08:09<1:12:13, 34.94s/it][A
Iteration:  11%|         | 15/138 [09:15<1:02:29, 30.48s/it][A
Iteration:  12%|        | 16/138 [09:36<1:23:59, 41.31s/it][A
Iteration:  12%|        | 17/138 [09:56<1:10:27, 34.94s/it][A
Iteration:  13%|        | 18/138 [10:16<1:01:04, 30.54s/it][A
Iteration:  14%|        | 19/138 [10:36<54:22, 27.41s/it]  [A03/11/2022 02:05:56 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 02:05:56 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 02:05:56 - INFO - __main__ -     Num examples = 1000
03/11/2022 02:05:56 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.575, "eval_f1": 0.5748771394933136, "eval_mcc": 0.15086065972631862, "eval_auc": 0.609359434449152, "eval_precision": 0.5755099749823593, "eval_recall": 0.5753507687506751, "learning_rate": 2.8985507246376814e-05, "loss": 0.6903170228004456, "step": 10}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.28s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.28s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.29s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.30s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:50,  6.30s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.29s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.31s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:31,  6.31s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.30s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.30s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.30s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.30s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:59,  6.31s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.32s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:47,  6.33s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:41,  6.33s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:34,  6.32s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.32s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:22,  6.32s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.32s/it][A[A

Evaluating:  66%|   | 21/32 [02:13<01:09,  6.32s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:05,  6.51s/it][A[A

Evaluating:  72%|  | 23/32 [02:26<00:58,  6.55s/it][A[A

Evaluating:  75%|  | 24/32 [02:32<00:52,  6.54s/it][A[A

Evaluating:  78%|  | 25/32 [02:39<00:45,  6.49s/it][A[A

Evaluating:  81%| | 26/32 [02:45<00:38,  6.44s/it][A[A

Evaluating:  84%| | 27/32 [02:51<00:32,  6.41s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.38s/it][A[A

Evaluating:  91%| | 29/32 [03:04<00:19,  6.36s/it][A[A

Evaluating:  94%|| 30/32 [03:10<00:12,  6.35s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.35s/it][A[A

Evaluating: 100%|| 32/32 [03:18<00:00,  4.93s/it][A[AEvaluating: 100%|| 32/32 [03:18<00:00,  6.21s/it]
03/11/2022 02:09:14 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 02:09:14 - INFO - __main__ -     acc = 0.666
03/11/2022 02:09:14 - INFO - __main__ -     auc = 0.7333397345879792
03/11/2022 02:09:14 - INFO - __main__ -     f1 = 0.6647932557205941
03/11/2022 02:09:14 - INFO - __main__ -     mcc = 0.3369475427433481
03/11/2022 02:09:14 - INFO - __main__ -     precision = 0.6699903466647116
03/11/2022 02:09:14 - INFO - __main__ -     recall = 0.6669707262623474
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  14%|        | 20/138 [14:15<2:46:56, 84.88s/it][A
Iteration:  15%|        | 21/138 [14:36<2:07:57, 65.62s/it][A
Iteration:  16%|        | 22/138 [14:56<1:40:33, 52.01s/it][A
Iteration:  17%|        | 23/138 [15:16<1:21:21, 42.45s/it][A
Iteration:  17%|        | 24/138 [15:37<1:08:17, 35.95s/it][A
Iteration:  18%|        | 25/138 [15:57<58:52, 31.26s/it]  [A
Iteration:  19%|        | 26/138 [16:17<52:04, 27.90s/it][A
Iteration:  20%|        | 27/138 [16:37<47:15, 25.54s/it][A
Iteration:  20%|        | 28/138 [16:58<43:51, 23.92s/it][A
Iteration:  21%|        | 29/138 [17:18<41:26, 22.81s/it][A03/11/2022 02:12:37 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 02:12:37 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 02:12:37 - INFO - __main__ -     Num examples = 1000
03/11/2022 02:12:37 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.666, "eval_f1": 0.6647932557205941, "eval_mcc": 0.3369475427433481, "eval_auc": 0.7333397345879792, "eval_precision": 0.6699903466647116, "eval_recall": 0.6669707262623474, "learning_rate": 5.797101449275363e-05, "loss": 0.6543586611747741, "step": 20}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:15,  6.29s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.29s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.29s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.29s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.29s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.30s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.31s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:31,  6.31s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.30s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.30s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.29s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.29s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:59,  6.29s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.30s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:47,  6.30s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.29s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:34,  6.29s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.29s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.30s/it][A[A

Evaluating:  62%|   | 20/32 [02:05<01:15,  6.30s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.29s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:03,  6.31s/it][A[A

Evaluating:  72%|  | 23/32 [02:24<00:56,  6.30s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.30s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:44,  6.29s/it][A[A

Evaluating:  81%| | 26/32 [02:43<00:37,  6.29s/it][A[A

Evaluating:  84%| | 27/32 [02:49<00:31,  6.29s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.29s/it][A[A

Evaluating:  91%| | 29/32 [03:02<00:18,  6.29s/it][A[A

Evaluating:  94%|| 30/32 [03:08<00:12,  6.31s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.31s/it][A[A

Evaluating: 100%|| 32/32 [03:16<00:00,  4.90s/it][A[AEvaluating: 100%|| 32/32 [03:16<00:00,  6.15s/it]
03/11/2022 02:15:54 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 02:15:54 - INFO - __main__ -     acc = 0.722
03/11/2022 02:15:54 - INFO - __main__ -     auc = 0.7982284527767441
03/11/2022 02:15:54 - INFO - __main__ -     f1 = 0.7219288137763268
03/11/2022 02:15:54 - INFO - __main__ -     mcc = 0.4438634053946713
03/11/2022 02:15:54 - INFO - __main__ -     precision = 0.7219459104749939
03/11/2022 02:15:54 - INFO - __main__ -     recall = 0.7219174958291825
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  22%|       | 30/138 [20:55<2:25:56, 81.08s/it][A
Iteration:  22%|       | 31/138 [21:15<1:52:13, 62.93s/it][A
Iteration:  23%|       | 32/138 [21:36<1:28:35, 50.14s/it][A
Iteration:  24%|       | 33/138 [21:56<1:12:04, 41.18s/it][A
Iteration:  25%|       | 34/138 [22:16<1:00:27, 34.88s/it][A
Iteration:  25%|       | 35/138 [22:37<52:35, 30.63s/it]  [A
Iteration:  26%|       | 36/138 [22:57<46:58, 27.63s/it][A
Iteration:  27%|       | 37/138 [23:18<42:43, 25.38s/it][A
Iteration:  28%|       | 38/138 [23:38<39:39, 23.80s/it][A
Iteration:  28%|       | 39/138 [23:58<37:23, 22.67s/it][A03/11/2022 02:19:17 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 02:19:17 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 02:19:17 - INFO - __main__ -     Num examples = 1000
03/11/2022 02:19:17 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.722, "eval_f1": 0.7219288137763268, "eval_mcc": 0.4438634053946713, "eval_auc": 0.7982284527767441, "eval_precision": 0.7219459104749939, "eval_recall": 0.7219174958291825, "learning_rate": 8.695652173913044e-05, "loss": 0.5999997735023499, "step": 30}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:15,  6.32s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:09,  6.31s/it][A[A

Evaluating:   9%|         | 3/32 [00:19<03:07,  6.47s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<03:02,  6.51s/it][A[A

Evaluating:  16%|        | 5/32 [00:32<02:56,  6.53s/it][A[A

Evaluating:  19%|        | 6/32 [00:38<02:49,  6.53s/it][A[A

Evaluating:  22%|       | 7/32 [00:45<02:41,  6.47s/it][A[A

Evaluating:  25%|       | 8/32 [00:51<02:33,  6.41s/it][A[A

Evaluating:  28%|       | 9/32 [00:57<02:26,  6.37s/it][A[A

Evaluating:  31%|      | 10/32 [01:04<02:19,  6.34s/it][A[A

Evaluating:  34%|      | 11/32 [01:10<02:12,  6.33s/it][A[A

Evaluating:  38%|      | 12/32 [01:16<02:06,  6.31s/it][A[A

Evaluating:  41%|      | 13/32 [01:23<02:00,  6.32s/it][A[A

Evaluating:  44%|     | 14/32 [01:29<01:53,  6.32s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:47,  6.32s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:40,  6.31s/it][A[A

Evaluating:  53%|    | 17/32 [01:48<01:34,  6.30s/it][A[A

Evaluating:  56%|    | 18/32 [01:54<01:28,  6.30s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:21,  6.30s/it][A[A

Evaluating:  62%|   | 20/32 [02:07<01:15,  6.29s/it][A[A

Evaluating:  66%|   | 21/32 [02:13<01:09,  6.31s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:02,  6.30s/it][A[A

Evaluating:  72%|  | 23/32 [02:26<00:57,  6.41s/it][A[A

Evaluating:  75%|  | 24/32 [02:32<00:51,  6.46s/it][A[A

Evaluating:  78%|  | 25/32 [02:39<00:45,  6.47s/it][A[A

Evaluating:  81%| | 26/32 [02:46<00:38,  6.49s/it][A[A

Evaluating:  84%| | 27/32 [02:52<00:32,  6.46s/it][A[A

Evaluating:  88%| | 28/32 [02:58<00:25,  6.42s/it][A[A

Evaluating:  91%| | 29/32 [03:05<00:19,  6.39s/it][A[A

Evaluating:  94%|| 30/32 [03:11<00:12,  6.35s/it][A[A

Evaluating:  97%|| 31/32 [03:17<00:06,  6.34s/it][A[A

Evaluating: 100%|| 32/32 [03:19<00:00,  4.93s/it][A[AEvaluating: 100%|| 32/32 [03:19<00:00,  6.23s/it]
03/11/2022 02:22:37 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 02:22:37 - INFO - __main__ -     acc = 0.744
03/11/2022 02:22:37 - INFO - __main__ -     auc = 0.8189285099879576
03/11/2022 02:22:37 - INFO - __main__ -     f1 = 0.7421808279218163
03/11/2022 02:22:37 - INFO - __main__ -     mcc = 0.49892892705097225
03/11/2022 02:22:37 - INFO - __main__ -     precision = 0.7536767899916845
03/11/2022 02:22:37 - INFO - __main__ -     recall = 0.7453220831282932
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  29%|       | 40/138 [27:37<2:13:29, 81.73s/it][A
Iteration:  30%|       | 41/138 [27:58<1:42:30, 63.41s/it][A
Iteration:  30%|       | 42/138 [28:18<1:20:46, 50.49s/it][A
Iteration:  31%|       | 43/138 [28:39<1:05:40, 41.48s/it][A
Iteration:  32%|      | 44/138 [28:59<55:08, 35.19s/it]  [A
Iteration:  33%|      | 45/138 [29:19<47:29, 30.64s/it][A
Iteration:  33%|      | 46/138 [29:40<42:12, 27.52s/it][A
Iteration:  34%|      | 47/138 [30:01<38:49, 25.60s/it][A
Iteration:  35%|      | 48/138 [30:21<35:58, 23.99s/it][A
Iteration:  36%|      | 49/138 [30:41<33:49, 22.80s/it][A03/11/2022 02:26:00 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 02:26:00 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 02:26:00 - INFO - __main__ -     Num examples = 1000
03/11/2022 02:26:00 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.744, "eval_f1": 0.7421808279218163, "eval_mcc": 0.49892892705097225, "eval_auc": 0.8189285099879576, "eval_precision": 0.7536767899916845, "eval_recall": 0.7453220831282932, "learning_rate": 0.00011594202898550725, "loss": 0.5321575552225113, "step": 40}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.27s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:07,  6.27s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.28s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.29s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.29s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.28s/it][A[A

Evaluating:  22%|       | 7/32 [00:43<02:37,  6.28s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:32,  6.37s/it][A[A

Evaluating:  28%|       | 9/32 [00:57<02:27,  6.42s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:20,  6.39s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:13,  6.37s/it][A[A

Evaluating:  38%|      | 12/32 [01:16<02:07,  6.35s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:00,  6.34s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.33s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:47,  6.31s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:40,  6.30s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:34,  6.30s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.29s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:21,  6.29s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.29s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.29s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.28s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.28s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.27s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:43,  6.27s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:38,  6.39s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:32,  6.46s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.48s/it][A[A

Evaluating:  91%| | 29/32 [03:04<00:19,  6.49s/it][A[A

Evaluating:  94%|| 30/32 [03:10<00:12,  6.43s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.39s/it][A[A

Evaluating: 100%|| 32/32 [03:18<00:00,  4.95s/it][A[AEvaluating: 100%|| 32/32 [03:18<00:00,  6.19s/it]
03/11/2022 02:29:19 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 02:29:19 - INFO - __main__ -     acc = 0.737
03/11/2022 02:29:19 - INFO - __main__ -     auc = 0.8125632623994301
03/11/2022 02:29:19 - INFO - __main__ -     f1 = 0.7274698947494951
03/11/2022 02:29:19 - INFO - __main__ -     mcc = 0.520234487105159
03/11/2022 02:29:19 - INFO - __main__ -     precision = 0.7821994311653576
03/11/2022 02:29:19 - INFO - __main__ -     recall = 0.7397629935467351
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  36%|      | 50/138 [34:19<1:59:29, 81.47s/it][A
Iteration:  37%|      | 51/138 [34:40<1:31:38, 63.20s/it][A
Iteration:  38%|      | 52/138 [35:00<1:12:05, 50.29s/it][A
Iteration:  38%|      | 53/138 [35:20<58:23, 41.22s/it]  [A
Iteration:  39%|      | 54/138 [35:40<48:49, 34.87s/it][A
Iteration:  40%|      | 55/138 [36:01<42:22, 30.64s/it][A
Iteration:  41%|      | 56/138 [36:21<37:42, 27.59s/it][A
Iteration:  41%|     | 57/138 [36:41<34:09, 25.31s/it][A
Iteration:  42%|     | 58/138 [37:01<31:35, 23.69s/it][A
Iteration:  43%|     | 59/138 [37:21<29:43, 22.58s/it][A03/11/2022 02:32:41 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 02:32:41 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 02:32:41 - INFO - __main__ -     Num examples = 1000
03/11/2022 02:32:41 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.737, "eval_f1": 0.7274698947494951, "eval_mcc": 0.520234487105159, "eval_auc": 0.8125632623994301, "eval_precision": 0.7821994311653576, "eval_recall": 0.7397629935467351, "learning_rate": 0.00014492753623188405, "loss": 0.5167684882879258, "step": 50}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.29s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.28s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.29s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.30s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.29s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.29s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.29s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.29s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.28s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.28s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.29s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.29s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:59,  6.30s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.30s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:47,  6.29s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.29s/it][A[A

Evaluating:  53%|    | 17/32 [01:46<01:34,  6.29s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.29s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.31s/it][A[A

Evaluating:  62%|   | 20/32 [02:05<01:15,  6.30s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.29s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.29s/it][A[A

Evaluating:  72%|  | 23/32 [02:24<00:56,  6.29s/it][A[A

Evaluating:  75%|  | 24/32 [02:30<00:50,  6.28s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:43,  6.28s/it][A[A

Evaluating:  81%| | 26/32 [02:43<00:37,  6.30s/it][A[A

Evaluating:  84%| | 27/32 [02:49<00:31,  6.31s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.30s/it][A[A

Evaluating:  91%| | 29/32 [03:02<00:18,  6.30s/it][A[A

Evaluating:  94%|| 30/32 [03:08<00:12,  6.29s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.36s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.96s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.16s/it]
03/11/2022 02:35:58 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 02:35:58 - INFO - __main__ -     acc = 0.711
03/11/2022 02:35:58 - INFO - __main__ -     auc = 0.8566679069097543
03/11/2022 02:35:58 - INFO - __main__ -     f1 = 0.7022948023144776
03/11/2022 02:35:58 - INFO - __main__ -     mcc = 0.44189558186069466
03/11/2022 02:35:58 - INFO - __main__ -     precision = 0.7338640275387264
03/11/2022 02:35:58 - INFO - __main__ -     recall = 0.7087449140031445
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  43%|     | 60/138 [40:58<1:45:13, 80.94s/it][A
Iteration:  44%|     | 61/138 [41:19<1:20:44, 62.91s/it][A
Iteration:  45%|     | 62/138 [41:39<1:03:29, 50.13s/it][A
Iteration:  46%|     | 63/138 [42:00<51:23, 41.12s/it]  [A
Iteration:  46%|     | 64/138 [42:20<42:54, 34.80s/it][A
Iteration:  47%|     | 65/138 [42:40<36:57, 30.38s/it][A
Iteration:  48%|     | 66/138 [43:00<32:53, 27.40s/it][A
Iteration:  49%|     | 67/138 [43:21<30:05, 25.43s/it][A
Iteration:  49%|     | 68/138 [43:41<27:44, 23.78s/it][A
Iteration:  50%|     | 69/138 [44:01<26:04, 22.68s/it][A03/11/2022 02:39:21 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 02:39:21 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 02:39:21 - INFO - __main__ -     Num examples = 1000
03/11/2022 02:39:21 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.711, "eval_f1": 0.7022948023144776, "eval_mcc": 0.44189558186069466, "eval_auc": 0.8566679069097543, "eval_precision": 0.7338640275387264, "eval_recall": 0.7087449140031445, "learning_rate": 0.00017391304347826088, "loss": 0.4857077211141586, "step": 60}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.28s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:09,  6.30s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.30s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.29s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.29s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.29s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:39,  6.36s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:34,  6.42s/it][A[A

Evaluating:  28%|       | 9/32 [00:57<02:27,  6.41s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:20,  6.38s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:13,  6.36s/it][A[A

Evaluating:  38%|      | 12/32 [01:16<02:06,  6.35s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:00,  6.33s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.32s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:47,  6.31s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:40,  6.30s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:34,  6.30s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.30s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:21,  6.30s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.30s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.29s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:02,  6.29s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.29s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.30s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:44,  6.30s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.31s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:32,  6.42s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.45s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:19,  6.41s/it][A[A

Evaluating:  94%|| 30/32 [03:10<00:12,  6.37s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.34s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.93s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.19s/it]
03/11/2022 02:42:39 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 02:42:39 - INFO - __main__ -     acc = 0.751
03/11/2022 02:42:39 - INFO - __main__ -     auc = 0.8478421770667051
03/11/2022 02:42:39 - INFO - __main__ -     f1 = 0.7379694844944231
03/11/2022 02:42:39 - INFO - __main__ -     mcc = 0.5726759794519702
03/11/2022 02:42:39 - INFO - __main__ -     precision = 0.8224505327245053
03/11/2022 02:42:39 - INFO - __main__ -     recall = 0.7542698368880301
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  51%|     | 70/138 [47:39<1:32:10, 81.33s/it][A
Iteration:  51%|    | 71/138 [48:00<1:10:27, 63.10s/it][A
Iteration:  52%|    | 72/138 [48:20<55:15, 50.24s/it]  [A
Iteration:  53%|    | 73/138 [48:40<44:37, 41.20s/it][A
Iteration:  54%|    | 74/138 [49:00<37:09, 34.84s/it][A
Iteration:  54%|    | 75/138 [49:21<32:05, 30.57s/it][A
Iteration:  55%|    | 76/138 [49:41<28:21, 27.44s/it][A
Iteration:  56%|    | 77/138 [50:01<25:37, 25.21s/it][A
Iteration:  57%|    | 78/138 [50:21<23:39, 23.66s/it][A
Iteration:  57%|    | 79/138 [50:41<22:13, 22.59s/it][A03/11/2022 02:46:01 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 02:46:01 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 02:46:01 - INFO - __main__ -     Num examples = 1000
03/11/2022 02:46:01 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.751, "eval_f1": 0.7379694844944231, "eval_mcc": 0.5726759794519702, "eval_auc": 0.8478421770667051, "eval_precision": 0.8224505327245053, "eval_recall": 0.7542698368880301, "learning_rate": 0.0001996779388083736, "loss": 0.4982983887195587, "step": 70}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:15,  6.31s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:09,  6.31s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.29s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.29s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.29s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.29s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.29s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.28s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.29s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.29s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.31s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:07,  6.39s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:02,  6.46s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:56,  6.48s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:50,  6.49s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:43,  6.46s/it][A[A

Evaluating:  53%|    | 17/32 [01:48<01:36,  6.42s/it][A[A

Evaluating:  56%|    | 18/32 [01:54<01:29,  6.38s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:22,  6.35s/it][A[A

Evaluating:  62%|   | 20/32 [02:07<01:15,  6.33s/it][A[A

Evaluating:  66%|   | 21/32 [02:13<01:09,  6.32s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:03,  6.31s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.30s/it][A[A

Evaluating:  75%|  | 24/32 [02:32<00:50,  6.29s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:44,  6.31s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.31s/it][A[A

Evaluating:  84%| | 27/32 [02:51<00:31,  6.30s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.29s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:18,  6.29s/it][A[A

Evaluating:  94%|| 30/32 [03:10<00:12,  6.31s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.39s/it][A[A

Evaluating: 100%|| 32/32 [03:18<00:00,  4.98s/it][A[AEvaluating: 100%|| 32/32 [03:18<00:00,  6.20s/it]
03/11/2022 02:49:19 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 02:49:19 - INFO - __main__ -     acc = 0.771
03/11/2022 02:49:19 - INFO - __main__ -     auc = 0.8837212093570339
03/11/2022 02:49:19 - INFO - __main__ -     f1 = 0.769087816206002
03/11/2022 02:49:19 - INFO - __main__ -     mcc = 0.547482261260391
03/11/2022 02:49:19 - INFO - __main__ -     precision = 0.7776606954689147
03/11/2022 02:49:19 - INFO - __main__ -     recall = 0.7698768958715909
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  58%|    | 80/138 [54:19<1:18:38, 81.35s/it][A
Iteration:  59%|    | 81/138 [54:40<59:59, 63.15s/it]  [A
Iteration:  59%|    | 82/138 [55:00<46:54, 50.27s/it][A
Iteration:  60%|    | 83/138 [55:20<37:46, 41.21s/it][A
Iteration:  61%|    | 84/138 [55:40<31:21, 34.84s/it][A
Iteration:  62%|   | 85/138 [56:00<26:51, 30.40s/it][A
Iteration:  62%|   | 86/138 [56:21<23:41, 27.33s/it][A
Iteration:  63%|   | 87/138 [56:41<21:22, 25.14s/it][A
Iteration:  64%|   | 88/138 [57:01<19:39, 23.60s/it][A
Iteration:  64%|   | 89/138 [57:21<18:23, 22.52s/it][A03/11/2022 02:52:40 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 02:52:40 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 02:52:40 - INFO - __main__ -     Num examples = 1000
03/11/2022 02:52:40 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.771, "eval_f1": 0.769087816206002, "eval_mcc": 0.547482261260391, "eval_auc": 0.8837212093570339, "eval_precision": 0.7776606954689147, "eval_recall": 0.7698768958715909, "learning_rate": 0.00019645732689210952, "loss": 0.4435038357973099, "step": 80}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:16,  6.32s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.30s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.29s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.29s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.28s/it][A[A

Evaluating:  19%|        | 6/32 [00:38<02:45,  6.38s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:41,  6.45s/it][A[A

Evaluating:  25%|       | 8/32 [00:51<02:35,  6.47s/it][A[A

Evaluating:  28%|       | 9/32 [00:57<02:29,  6.49s/it][A[A

Evaluating:  31%|      | 10/32 [01:04<02:22,  6.45s/it][A[A

Evaluating:  34%|      | 11/32 [01:10<02:14,  6.40s/it][A[A

Evaluating:  38%|      | 12/32 [01:16<02:07,  6.37s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:00,  6.35s/it][A[A

Evaluating:  44%|     | 14/32 [01:29<01:53,  6.33s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:47,  6.32s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:41,  6.31s/it][A[A

Evaluating:  53%|    | 17/32 [01:48<01:34,  6.31s/it][A[A

Evaluating:  56%|    | 18/32 [01:54<01:28,  6.30s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:21,  6.29s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.28s/it][A[A

Evaluating:  66%|   | 21/32 [02:13<01:09,  6.28s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:02,  6.27s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.28s/it][A[A

Evaluating:  75%|  | 24/32 [02:32<00:50,  6.29s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:44,  6.29s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.30s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.29s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.29s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:18,  6.28s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.28s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.28s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.89s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.18s/it]
03/11/2022 02:55:58 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 02:55:58 - INFO - __main__ -     acc = 0.798
03/11/2022 02:55:58 - INFO - __main__ -     auc = 0.8645494516925318
03/11/2022 02:55:58 - INFO - __main__ -     f1 = 0.7975716616360218
03/11/2022 02:55:58 - INFO - __main__ -     mcc = 0.6009281130574889
03/11/2022 02:55:58 - INFO - __main__ -     precision = 0.8021368091880368
03/11/2022 02:55:58 - INFO - __main__ -     recall = 0.7988005649107225
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  65%|   | 90/138 [1:00:59<1:04:53, 81.12s/it][A
Iteration:  66%|   | 91/138 [1:01:19<49:18, 62.95s/it]  [A
Iteration:  67%|   | 92/138 [1:01:39<38:25, 50.11s/it][A
Iteration:  67%|   | 93/138 [1:01:59<30:50, 41.12s/it][A
Iteration:  68%|   | 94/138 [1:02:19<25:31, 34.80s/it][A
Iteration:  69%|   | 95/138 [1:02:39<21:45, 30.37s/it][A
Iteration:  70%|   | 96/138 [1:03:00<19:06, 27.29s/it][A
Iteration:  70%|   | 97/138 [1:03:20<17:09, 25.11s/it][A
Iteration:  71%|   | 98/138 [1:03:40<15:53, 23.85s/it][A
Iteration:  72%|  | 99/138 [1:04:01<14:48, 22.78s/it][A03/11/2022 02:59:20 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 02:59:20 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 02:59:20 - INFO - __main__ -     Num examples = 1000
03/11/2022 02:59:20 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.798, "eval_f1": 0.7975716616360218, "eval_mcc": 0.6009281130574889, "eval_auc": 0.8645494516925318, "eval_precision": 0.8021368091880368, "eval_recall": 0.7988005649107225, "learning_rate": 0.0001932367149758454, "loss": 0.5278758496046067, "step": 90}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:15,  6.30s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.28s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.28s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.28s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.28s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.28s/it][A[A

Evaluating:  22%|       | 7/32 [00:43<02:37,  6.29s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:31,  6.30s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.29s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.29s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.32s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:08,  6.41s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:02,  6.44s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:55,  6.40s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:48,  6.37s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:41,  6.35s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:34,  6.33s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.32s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:21,  6.31s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.30s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.30s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:02,  6.29s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.30s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.30s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:44,  6.30s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.30s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.29s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.29s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:18,  6.29s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.29s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.37s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.98s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.18s/it]
03/11/2022 03:02:38 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 03:02:38 - INFO - __main__ -     acc = 0.857
03/11/2022 03:02:38 - INFO - __main__ -     auc = 0.9199203043796584
03/11/2022 03:02:38 - INFO - __main__ -     f1 = 0.8568956769484954
03/11/2022 03:02:38 - INFO - __main__ -     mcc = 0.716680961875645
03/11/2022 03:02:38 - INFO - __main__ -     precision = 0.8591366478597033
03/11/2022 03:02:38 - INFO - __main__ -     recall = 0.8575460790314902
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  72%|  | 100/138 [1:07:39<51:30, 81.33s/it][A
Iteration:  73%|  | 101/138 [1:08:00<39:03, 63.34s/it][A
Iteration:  74%|  | 102/138 [1:08:20<30:14, 50.41s/it][A
Iteration:  75%|  | 103/138 [1:08:40<24:05, 41.31s/it][A
Iteration:  75%|  | 104/138 [1:09:00<19:47, 34.92s/it][A
Iteration:  76%|  | 105/138 [1:09:20<16:45, 30.48s/it][A
Iteration:  77%|  | 106/138 [1:09:41<14:36, 27.38s/it][A
Iteration:  78%|  | 107/138 [1:10:01<13:00, 25.18s/it][A
Iteration:  78%|  | 108/138 [1:10:21<11:49, 23.64s/it][A
Iteration:  79%|  | 109/138 [1:10:41<10:54, 22.56s/it][A03/11/2022 03:06:00 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 03:06:00 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 03:06:00 - INFO - __main__ -     Num examples = 1000
03/11/2022 03:06:00 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.857, "eval_f1": 0.8568956769484954, "eval_mcc": 0.716680961875645, "eval_auc": 0.9199203043796584, "eval_precision": 0.8591366478597033, "eval_recall": 0.8575460790314902, "learning_rate": 0.00019001610305958134, "loss": 0.41070992276072504, "step": 100}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.28s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.27s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.28s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.28s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.28s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.27s/it][A[A

Evaluating:  22%|       | 7/32 [00:43<02:37,  6.29s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.29s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.29s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.29s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.30s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.30s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:59,  6.29s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.29s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:47,  6.30s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.30s/it][A[A

Evaluating:  53%|    | 17/32 [01:46<01:34,  6.29s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.29s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.28s/it][A[A

Evaluating:  62%|   | 20/32 [02:05<01:15,  6.28s/it][A[A

Evaluating:  66%|   | 21/32 [02:11<01:09,  6.28s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.27s/it][A[A

Evaluating:  72%|  | 23/32 [02:24<00:56,  6.29s/it][A[A

Evaluating:  75%|  | 24/32 [02:30<00:50,  6.30s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:44,  6.29s/it][A[A

Evaluating:  81%| | 26/32 [02:43<00:37,  6.29s/it][A[A

Evaluating:  84%| | 27/32 [02:49<00:31,  6.28s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.28s/it][A[A

Evaluating:  91%| | 29/32 [03:02<00:18,  6.28s/it][A[A

Evaluating:  94%|| 30/32 [03:08<00:12,  6.29s/it][A[A

Evaluating:  97%|| 31/32 [03:14<00:06,  6.29s/it][A[A

Evaluating: 100%|| 32/32 [03:16<00:00,  4.89s/it][A[AEvaluating: 100%|| 32/32 [03:16<00:00,  6.14s/it]
03/11/2022 03:09:17 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 03:09:17 - INFO - __main__ -     acc = 0.825
03/11/2022 03:09:17 - INFO - __main__ -     auc = 0.924217146560726
03/11/2022 03:09:17 - INFO - __main__ -     f1 = 0.8206921283844361
03/11/2022 03:09:17 - INFO - __main__ -     mcc = 0.6919245860444811
03/11/2022 03:09:17 - INFO - __main__ -     precision = 0.8656527646186024
03/11/2022 03:09:17 - INFO - __main__ -     recall = 0.8273321571027922
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  80%|  | 110/138 [1:14:18<37:43, 80.82s/it][A
Iteration:  80%|  | 111/138 [1:14:38<28:14, 62.75s/it][A
Iteration:  81%|  | 112/138 [1:14:59<21:46, 50.27s/it][A
Iteration:  82%| | 113/138 [1:15:20<17:12, 41.29s/it][A
Iteration:  83%| | 114/138 [1:15:40<13:57, 34.91s/it][A
Iteration:  83%| | 115/138 [1:16:00<11:41, 30.51s/it][A
Iteration:  84%| | 116/138 [1:16:20<10:02, 27.37s/it][A
Iteration:  85%| | 117/138 [1:16:40<08:48, 25.17s/it][A
Iteration:  86%| | 118/138 [1:17:01<07:56, 23.84s/it][A
Iteration:  86%| | 119/138 [1:17:21<07:14, 22.88s/it][A03/11/2022 03:12:41 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 03:12:41 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 03:12:41 - INFO - __main__ -     Num examples = 1000
03/11/2022 03:12:41 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.825, "eval_f1": 0.8206921283844361, "eval_mcc": 0.6919245860444811, "eval_auc": 0.924217146560726, "eval_precision": 0.8656527646186024, "eval_recall": 0.8273321571027922, "learning_rate": 0.00018679549114331725, "loss": 0.4181971222162247, "step": 110}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.27s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.27s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.28s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.27s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.27s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.30s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.30s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:31,  6.30s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:25,  6.32s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:19,  6.35s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:15,  6.46s/it][A[A

Evaluating:  38%|      | 12/32 [01:16<02:09,  6.48s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:03,  6.50s/it][A[A

Evaluating:  44%|     | 14/32 [01:29<01:56,  6.49s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:49,  6.42s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:42,  6.38s/it][A[A

Evaluating:  53%|    | 17/32 [01:48<01:35,  6.35s/it][A[A

Evaluating:  56%|    | 18/32 [01:54<01:28,  6.33s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:21,  6.31s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.30s/it][A[A

Evaluating:  66%|   | 21/32 [02:13<01:09,  6.31s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:03,  6.32s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.31s/it][A[A

Evaluating:  75%|  | 24/32 [02:32<00:50,  6.31s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:44,  6.30s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.31s/it][A[A

Evaluating:  84%| | 27/32 [02:51<00:31,  6.30s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.30s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:18,  6.31s/it][A[A

Evaluating:  94%|| 30/32 [03:10<00:12,  6.37s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.42s/it][A[A

Evaluating: 100%|| 32/32 [03:18<00:00,  5.00s/it][A[AEvaluating: 100%|| 32/32 [03:18<00:00,  6.20s/it]
03/11/2022 03:15:59 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 03:15:59 - INFO - __main__ -     acc = 0.866
03/11/2022 03:15:59 - INFO - __main__ -     auc = 0.9141191673568019
03/11/2022 03:15:59 - INFO - __main__ -     f1 = 0.8644185782972593
03/11/2022 03:15:59 - INFO - __main__ -     mcc = 0.7555478242261849
03/11/2022 03:15:59 - INFO - __main__ -     precision = 0.8881427515573856
03/11/2022 03:15:59 - INFO - __main__ -     recall = 0.8676820656848743
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  87%| | 120/138 [1:21:00<24:28, 81.61s/it][A
Iteration:  88%| | 121/138 [1:21:21<17:56, 63.33s/it][A
Iteration:  88%| | 122/138 [1:21:41<13:26, 50.38s/it][A
Iteration:  89%| | 123/138 [1:22:01<10:21, 41.43s/it][A
Iteration:  90%| | 124/138 [1:22:22<08:13, 35.27s/it][A
Iteration:  91%| | 125/138 [1:22:42<06:39, 30.73s/it][A
Iteration:  91%|| 126/138 [1:23:02<05:30, 27.51s/it][A
Iteration:  92%|| 127/138 [1:23:23<04:38, 25.30s/it][A
Iteration:  93%|| 128/138 [1:23:43<03:57, 23.77s/it][A
Iteration:  93%|| 129/138 [1:24:04<03:25, 22.89s/it][A03/11/2022 03:19:23 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 03:19:24 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 03:19:24 - INFO - __main__ -     Num examples = 1000
03/11/2022 03:19:24 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.866, "eval_f1": 0.8644185782972593, "eval_mcc": 0.7555478242261849, "eval_auc": 0.9141191673568019, "eval_precision": 0.8881427515573856, "eval_recall": 0.8676820656848743, "learning_rate": 0.00018357487922705313, "loss": 0.4623028814792633, "step": 120}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.28s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.28s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.28s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.28s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:50,  6.31s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.31s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.29s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.29s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.29s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.28s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:11,  6.28s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.28s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:59,  6.29s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.29s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:46,  6.29s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:42,  6.43s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:36,  6.46s/it][A[A

Evaluating:  56%|    | 18/32 [01:54<01:30,  6.48s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:24,  6.48s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:17,  6.43s/it][A[A

Evaluating:  66%|   | 21/32 [02:13<01:10,  6.39s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:03,  6.37s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:57,  6.35s/it][A[A

Evaluating:  75%|  | 24/32 [02:32<00:50,  6.34s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:44,  6.33s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.33s/it][A[A

Evaluating:  84%| | 27/32 [02:51<00:31,  6.32s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.33s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:18,  6.32s/it][A[A

Evaluating:  94%|| 30/32 [03:10<00:12,  6.31s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.30s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.90s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.19s/it]
03/11/2022 03:22:41 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 03:22:41 - INFO - __main__ -     acc = 0.873
03/11/2022 03:22:41 - INFO - __main__ -     auc = 0.9382038879620406
03/11/2022 03:22:41 - INFO - __main__ -     f1 = 0.8716382097674226
03/11/2022 03:22:41 - INFO - __main__ -     mcc = 0.767968753500854
03/11/2022 03:22:41 - INFO - __main__ -     precision = 0.8935897435897436
03/11/2022 03:22:41 - INFO - __main__ -     recall = 0.8746134242311493
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  94%|| 130/138 [1:27:42<10:52, 81.56s/it][A
Iteration:  95%|| 131/138 [1:28:03<07:22, 63.26s/it][A
Iteration:  96%|| 132/138 [1:28:23<05:02, 50.35s/it][A
Iteration:  96%|| 133/138 [1:28:43<03:26, 41.30s/it][A
Iteration:  97%|| 134/138 [1:29:03<02:19, 34.97s/it][A
Iteration:  98%|| 135/138 [1:29:23<01:31, 30.51s/it][A
Iteration:  99%|| 136/138 [1:29:43<00:54, 27.38s/it][A
Iteration:  99%|| 137/138 [1:30:03<00:25, 25.17s/it][A
Iteration: 100%|| 138/138 [1:30:09<00:00, 19.31s/it][AIteration: 100%|| 138/138 [1:30:09<00:00, 39.20s/it]
Epoch:  20%|        | 1/5 [1:30:09<6:00:38, 5409.52s/it]{"eval_acc": 0.873, "eval_f1": 0.8716382097674226, "eval_mcc": 0.767968753500854, "eval_auc": 0.9382038879620406, "eval_precision": 0.8935897435897436, "eval_recall": 0.8746134242311493, "learning_rate": 0.00018035426731078907, "loss": 0.33002448454499245, "step": 130}

Iteration:   0%|          | 0/138 [00:00<?, ?it/s][A
Iteration:   1%|          | 1/138 [00:20<46:25, 20.33s/it][A03/11/2022 03:25:49 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 03:25:49 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 03:25:49 - INFO - __main__ -     Num examples = 1000
03/11/2022 03:25:49 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:15,  6.30s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.30s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.30s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.30s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.29s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.29s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.29s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.28s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.28s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.28s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:11,  6.28s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.29s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:59,  6.30s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.30s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:46,  6.29s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.30s/it][A[A

Evaluating:  53%|    | 17/32 [01:46<01:34,  6.30s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.29s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.28s/it][A[A

Evaluating:  62%|   | 20/32 [02:05<01:15,  6.28s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.27s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.28s/it][A[A

Evaluating:  72%|  | 23/32 [02:24<00:57,  6.39s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:51,  6.45s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:45,  6.46s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:38,  6.48s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:32,  6.43s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.39s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:19,  6.37s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.35s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.33s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.91s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.18s/it]
03/11/2022 03:29:07 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 03:29:07 - INFO - __main__ -     acc = 0.885
03/11/2022 03:29:07 - INFO - __main__ -     auc = 0.9315465831302935
03/11/2022 03:29:07 - INFO - __main__ -     f1 = 0.884723220452306
03/11/2022 03:29:07 - INFO - __main__ -     mcc = 0.7765293872012585
03/11/2022 03:29:07 - INFO - __main__ -     precision = 0.8906847494976992
03/11/2022 03:29:07 - INFO - __main__ -     recall = 0.8858596284871835
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:   1%|         | 2/138 [03:58<5:09:36, 136.59s/it][A
Iteration:   2%|         | 3/138 [04:18<3:08:06, 83.60s/it] [A
Iteration:   3%|         | 4/138 [04:39<2:10:45, 58.55s/it][A
Iteration:   4%|         | 5/138 [04:59<1:39:17, 44.80s/it][A
Iteration:   4%|         | 6/138 [05:20<1:20:28, 36.58s/it][A
Iteration:   5%|         | 7/138 [05:40<1:08:00, 31.15s/it][A
Iteration:   6%|         | 8/138 [05:59<59:46, 27.58s/it]  [A
Iteration:   7%|         | 9/138 [06:19<54:12, 25.22s/it][A
Iteration:   7%|         | 10/138 [06:39<50:19, 23.59s/it][A
Iteration:   8%|         | 11/138 [06:59<47:33, 22.47s/it][A03/11/2022 03:32:29 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 03:32:29 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 03:32:29 - INFO - __main__ -     Num examples = 1000
03/11/2022 03:32:29 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.885, "eval_f1": 0.884723220452306, "eval_mcc": 0.7765293872012585, "eval_auc": 0.9315465831302935, "eval_precision": 0.8906847494976992, "eval_recall": 0.8858596284871835, "learning_rate": 0.00017713365539452497, "loss": 0.307803962379694, "step": 140}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:21,  6.49s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:10,  6.35s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.31s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.29s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.29s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.28s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:36,  6.27s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.26s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:23,  6.26s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:17,  6.25s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:11,  6.25s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:04,  6.25s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:58,  6.26s/it][A[A

Evaluating:  44%|     | 14/32 [01:27<01:52,  6.26s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:46,  6.26s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.25s/it][A[A

Evaluating:  53%|    | 17/32 [01:46<01:33,  6.26s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:29,  6.40s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:23,  6.43s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:17,  6.44s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:11,  6.46s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:03,  6.40s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:57,  6.35s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.32s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:44,  6.29s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.28s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.27s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.26s/it][A[A

Evaluating:  91%| | 29/32 [03:02<00:18,  6.28s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.28s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.27s/it][A[A

Evaluating: 100%|| 32/32 [03:16<00:00,  4.87s/it][A[AEvaluating: 100%|| 32/32 [03:16<00:00,  6.16s/it]
03/11/2022 03:35:46 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 03:35:46 - INFO - __main__ -     acc = 0.898
03/11/2022 03:35:46 - INFO - __main__ -     auc = 0.9379798440494337
03/11/2022 03:35:46 - INFO - __main__ -     f1 = 0.8978819515359755
03/11/2022 03:35:46 - INFO - __main__ -     mcc = 0.7999199990548741
03/11/2022 03:35:46 - INFO - __main__ -     precision = 0.9012721539632489
03/11/2022 03:35:46 - INFO - __main__ -     recall = 0.8986521358186205
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:   9%|         | 12/138 [10:37<2:52:05, 81.95s/it][A
Iteration:   9%|         | 13/138 [10:58<2:11:51, 63.29s/it][A
Iteration:  10%|         | 14/138 [11:18<1:43:52, 50.26s/it][A
Iteration:  11%|         | 15/138 [11:38<1:24:25, 41.18s/it][A
Iteration:  12%|        | 16/138 [11:58<1:10:46, 34.81s/it][A
Iteration:  12%|        | 17/138 [12:18<1:01:14, 30.36s/it][A
Iteration:  13%|        | 18/138 [12:38<54:33, 27.28s/it]  [A
Iteration:  14%|        | 19/138 [12:58<49:49, 25.12s/it][A
Iteration:  14%|        | 20/138 [13:18<46:21, 23.57s/it][A
Iteration:  15%|        | 21/138 [13:38<43:50, 22.48s/it][A03/11/2022 03:39:07 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 03:39:07 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 03:39:07 - INFO - __main__ -     Num examples = 1000
03/11/2022 03:39:07 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.898, "eval_f1": 0.8978819515359755, "eval_mcc": 0.7999199990548741, "eval_auc": 0.9379798440494337, "eval_precision": 0.9012721539632489, "eval_recall": 0.8986521358186205, "learning_rate": 0.00017391304347826088, "loss": 0.24686373323202132, "step": 150}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:13,  6.26s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:07,  6.26s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.28s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.28s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.29s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.28s/it][A[A

Evaluating:  22%|       | 7/32 [00:43<02:36,  6.27s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.27s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.27s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:17,  6.27s/it][A[A

Evaluating:  34%|      | 11/32 [01:08<02:11,  6.27s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.27s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:59,  6.28s/it][A[A

Evaluating:  44%|     | 14/32 [01:27<01:52,  6.27s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:46,  6.27s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.27s/it][A[A

Evaluating:  53%|    | 17/32 [01:46<01:34,  6.28s/it][A[A

Evaluating:  56%|    | 18/32 [01:52<01:27,  6.28s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.27s/it][A[A

Evaluating:  62%|   | 20/32 [02:05<01:15,  6.27s/it][A[A

Evaluating:  66%|   | 21/32 [02:11<01:09,  6.28s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:03,  6.33s/it][A[A

Evaluating:  72%|  | 23/32 [02:24<00:57,  6.41s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:51,  6.45s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:45,  6.47s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:38,  6.45s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.39s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.36s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:19,  6.35s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.34s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.33s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.92s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.17s/it]
03/11/2022 03:42:25 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 03:42:25 - INFO - __main__ -     acc = 0.91
03/11/2022 03:42:25 - INFO - __main__ -     auc = 0.9485539165676472
03/11/2022 03:42:25 - INFO - __main__ -     f1 = 0.9099077455314242
03/11/2022 03:42:25 - INFO - __main__ -     mcc = 0.8236819420309681
03/11/2022 03:42:25 - INFO - __main__ -     precision = 0.913059051267914
03/11/2022 03:42:25 - INFO - __main__ -     recall = 0.910626482790627
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  16%|        | 22/138 [17:16<2:36:34, 80.98s/it][A
Iteration:  17%|        | 23/138 [17:36<2:00:24, 62.82s/it][A
Iteration:  17%|        | 24/138 [17:56<1:35:00, 50.01s/it][A
Iteration:  18%|        | 25/138 [18:16<1:17:15, 41.03s/it][A
Iteration:  19%|        | 26/138 [18:37<1:05:11, 34.92s/it][A
Iteration:  20%|        | 27/138 [18:57<56:17, 30.42s/it]  [A
Iteration:  20%|        | 28/138 [19:17<50:00, 27.28s/it][A
Iteration:  21%|        | 29/138 [19:37<45:39, 25.13s/it][A
Iteration:  22%|       | 30/138 [19:57<42:29, 23.61s/it][A
Iteration:  22%|       | 31/138 [20:17<40:11, 22.54s/it][A03/11/2022 03:45:46 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 03:45:46 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 03:45:46 - INFO - __main__ -     Num examples = 1000
03/11/2022 03:45:46 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.91, "eval_f1": 0.9099077455314242, "eval_mcc": 0.8236819420309681, "eval_auc": 0.9485539165676472, "eval_precision": 0.913059051267914, "eval_recall": 0.910626482790627, "learning_rate": 0.0001706924315619968, "loss": 0.24894209206104279, "step": 160}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.28s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.27s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.29s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.31s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:50,  6.30s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.30s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.29s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.29s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.29s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.28s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:11,  6.28s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.30s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:59,  6.29s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.28s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:46,  6.29s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.30s/it][A[A

Evaluating:  53%|    | 17/32 [01:46<01:34,  6.31s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:29,  6.40s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:23,  6.44s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:17,  6.42s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:10,  6.38s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:03,  6.35s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:57,  6.34s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.32s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:44,  6.32s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.31s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.32s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.33s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:18,  6.33s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.32s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.31s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.90s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.17s/it]
03/11/2022 03:49:04 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 03:49:04 - INFO - __main__ -     acc = 0.892
03/11/2022 03:49:04 - INFO - __main__ -     auc = 0.9501302255242028
03/11/2022 03:49:04 - INFO - __main__ -     f1 = 0.8919567827130852
03/11/2022 03:49:04 - INFO - __main__ -     mcc = 0.78397783692531
03/11/2022 03:49:04 - INFO - __main__ -     precision = 0.892083048140543
03/11/2022 03:49:04 - INFO - __main__ -     recall = 0.8918948113830312
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  23%|       | 32/138 [23:55<2:23:12, 81.06s/it][A
Iteration:  24%|       | 33/138 [24:15<1:50:07, 62.92s/it][A
Iteration:  25%|       | 34/138 [24:35<1:26:54, 50.14s/it][A
Iteration:  25%|       | 35/138 [24:56<1:10:39, 41.16s/it][A
Iteration:  26%|       | 36/138 [25:16<59:17, 34.88s/it]  [A
Iteration:  27%|       | 37/138 [25:37<51:30, 30.60s/it][A
Iteration:  28%|       | 38/138 [25:58<46:13, 27.74s/it][A
Iteration:  28%|       | 39/138 [26:18<42:00, 25.46s/it][A
Iteration:  29%|       | 40/138 [26:38<38:56, 23.84s/it][A
Iteration:  30%|       | 41/138 [26:58<36:46, 22.74s/it][A03/11/2022 03:52:27 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 03:52:27 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 03:52:27 - INFO - __main__ -     Num examples = 1000
03/11/2022 03:52:27 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.892, "eval_f1": 0.8919567827130852, "eval_mcc": 0.78397783692531, "eval_auc": 0.9501302255242028, "eval_precision": 0.892083048140543, "eval_recall": 0.8918948113830312, "learning_rate": 0.0001674718196457327, "loss": 0.19902635514736175, "step": 170}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:15,  6.31s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:09,  6.30s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:03,  6.32s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.32s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:50,  6.32s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:44,  6.31s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.31s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:31,  6.31s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:25,  6.31s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:18,  6.31s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.33s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:06,  6.34s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:00,  6.33s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.32s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:47,  6.32s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:40,  6.31s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:34,  6.30s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.31s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.31s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.29s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.29s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.28s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:57,  6.37s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:51,  6.42s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:44,  6.39s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:38,  6.37s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.35s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.32s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:18,  6.31s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.29s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.28s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.88s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.17s/it]
03/11/2022 03:55:45 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 03:55:45 - INFO - __main__ -     acc = 0.891
03/11/2022 03:55:45 - INFO - __main__ -     auc = 0.929750231045285
03/11/2022 03:55:45 - INFO - __main__ -     f1 = 0.8901686867903158
03/11/2022 03:55:45 - INFO - __main__ -     mcc = 0.7989550501925382
03/11/2022 03:55:45 - INFO - __main__ -     precision = 0.9066900532417774
03/11/2022 03:55:45 - INFO - __main__ -     recall = 0.892392909010166
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  30%|       | 42/138 [30:36<2:10:02, 81.28s/it][A
Iteration:  31%|       | 43/138 [30:56<1:39:50, 63.06s/it][A
Iteration:  32%|      | 44/138 [31:17<1:18:40, 50.21s/it][A
Iteration:  33%|      | 45/138 [31:37<1:03:48, 41.16s/it][A
Iteration:  33%|      | 46/138 [31:58<53:47, 35.09s/it]  [A
Iteration:  34%|      | 47/138 [32:18<46:33, 30.70s/it][A
Iteration:  35%|      | 48/138 [32:38<41:17, 27.53s/it][A
Iteration:  36%|      | 49/138 [32:59<37:57, 25.59s/it][A
Iteration:  36%|      | 50/138 [33:19<35:09, 23.97s/it][A
Iteration:  37%|      | 51/138 [33:40<33:03, 22.80s/it][A03/11/2022 03:59:09 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 03:59:09 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 03:59:09 - INFO - __main__ -     Num examples = 1000
03/11/2022 03:59:09 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.891, "eval_f1": 0.8901686867903158, "eval_mcc": 0.7989550501925382, "eval_auc": 0.929750231045285, "eval_precision": 0.9066900532417774, "eval_recall": 0.892392909010166, "learning_rate": 0.0001642512077294686, "loss": 0.17931469334289432, "step": 180}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.28s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.30s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.29s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.28s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.28s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.28s/it][A[A

Evaluating:  22%|       | 7/32 [00:43<02:37,  6.28s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.29s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.30s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.31s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.30s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.30s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:59,  6.30s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.30s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:46,  6.29s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.29s/it][A[A

Evaluating:  53%|    | 17/32 [01:46<01:34,  6.29s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.30s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.29s/it][A[A

Evaluating:  62%|   | 20/32 [02:05<01:15,  6.28s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.28s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.28s/it][A[A

Evaluating:  72%|  | 23/32 [02:24<00:56,  6.29s/it][A[A

Evaluating:  75%|  | 24/32 [02:30<00:50,  6.29s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:44,  6.30s/it][A[A

Evaluating:  81%| | 26/32 [02:43<00:37,  6.31s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.39s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.45s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:19,  6.47s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.49s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.46s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  5.01s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.18s/it]
03/11/2022 04:02:27 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 04:02:27 - INFO - __main__ -     acc = 0.823
03/11/2022 04:02:27 - INFO - __main__ -     auc = 0.8885341526939281
03/11/2022 04:02:27 - INFO - __main__ -     f1 = 0.8191986516509615
03/11/2022 04:02:27 - INFO - __main__ -     mcc = 0.6826270876145141
03/11/2022 04:02:27 - INFO - __main__ -     precision = 0.8582346096890205
03/11/2022 04:02:27 - INFO - __main__ -     recall = 0.8251917375805657
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  38%|      | 52/138 [37:18<1:56:38, 81.37s/it][A
Iteration:  38%|      | 53/138 [37:38<1:29:25, 63.13s/it][A
Iteration:  39%|      | 54/138 [37:58<1:10:18, 50.22s/it][A
Iteration:  40%|      | 55/138 [38:18<56:57, 41.18s/it]  [A
Iteration:  41%|      | 56/138 [38:38<47:34, 34.81s/it][A
Iteration:  41%|     | 57/138 [38:59<41:07, 30.46s/it][A
Iteration:  42%|     | 58/138 [39:19<36:35, 27.45s/it][A
Iteration:  43%|     | 59/138 [39:39<33:12, 25.22s/it][A
Iteration:  43%|     | 60/138 [39:59<30:45, 23.66s/it][A
Iteration:  44%|     | 61/138 [40:19<28:57, 22.57s/it][A03/11/2022 04:05:48 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 04:05:48 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 04:05:48 - INFO - __main__ -     Num examples = 1000
03/11/2022 04:05:48 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.823, "eval_f1": 0.8191986516509615, "eval_mcc": 0.6826270876145141, "eval_auc": 0.8885341526939281, "eval_precision": 0.8582346096890205, "eval_recall": 0.8251917375805657, "learning_rate": 0.00016103059581320451, "loss": 0.3310727223753929, "step": 190}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:15,  6.32s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:14,  6.48s/it][A[A

Evaluating:   9%|         | 3/32 [00:19<03:08,  6.49s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<03:00,  6.44s/it][A[A

Evaluating:  16%|        | 5/32 [00:32<02:52,  6.38s/it][A[A

Evaluating:  19%|        | 6/32 [00:38<02:45,  6.35s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:38,  6.33s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:31,  6.31s/it][A[A

Evaluating:  28%|       | 9/32 [00:57<02:25,  6.31s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:18,  6.31s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.30s/it][A[A

Evaluating:  38%|      | 12/32 [01:16<02:07,  6.39s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:02,  6.44s/it][A[A

Evaluating:  44%|     | 14/32 [01:29<01:55,  6.42s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:48,  6.38s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:41,  6.35s/it][A[A

Evaluating:  53%|    | 17/32 [01:48<01:35,  6.34s/it][A[A

Evaluating:  56%|    | 18/32 [01:54<01:28,  6.32s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:22,  6.31s/it][A[A

Evaluating:  62%|   | 20/32 [02:07<01:15,  6.30s/it][A[A

Evaluating:  66%|   | 21/32 [02:13<01:09,  6.29s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:02,  6.29s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.28s/it][A[A

Evaluating:  75%|  | 24/32 [02:32<00:50,  6.30s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:44,  6.31s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.30s/it][A[A

Evaluating:  84%| | 27/32 [02:51<00:31,  6.29s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.29s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:18,  6.29s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.28s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.28s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.88s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.18s/it]
03/11/2022 04:09:06 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 04:09:06 - INFO - __main__ -     acc = 0.908
03/11/2022 04:09:06 - INFO - __main__ -     auc = 0.9473096726958485
03/11/2022 04:09:06 - INFO - __main__ -     f1 = 0.9075974946868559
03/11/2022 04:09:06 - INFO - __main__ -     mcc = 0.8269856706283684
03/11/2022 04:09:06 - INFO - __main__ -     precision = 0.9179306088584263
03/11/2022 04:09:06 - INFO - __main__ -     recall = 0.9091021840280695
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  45%|     | 62/138 [43:57<1:42:49, 81.18s/it][A
Iteration:  46%|     | 63/138 [44:17<1:18:43, 62.98s/it][A
Iteration:  46%|     | 64/138 [44:38<1:01:49, 50.13s/it][A
Iteration:  47%|     | 65/138 [44:58<50:00, 41.11s/it]  [A
Iteration:  48%|     | 66/138 [45:18<41:45, 34.80s/it][A
Iteration:  49%|     | 67/138 [45:38<35:58, 30.40s/it][A
Iteration:  49%|     | 68/138 [45:58<31:55, 27.36s/it][A
Iteration:  50%|     | 69/138 [46:19<29:11, 25.39s/it][A
Iteration:  51%|     | 70/138 [46:39<26:59, 23.82s/it][A
Iteration:  51%|    | 71/138 [46:59<25:23, 22.73s/it][A03/11/2022 04:12:28 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 04:12:28 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 04:12:28 - INFO - __main__ -     Num examples = 1000
03/11/2022 04:12:28 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.908, "eval_f1": 0.9075974946868559, "eval_mcc": 0.8269856706283684, "eval_auc": 0.9473096726958485, "eval_precision": 0.9179306088584263, "eval_recall": 0.9091021840280695, "learning_rate": 0.00015780998389694042, "loss": 0.2898297056555748, "step": 200}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:15,  6.30s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.29s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.28s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.28s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.28s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.28s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:38,  6.33s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:34,  6.46s/it][A[A

Evaluating:  28%|       | 9/32 [00:57<02:29,  6.49s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:22,  6.50s/it][A[A

Evaluating:  34%|      | 11/32 [01:10<02:15,  6.47s/it][A[A

Evaluating:  38%|      | 12/32 [01:16<02:08,  6.42s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:01,  6.38s/it][A[A

Evaluating:  44%|     | 14/32 [01:29<01:54,  6.35s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:47,  6.33s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:41,  6.33s/it][A[A

Evaluating:  53%|    | 17/32 [01:48<01:34,  6.32s/it][A[A

Evaluating:  56%|    | 18/32 [01:54<01:28,  6.31s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:21,  6.30s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.30s/it][A[A

Evaluating:  66%|   | 21/32 [02:13<01:09,  6.30s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:02,  6.30s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.29s/it][A[A

Evaluating:  75%|  | 24/32 [02:32<00:50,  6.30s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:44,  6.30s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.29s/it][A[A

Evaluating:  84%| | 27/32 [02:51<00:31,  6.29s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.29s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:18,  6.28s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.28s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.29s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.89s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.18s/it]
03/11/2022 04:15:46 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 04:15:46 - INFO - __main__ -     acc = 0.819
03/11/2022 04:15:46 - INFO - __main__ -     auc = 0.9518305587895227
03/11/2022 04:15:46 - INFO - __main__ -     f1 = 0.8157406361121885
03/11/2022 04:15:46 - INFO - __main__ -     mcc = 0.6556952877932505
03/11/2022 04:15:46 - INFO - __main__ -     precision = 0.8387478430233551
03/11/2022 04:15:46 - INFO - __main__ -     recall = 0.8172981904453274
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  52%|    | 72/138 [50:37<1:29:24, 81.29s/it][A
Iteration:  53%|    | 73/138 [50:58<1:08:20, 63.08s/it][A
Iteration:  54%|    | 74/138 [51:18<53:34, 50.22s/it]  [A
Iteration:  54%|    | 75/138 [51:38<43:16, 41.21s/it][A
Iteration:  55%|    | 76/138 [51:58<36:05, 34.93s/it][A
Iteration:  56%|    | 77/138 [52:19<31:11, 30.67s/it][A
Iteration:  57%|    | 78/138 [52:40<27:35, 27.59s/it][A
Iteration:  57%|    | 79/138 [53:00<24:53, 25.31s/it][A
Iteration:  58%|    | 80/138 [53:20<22:58, 23.76s/it][A
Iteration:  59%|    | 81/138 [53:40<21:28, 22.61s/it][A03/11/2022 04:19:09 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 04:19:09 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 04:19:09 - INFO - __main__ -     Num examples = 1000
03/11/2022 04:19:09 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.819, "eval_f1": 0.8157406361121885, "eval_mcc": 0.6556952877932505, "eval_auc": 0.9518305587895227, "eval_precision": 0.8387478430233551, "eval_recall": 0.8172981904453274, "learning_rate": 0.00015458937198067633, "loss": 0.4069671183824539, "step": 210}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.28s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.27s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:01,  6.28s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.27s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.27s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.28s/it][A[A

Evaluating:  22%|       | 7/32 [00:43<02:37,  6.30s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:31,  6.30s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.29s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.29s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.29s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:06,  6.33s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:02,  6.43s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:56,  6.46s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:50,  6.50s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:43,  6.47s/it][A[A

Evaluating:  53%|    | 17/32 [01:48<01:36,  6.41s/it][A[A

Evaluating:  56%|    | 18/32 [01:54<01:29,  6.37s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:22,  6.35s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:16,  6.34s/it][A[A

Evaluating:  66%|   | 21/32 [02:13<01:09,  6.32s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:03,  6.31s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.31s/it][A[A

Evaluating:  75%|  | 24/32 [02:32<00:50,  6.30s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:44,  6.29s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.29s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.29s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.28s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:18,  6.28s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.29s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.44s/it][A[A

Evaluating: 100%|| 32/32 [03:18<00:00,  5.02s/it][A[AEvaluating: 100%|| 32/32 [03:18<00:00,  6.20s/it]
03/11/2022 04:22:27 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 04:22:27 - INFO - __main__ -     acc = 0.92
03/11/2022 04:22:27 - INFO - __main__ -     auc = 0.9576316958123794
03/11/2022 04:22:27 - INFO - __main__ -     f1 = 0.9199612612504452
03/11/2022 04:22:27 - INFO - __main__ -     mcc = 0.8423123746162299
03/11/2022 04:22:27 - INFO - __main__ -     precision = 0.9218250193651443
03/11/2022 04:22:27 - INFO - __main__ -     recall = 0.920488415729483
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  59%|    | 82/138 [57:18<1:15:53, 81.32s/it][A
Iteration:  60%|    | 83/138 [57:39<57:59, 63.26s/it]  [A
Iteration:  61%|    | 84/138 [57:59<45:17, 50.32s/it][A
Iteration:  62%|   | 85/138 [58:19<36:27, 41.26s/it][A
Iteration:  62%|   | 86/138 [58:39<30:14, 34.90s/it][A
Iteration:  63%|   | 87/138 [58:59<25:52, 30.43s/it][A
Iteration:  64%|   | 88/138 [59:19<22:45, 27.30s/it][A
Iteration:  64%|   | 89/138 [59:40<20:32, 25.14s/it][A
Iteration:  65%|   | 90/138 [59:59<18:51, 23.58s/it][A
Iteration:  66%|   | 91/138 [1:00:19<17:36, 22.48s/it][A03/11/2022 04:25:48 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 04:25:48 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 04:25:48 - INFO - __main__ -     Num examples = 1000
03/11/2022 04:25:48 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.92, "eval_f1": 0.9199612612504452, "eval_mcc": 0.8423123746162299, "eval_auc": 0.9576316958123794, "eval_precision": 0.9218250193651443, "eval_recall": 0.920488415729483, "learning_rate": 0.00015136876006441224, "loss": 0.2025081504136324, "step": 220}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.28s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.29s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.29s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.29s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.28s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:44,  6.32s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:41,  6.44s/it][A[A

Evaluating:  25%|       | 8/32 [00:51<02:35,  6.46s/it][A[A

Evaluating:  28%|       | 9/32 [00:57<02:29,  6.48s/it][A[A

Evaluating:  31%|      | 10/32 [01:04<02:22,  6.47s/it][A[A

Evaluating:  34%|      | 11/32 [01:10<02:14,  6.41s/it][A[A

Evaluating:  38%|      | 12/32 [01:16<02:07,  6.37s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:00,  6.34s/it][A[A

Evaluating:  44%|     | 14/32 [01:29<01:53,  6.33s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:47,  6.33s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:41,  6.32s/it][A[A

Evaluating:  53%|    | 17/32 [01:48<01:34,  6.31s/it][A[A

Evaluating:  56%|    | 18/32 [01:54<01:28,  6.30s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:21,  6.29s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.28s/it][A[A

Evaluating:  66%|   | 21/32 [02:13<01:09,  6.28s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:02,  6.29s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.29s/it][A[A

Evaluating:  75%|  | 24/32 [02:32<00:50,  6.29s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:43,  6.28s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.28s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.28s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.29s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:18,  6.29s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.30s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.29s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.89s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.18s/it]
03/11/2022 04:29:06 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 04:29:06 - INFO - __main__ -     acc = 0.917
03/11/2022 04:29:06 - INFO - __main__ -     auc = 0.9559033570579833
03/11/2022 04:29:06 - INFO - __main__ -     f1 = 0.9169560697609035
03/11/2022 04:29:06 - INFO - __main__ -     mcc = 0.8364287555678038
03/11/2022 04:29:06 - INFO - __main__ -     precision = 0.9189281413087114
03/11/2022 04:29:06 - INFO - __main__ -     recall = 0.9175018303587503
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  67%|   | 92/138 [1:03:57<1:02:09, 81.07s/it][A
Iteration:  67%|   | 93/138 [1:04:18<47:10, 62.91s/it]  [A
Iteration:  68%|   | 94/138 [1:04:39<36:54, 50.34s/it][A
Iteration:  69%|   | 95/138 [1:04:59<29:39, 41.38s/it][A
Iteration:  70%|   | 96/138 [1:05:19<24:27, 34.95s/it][A
Iteration:  70%|   | 97/138 [1:05:39<20:48, 30.45s/it][A
Iteration:  71%|   | 98/138 [1:05:59<18:12, 27.31s/it][A
Iteration:  72%|  | 99/138 [1:06:19<16:21, 25.16s/it][A
Iteration:  72%|  | 100/138 [1:06:40<15:02, 23.75s/it][A
Iteration:  73%|  | 101/138 [1:07:00<13:58, 22.65s/it][A03/11/2022 04:32:29 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 04:32:29 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 04:32:29 - INFO - __main__ -     Num examples = 1000
03/11/2022 04:32:29 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.917, "eval_f1": 0.9169560697609035, "eval_mcc": 0.8364287555678038, "eval_auc": 0.9559033570579833, "eval_precision": 0.9189281413087114, "eval_recall": 0.9175018303587503, "learning_rate": 0.00014814814814814815, "loss": 0.17833098731935024, "step": 230}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:15,  6.29s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:09,  6.30s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.29s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.29s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.29s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.30s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.29s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.29s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.28s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.28s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:11,  6.28s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:06,  6.33s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:01,  6.41s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:55,  6.43s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:48,  6.39s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:41,  6.36s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:35,  6.34s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.32s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:21,  6.30s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.30s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.30s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.30s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.29s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.29s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:43,  6.29s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.30s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.31s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.32s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:18,  6.33s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.33s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.32s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.93s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.17s/it]
03/11/2022 04:35:46 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 04:35:46 - INFO - __main__ -     acc = 0.921
03/11/2022 04:35:46 - INFO - __main__ -     auc = 0.9632007873543215
03/11/2022 04:35:46 - INFO - __main__ -     f1 = 0.920999920999921
03/11/2022 04:35:46 - INFO - __main__ -     mcc = 0.842414386030415
03/11/2022 04:35:46 - INFO - __main__ -     precision = 0.9212198322770629
03/11/2022 04:35:46 - INFO - __main__ -     recall = 0.92119455413261
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  74%|  | 102/138 [1:10:37<48:41, 81.14s/it][A
Iteration:  75%|  | 103/138 [1:10:59<36:53, 63.25s/it][A
Iteration:  75%|  | 104/138 [1:11:19<28:33, 50.40s/it][A
Iteration:  76%|  | 105/138 [1:11:39<22:44, 41.33s/it][A
Iteration:  77%|  | 106/138 [1:12:00<18:38, 34.95s/it][A
Iteration:  78%|  | 107/138 [1:12:20<15:44, 30.47s/it][A
Iteration:  78%|  | 108/138 [1:12:40<13:40, 27.34s/it][A
Iteration:  79%|  | 109/138 [1:13:00<12:09, 25.17s/it][A
Iteration:  80%|  | 110/138 [1:13:20<11:01, 23.61s/it][A
Iteration:  80%|  | 111/138 [1:13:40<10:08, 22.54s/it][A03/11/2022 04:39:09 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 04:39:09 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 04:39:09 - INFO - __main__ -     Num examples = 1000
03/11/2022 04:39:09 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.921, "eval_f1": 0.920999920999921, "eval_mcc": 0.842414386030415, "eval_auc": 0.9632007873543215, "eval_precision": 0.9212198322770629, "eval_recall": 0.92119455413261, "learning_rate": 0.00014492753623188405, "loss": 0.15249179750680925, "step": 240}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.26s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:07,  6.27s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:01,  6.26s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.26s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.28s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.28s/it][A[A

Evaluating:  22%|       | 7/32 [00:43<02:36,  6.28s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.28s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.29s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.29s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:11,  6.28s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.27s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:59,  6.30s/it][A[A

Evaluating:  44%|     | 14/32 [01:27<01:53,  6.30s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:46,  6.29s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.29s/it][A[A

Evaluating:  53%|    | 17/32 [01:46<01:35,  6.34s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:29,  6.41s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:23,  6.42s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:16,  6.37s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.35s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:03,  6.33s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.31s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.30s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:44,  6.30s/it][A[A

Evaluating:  81%| | 26/32 [02:43<00:37,  6.30s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.29s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.30s/it][A[A

Evaluating:  91%| | 29/32 [03:02<00:18,  6.31s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.31s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.31s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.90s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.16s/it]
03/11/2022 04:42:26 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 04:42:26 - INFO - __main__ -     acc = 0.924
03/11/2022 04:42:26 - INFO - __main__ -     auc = 0.9603722329576597
03/11/2022 04:42:26 - INFO - __main__ -     f1 = 0.923969587835134
03/11/2022 04:42:26 - INFO - __main__ -     mcc = 0.8500795110983274
03/11/2022 04:42:26 - INFO - __main__ -     precision = 0.9256191053110872
03/11/2022 04:42:26 - INFO - __main__ -     recall = 0.9244611943941012
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  81%|  | 112/138 [1:17:17<35:04, 80.95s/it][A
Iteration:  82%| | 113/138 [1:17:38<26:11, 62.84s/it][A
Iteration:  83%| | 114/138 [1:17:59<20:08, 50.35s/it][A
Iteration:  83%| | 115/138 [1:18:19<15:50, 41.32s/it][A
Iteration:  84%| | 116/138 [1:18:39<12:48, 34.95s/it][A
Iteration:  85%| | 117/138 [1:18:59<10:40, 30.49s/it][A
Iteration:  86%| | 118/138 [1:19:19<09:06, 27.35s/it][A
Iteration:  86%| | 119/138 [1:19:39<07:57, 25.14s/it][A
Iteration:  87%| | 120/138 [1:20:00<07:08, 23.81s/it][A
Iteration:  88%| | 121/138 [1:20:20<06:28, 22.85s/it][A03/11/2022 04:45:50 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 04:45:50 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 04:45:50 - INFO - __main__ -     Num examples = 1000
03/11/2022 04:45:50 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.924, "eval_f1": 0.923969587835134, "eval_mcc": 0.8500795110983274, "eval_auc": 0.9603722329576597, "eval_precision": 0.9256191053110872, "eval_recall": 0.9244611943941012, "learning_rate": 0.00014170692431562, "loss": 0.15988330990076066, "step": 250}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.29s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.27s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:01,  6.27s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.28s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.27s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:42,  6.27s/it][A[A

Evaluating:  22%|       | 7/32 [00:43<02:36,  6.27s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.26s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.27s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.27s/it][A[A

Evaluating:  34%|      | 11/32 [01:08<02:11,  6.27s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:07,  6.38s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:02,  6.44s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:56,  6.45s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:50,  6.47s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:42,  6.43s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:35,  6.38s/it][A[A

Evaluating:  56%|    | 18/32 [01:54<01:28,  6.34s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:22,  6.33s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.32s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.30s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:03,  6.30s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.29s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.28s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:43,  6.28s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.27s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.28s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.28s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:18,  6.27s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.30s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.38s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.97s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.18s/it]
03/11/2022 04:49:07 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 04:49:07 - INFO - __main__ -     acc = 0.933
03/11/2022 04:49:07 - INFO - __main__ -     auc = 0.9524586819016528
03/11/2022 04:49:07 - INFO - __main__ -     f1 = 0.9329758042653398
03/11/2022 04:49:07 - INFO - __main__ -     mcc = 0.8679872649874391
03/11/2022 04:49:07 - INFO - __main__ -     precision = 0.9345389934382571
03/11/2022 04:49:07 - INFO - __main__ -     recall = 0.9334489559953751
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  88%| | 122/138 [1:23:58<21:41, 81.35s/it][A
Iteration:  89%| | 123/138 [1:24:19<15:47, 63.17s/it][A
Iteration:  90%| | 124/138 [1:24:39<11:43, 50.27s/it][A
Iteration:  91%| | 125/138 [1:25:00<08:57, 41.33s/it][A
Iteration:  91%|| 126/138 [1:25:20<07:01, 35.13s/it][A
Iteration:  92%|| 127/138 [1:25:40<05:36, 30.62s/it][A
Iteration:  93%|| 128/138 [1:26:01<04:34, 27.45s/it][A
Iteration:  93%|| 129/138 [1:26:21<03:46, 25.21s/it][A
Iteration:  94%|| 130/138 [1:26:41<03:09, 23.65s/it][A
Iteration:  95%|| 131/138 [1:27:01<02:38, 22.63s/it][A03/11/2022 04:52:31 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 04:52:31 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 04:52:31 - INFO - __main__ -     Num examples = 1000
03/11/2022 04:52:31 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.933, "eval_f1": 0.9329758042653398, "eval_mcc": 0.8679872649874391, "eval_auc": 0.9524586819016528, "eval_precision": 0.9345389934382571, "eval_recall": 0.9334489559953751, "learning_rate": 0.00013848631239935587, "loss": 0.21095541212707758, "step": 260}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:17,  6.38s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:09,  6.32s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:03,  6.32s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.30s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.29s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.28s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.28s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.28s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.28s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.29s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.30s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.30s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:59,  6.29s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.29s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:47,  6.30s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:42,  6.38s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:36,  6.42s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:29,  6.39s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:22,  6.36s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:16,  6.33s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.31s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:03,  6.31s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.31s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.30s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:44,  6.30s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.30s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.30s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.29s/it][A[A

Evaluating:  91%| | 29/32 [03:02<00:18,  6.29s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.28s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.28s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.88s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.16s/it]
03/11/2022 04:55:48 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 04:55:48 - INFO - __main__ -     acc = 0.91
03/11/2022 04:55:48 - INFO - __main__ -     auc = 0.9515865109561474
03/11/2022 04:55:48 - INFO - __main__ -     f1 = 0.9096748293857888
03/11/2022 04:55:48 - INFO - __main__ -     mcc = 0.8294369725279204
03/11/2022 04:55:48 - INFO - __main__ -     precision = 0.9184517190675774
03/11/2022 04:55:48 - INFO - __main__ -     recall = 0.911018559637689
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  96%|| 132/138 [1:30:39<08:07, 81.28s/it][A
Iteration:  96%|| 133/138 [1:31:00<05:15, 63.12s/it][A
Iteration:  97%|| 134/138 [1:31:21<03:22, 50.53s/it][A
Iteration:  98%|| 135/138 [1:31:41<02:04, 41.44s/it][A
Iteration:  99%|| 136/138 [1:32:01<01:10, 35.03s/it][A
Iteration:  99%|| 137/138 [1:32:21<00:30, 30.57s/it][A
Iteration: 100%|| 138/138 [1:32:27<00:00, 23.01s/it][AIteration: 100%|| 138/138 [1:32:27<00:00, 40.20s/it]
Epoch:  40%|      | 2/5 [3:02:36<4:34:31, 5490.46s/it]{"eval_acc": 0.91, "eval_f1": 0.9096748293857888, "eval_mcc": 0.8294369725279204, "eval_auc": 0.9515865109561474, "eval_precision": 0.9184517190675774, "eval_recall": 0.911018559637689, "learning_rate": 0.0001352657004830918, "loss": 0.1540957920253277, "step": 270}

Iteration:   0%|          | 0/138 [00:00<?, ?it/s][A
Iteration:   1%|          | 1/138 [00:20<45:51, 20.09s/it][A
Iteration:   1%|         | 2/138 [00:40<45:24, 20.03s/it][A
Iteration:   2%|         | 3/138 [01:00<45:05, 20.04s/it][A03/11/2022 04:58:56 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 04:58:56 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 04:58:56 - INFO - __main__ -     Num examples = 1000
03/11/2022 04:58:56 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.27s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.27s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:01,  6.27s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.27s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.28s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.28s/it][A[A

Evaluating:  22%|       | 7/32 [00:43<02:36,  6.27s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.27s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.27s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:17,  6.27s/it][A[A

Evaluating:  34%|      | 11/32 [01:08<02:11,  6.27s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.29s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:59,  6.30s/it][A[A

Evaluating:  44%|     | 14/32 [01:27<01:53,  6.29s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:46,  6.28s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.28s/it][A[A

Evaluating:  53%|    | 17/32 [01:46<01:34,  6.27s/it][A[A

Evaluating:  56%|    | 18/32 [01:52<01:27,  6.27s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.27s/it][A[A

Evaluating:  62%|   | 20/32 [02:05<01:15,  6.28s/it][A[A

Evaluating:  66%|   | 21/32 [02:11<01:09,  6.27s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.27s/it][A[A

Evaluating:  72%|  | 23/32 [02:24<00:57,  6.35s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:51,  6.42s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:45,  6.49s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:38,  6.50s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:32,  6.49s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.44s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:19,  6.39s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.35s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.33s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.91s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.17s/it]
03/11/2022 05:02:13 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 05:02:13 - INFO - __main__ -     acc = 0.931
03/11/2022 05:02:13 - INFO - __main__ -     auc = 0.9563174382178907
03/11/2022 05:02:13 - INFO - __main__ -     f1 = 0.9309419221565336
03/11/2022 05:02:13 - INFO - __main__ -     mcc = 0.8653384589358429
03/11/2022 05:02:13 - INFO - __main__ -     precision = 0.9337525733401956
03/11/2022 05:02:13 - INFO - __main__ -     recall = 0.9315885913639073
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:   3%|         | 4/138 [04:37<3:39:02, 98.08s/it][A
Iteration:   4%|         | 5/138 [04:58<2:35:22, 70.10s/it][A
Iteration:   4%|         | 6/138 [05:18<1:56:53, 53.13s/it][A
Iteration:   5%|         | 7/138 [05:38<1:32:18, 42.28s/it][A
Iteration:   6%|         | 8/138 [05:59<1:16:51, 35.47s/it][A
Iteration:   7%|         | 9/138 [06:19<1:06:05, 30.74s/it][A
Iteration:   7%|         | 10/138 [06:39<58:33, 27.45s/it] [A
Iteration:   8%|         | 11/138 [06:59<53:15, 25.16s/it][A
Iteration:   9%|         | 12/138 [07:19<49:28, 23.56s/it][A
Iteration:   9%|         | 13/138 [07:39<46:48, 22.47s/it][A03/11/2022 05:05:36 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 05:05:36 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 05:05:36 - INFO - __main__ -     Num examples = 1000
03/11/2022 05:05:36 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.931, "eval_f1": 0.9309419221565336, "eval_mcc": 0.8653384589358429, "eval_auc": 0.9563174382178907, "eval_precision": 0.9337525733401956, "eval_recall": 0.9315885913639073, "learning_rate": 0.00013204508856682771, "loss": 0.11160076465457677, "step": 280}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:17,  6.38s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:09,  6.32s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.30s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.31s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.29s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.29s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.28s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.28s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.28s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.27s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:11,  6.28s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.30s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:59,  6.30s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.30s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:46,  6.29s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.29s/it][A[A

Evaluating:  53%|    | 17/32 [01:46<01:34,  6.29s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:27,  6.28s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:23,  6.42s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:17,  6.46s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:11,  6.47s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:04,  6.48s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:57,  6.42s/it][A[A

Evaluating:  75%|  | 24/32 [02:32<00:51,  6.38s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:44,  6.35s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:38,  6.34s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.33s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.32s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:18,  6.31s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.30s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.29s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.89s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.18s/it]
03/11/2022 05:08:54 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 05:08:54 - INFO - __main__ -     acc = 0.931
03/11/2022 05:08:54 - INFO - __main__ -     auc = 0.9663554056595093
03/11/2022 05:08:54 - INFO - __main__ -     f1 = 0.9309247770822426
03/11/2022 05:08:54 - INFO - __main__ -     mcc = 0.8659801828450059
03/11/2022 05:08:54 - INFO - __main__ -     precision = 0.9343397745571659
03/11/2022 05:08:54 - INFO - __main__ -     recall = 0.931644602342059
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  10%|         | 14/138 [11:18<2:48:45, 81.66s/it][A
Iteration:  11%|         | 15/138 [11:38<2:09:37, 63.23s/it][A
Iteration:  12%|        | 16/138 [11:58<1:42:08, 50.23s/it][A
Iteration:  12%|        | 17/138 [12:18<1:22:58, 41.14s/it][A
Iteration:  13%|        | 18/138 [12:38<1:09:34, 34.78s/it][A
Iteration:  14%|        | 19/138 [12:58<1:00:10, 30.34s/it][A
Iteration:  14%|        | 20/138 [13:19<54:10, 27.55s/it]  [A
Iteration:  15%|        | 21/138 [13:39<49:28, 25.37s/it][A
Iteration:  16%|        | 22/138 [13:59<45:57, 23.77s/it][A
Iteration:  17%|        | 23/138 [14:20<43:25, 22.66s/it][A03/11/2022 05:12:16 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 05:12:16 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 05:12:16 - INFO - __main__ -     Num examples = 1000
03/11/2022 05:12:16 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.931, "eval_f1": 0.9309247770822426, "eval_mcc": 0.8659801828450059, "eval_auc": 0.9663554056595093, "eval_precision": 0.9343397745571659, "eval_recall": 0.931644602342059, "learning_rate": 0.0001288244766505636, "loss": 0.22795881503261625, "step": 290}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:15,  6.30s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.29s/it][A[A

Evaluating:   9%|         | 3/32 [00:19<03:06,  6.43s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<03:02,  6.50s/it][A[A

Evaluating:  16%|        | 5/32 [00:32<02:55,  6.51s/it][A[A

Evaluating:  19%|        | 6/32 [00:38<02:49,  6.50s/it][A[A

Evaluating:  22%|       | 7/32 [00:45<02:41,  6.46s/it][A[A

Evaluating:  25%|       | 8/32 [00:51<02:33,  6.41s/it][A[A

Evaluating:  28%|       | 9/32 [00:57<02:26,  6.37s/it][A[A

Evaluating:  31%|      | 10/32 [01:04<02:19,  6.36s/it][A[A

Evaluating:  34%|      | 11/32 [01:10<02:13,  6.35s/it][A[A

Evaluating:  38%|      | 12/32 [01:16<02:06,  6.33s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:00,  6.32s/it][A[A

Evaluating:  44%|     | 14/32 [01:29<01:53,  6.32s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:47,  6.32s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:40,  6.31s/it][A[A

Evaluating:  53%|    | 17/32 [01:48<01:34,  6.31s/it][A[A

Evaluating:  56%|    | 18/32 [01:54<01:28,  6.32s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:22,  6.31s/it][A[A

Evaluating:  62%|   | 20/32 [02:07<01:15,  6.30s/it][A[A

Evaluating:  66%|   | 21/32 [02:13<01:09,  6.29s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:02,  6.28s/it][A[A

Evaluating:  72%|  | 23/32 [02:26<00:57,  6.38s/it][A[A

Evaluating:  75%|  | 24/32 [02:32<00:51,  6.44s/it][A[A

Evaluating:  78%|  | 25/32 [02:39<00:45,  6.45s/it][A[A

Evaluating:  81%| | 26/32 [02:45<00:38,  6.49s/it][A[A

Evaluating:  84%| | 27/32 [02:52<00:32,  6.46s/it][A[A

Evaluating:  88%| | 28/32 [02:58<00:25,  6.40s/it][A[A

Evaluating:  91%| | 29/32 [03:04<00:19,  6.36s/it][A[A

Evaluating:  94%|| 30/32 [03:11<00:12,  6.33s/it][A[A

Evaluating:  97%|| 31/32 [03:17<00:06,  6.31s/it][A[A

Evaluating: 100%|| 32/32 [03:18<00:00,  4.90s/it][A[AEvaluating: 100%|| 32/32 [03:18<00:00,  6.22s/it]
03/11/2022 05:15:35 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 05:15:35 - INFO - __main__ -     acc = 0.933
03/11/2022 05:15:35 - INFO - __main__ -     auc = 0.9689319106544882
03/11/2022 05:15:35 - INFO - __main__ -     f1 = 0.9329806314024753
03/11/2022 05:15:35 - INFO - __main__ -     mcc = 0.8677571790799088
03/11/2022 05:15:35 - INFO - __main__ -     precision = 0.9343367117839503
03/11/2022 05:15:35 - INFO - __main__ -     recall = 0.9334209505062991
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  17%|        | 24/138 [17:59<2:35:07, 81.65s/it][A
Iteration:  18%|        | 25/138 [18:19<1:59:14, 63.32s/it][A
Iteration:  19%|        | 26/138 [18:39<1:33:57, 50.34s/it][A
Iteration:  20%|        | 27/138 [18:59<1:16:17, 41.24s/it][A
Iteration:  20%|        | 28/138 [19:20<1:04:18, 35.08s/it][A
Iteration:  21%|        | 29/138 [19:41<55:44, 30.68s/it]  [A
Iteration:  22%|       | 30/138 [20:01<49:31, 27.51s/it][A
Iteration:  22%|       | 31/138 [20:21<45:01, 25.24s/it][A
Iteration:  23%|       | 32/138 [20:41<41:49, 23.67s/it][A
Iteration:  24%|       | 33/138 [21:01<39:32, 22.59s/it][A03/11/2022 05:18:58 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 05:18:58 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 05:18:58 - INFO - __main__ -     Num examples = 1000
03/11/2022 05:18:58 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.933, "eval_f1": 0.9329806314024753, "eval_mcc": 0.8677571790799088, "eval_auc": 0.9689319106544882, "eval_precision": 0.9343367117839503, "eval_recall": 0.9334209505062991, "learning_rate": 0.00012560386473429953, "loss": 0.09615909156855196, "step": 300}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:21,  6.51s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:13,  6.46s/it][A[A

Evaluating:   9%|         | 3/32 [00:19<03:04,  6.38s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:57,  6.34s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:50,  6.31s/it][A[A

Evaluating:  19%|        | 6/32 [00:38<02:43,  6.30s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.28s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.28s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.28s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:18,  6.29s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:11,  6.28s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.28s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<01:59,  6.29s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.29s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:46,  6.29s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.28s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:34,  6.29s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.30s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.29s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.29s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.29s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.29s/it][A[A

Evaluating:  72%|  | 23/32 [02:24<00:56,  6.29s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.30s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:44,  6.32s/it][A[A

Evaluating:  81%| | 26/32 [02:43<00:37,  6.32s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.36s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.47s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:19,  6.49s/it][A[A

Evaluating:  94%|| 30/32 [03:10<00:13,  6.50s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.49s/it][A[A

Evaluating: 100%|| 32/32 [03:18<00:00,  5.03s/it][A[AEvaluating: 100%|| 32/32 [03:18<00:00,  6.20s/it]
03/11/2022 05:22:16 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 05:22:16 - INFO - __main__ -     acc = 0.931
03/11/2022 05:22:16 - INFO - __main__ -     auc = 0.9743229673015911
03/11/2022 05:22:16 - INFO - __main__ -     f1 = 0.9309966188343228
03/11/2022 05:22:16 - INFO - __main__ -     mcc = 0.8620844884200006
03/11/2022 05:22:16 - INFO - __main__ -     precision = 0.931
03/11/2022 05:22:16 - INFO - __main__ -     recall = 0.9310844925605418
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  25%|       | 34/138 [24:40<2:21:23, 81.58s/it][A
Iteration:  25%|       | 35/138 [25:00<1:48:38, 63.28s/it][A
Iteration:  26%|       | 36/138 [25:21<1:25:34, 50.34s/it][A
Iteration:  27%|       | 37/138 [25:41<1:09:26, 41.25s/it][A
Iteration:  28%|       | 38/138 [26:01<58:05, 34.86s/it]  [A
Iteration:  28%|       | 39/138 [26:21<50:09, 30.40s/it][A
Iteration:  29%|       | 40/138 [26:41<44:37, 27.32s/it][A
Iteration:  30%|       | 41/138 [27:01<40:34, 25.10s/it][A
Iteration:  30%|       | 42/138 [27:21<37:42, 23.56s/it][A
Iteration:  31%|       | 43/138 [27:41<35:35, 22.48s/it][A03/11/2022 05:25:37 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 05:25:37 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 05:25:37 - INFO - __main__ -     Num examples = 1000
03/11/2022 05:25:37 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.931, "eval_f1": 0.9309966188343228, "eval_mcc": 0.8620844884200006, "eval_auc": 0.9743229673015911, "eval_precision": 0.931, "eval_recall": 0.9310844925605418, "learning_rate": 0.00012238325281803544, "loss": 0.15018642405048013, "step": 310}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:16,  6.33s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:09,  6.32s/it][A[A

Evaluating:   9%|         | 3/32 [00:19<03:03,  6.34s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<03:00,  6.43s/it][A[A

Evaluating:  16%|        | 5/32 [00:32<02:54,  6.45s/it][A[A

Evaluating:  19%|        | 6/32 [00:38<02:46,  6.40s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:39,  6.36s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:31,  6.33s/it][A[A

Evaluating:  28%|       | 9/32 [00:57<02:25,  6.34s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:19,  6.33s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.31s/it][A[A

Evaluating:  38%|      | 12/32 [01:16<02:06,  6.30s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<01:59,  6.31s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.30s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:47,  6.30s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:40,  6.29s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:34,  6.30s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.29s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:21,  6.29s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.28s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.28s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.28s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:57,  6.34s/it][A[A

Evaluating:  75%|  | 24/32 [02:32<00:51,  6.44s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:45,  6.47s/it][A[A

Evaluating:  81%| | 26/32 [02:45<00:38,  6.49s/it][A[A

Evaluating:  84%| | 27/32 [02:51<00:32,  6.47s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.41s/it][A[A

Evaluating:  91%| | 29/32 [03:04<00:19,  6.37s/it][A[A

Evaluating:  94%|| 30/32 [03:10<00:12,  6.35s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.32s/it][A[A

Evaluating: 100%|| 32/32 [03:18<00:00,  4.91s/it][A[AEvaluating: 100%|| 32/32 [03:18<00:00,  6.20s/it]
03/11/2022 05:28:55 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 05:28:55 - INFO - __main__ -     acc = 0.911
03/11/2022 05:28:55 - INFO - __main__ -     auc = 0.9647250861168789
03/11/2022 05:28:55 - INFO - __main__ -     f1 = 0.9105742439755676
03/11/2022 05:28:55 - INFO - __main__ -     mcc = 0.8338982626728133
03/11/2022 05:28:55 - INFO - __main__ -     precision = 0.9218094863731656
03/11/2022 05:28:55 - INFO - __main__ -     recall = 0.9121447803769539
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  32%|      | 44/138 [31:19<2:07:21, 81.30s/it][A
Iteration:  33%|      | 45/138 [31:40<1:37:44, 63.06s/it][A
Iteration:  33%|      | 46/138 [32:00<1:16:55, 50.17s/it][A
Iteration:  34%|      | 47/138 [32:20<1:02:22, 41.13s/it][A
Iteration:  35%|      | 48/138 [32:40<52:10, 34.79s/it]  [A
Iteration:  36%|      | 49/138 [33:00<45:14, 30.50s/it][A
Iteration:  36%|      | 50/138 [33:20<40:11, 27.40s/it][A
Iteration:  37%|      | 51/138 [33:40<36:30, 25.17s/it][A
Iteration:  38%|      | 52/138 [34:00<33:52, 23.64s/it][A
Iteration:  38%|      | 53/138 [34:20<31:55, 22.54s/it][A03/11/2022 05:32:17 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 05:32:17 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 05:32:17 - INFO - __main__ -     Num examples = 1000
03/11/2022 05:32:17 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.911, "eval_f1": 0.9105742439755676, "eval_mcc": 0.8338982626728133, "eval_auc": 0.9647250861168789, "eval_precision": 0.9218094863731656, "eval_recall": 0.9121447803769539, "learning_rate": 0.00011916264090177133, "loss": 0.15217323780525477, "step": 320}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:23,  6.56s/it][A[A

Evaluating:   6%|         | 2/32 [00:13<03:17,  6.59s/it][A[A

Evaluating:   9%|         | 3/32 [00:19<03:10,  6.55s/it][A[A

Evaluating:  12%|        | 4/32 [00:26<03:03,  6.54s/it][A[A

Evaluating:  16%|        | 5/32 [00:32<02:54,  6.48s/it][A[A

Evaluating:  19%|        | 6/32 [00:38<02:46,  6.41s/it][A[A

Evaluating:  22%|       | 7/32 [00:45<02:39,  6.36s/it][A[A

Evaluating:  25%|       | 8/32 [00:51<02:32,  6.34s/it][A[A

Evaluating:  28%|       | 9/32 [00:57<02:25,  6.33s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:18,  6.31s/it][A[A

Evaluating:  34%|      | 11/32 [01:10<02:12,  6.30s/it][A[A

Evaluating:  38%|      | 12/32 [01:16<02:05,  6.29s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<01:59,  6.30s/it][A[A

Evaluating:  44%|     | 14/32 [01:29<01:53,  6.30s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:47,  6.30s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:40,  6.30s/it][A[A

Evaluating:  53%|    | 17/32 [01:48<01:34,  6.30s/it][A[A

Evaluating:  56%|    | 18/32 [01:54<01:28,  6.29s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:21,  6.28s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.28s/it][A[A

Evaluating:  66%|   | 21/32 [02:13<01:09,  6.28s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:02,  6.28s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.27s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.29s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:44,  6.30s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.32s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.30s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.30s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:18,  6.30s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.31s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.45s/it][A[A

Evaluating: 100%|| 32/32 [03:18<00:00,  5.02s/it][A[AEvaluating: 100%|| 32/32 [03:18<00:00,  6.20s/it]
03/11/2022 05:35:35 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 05:35:35 - INFO - __main__ -     acc = 0.914
03/11/2022 05:35:35 - INFO - __main__ -     auc = 0.9684298122431997
03/11/2022 05:35:35 - INFO - __main__ -     f1 = 0.9137844611528823
03/11/2022 05:35:35 - INFO - __main__ -     mcc = 0.8351216796318754
03/11/2022 05:35:35 - INFO - __main__ -     precision = 0.9202596949961702
03/11/2022 05:35:35 - INFO - __main__ -     recall = 0.9148793163460038
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  39%|      | 54/138 [37:59<1:53:50, 81.31s/it][A
Iteration:  40%|      | 55/138 [38:20<1:27:31, 63.27s/it][A
Iteration:  41%|      | 56/138 [38:40<1:08:49, 50.36s/it][A
Iteration:  41%|     | 57/138 [39:00<55:42, 41.27s/it]  [A
Iteration:  42%|     | 58/138 [39:20<46:29, 34.87s/it][A
Iteration:  43%|     | 59/138 [39:40<40:03, 30.42s/it][A
Iteration:  43%|     | 60/138 [40:01<35:37, 27.41s/it][A
Iteration:  44%|     | 61/138 [40:21<32:36, 25.41s/it][A
Iteration:  45%|     | 62/138 [40:41<30:06, 23.77s/it][A
Iteration:  46%|     | 63/138 [41:01<28:16, 22.62s/it][A03/11/2022 05:38:58 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 05:38:58 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 05:38:58 - INFO - __main__ -     Num examples = 1000
03/11/2022 05:38:58 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.914, "eval_f1": 0.9137844611528823, "eval_mcc": 0.8351216796318754, "eval_auc": 0.9684298122431997, "eval_precision": 0.9202596949961702, "eval_recall": 0.9148793163460038, "learning_rate": 0.00011594202898550725, "loss": 0.18548168018460273, "step": 330}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.27s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.27s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:01,  6.27s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.27s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.26s/it][A[A

Evaluating:  19%|        | 6/32 [00:38<02:46,  6.40s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:41,  6.46s/it][A[A

Evaluating:  25%|       | 8/32 [00:51<02:35,  6.47s/it][A[A

Evaluating:  28%|       | 9/32 [00:57<02:29,  6.48s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:21,  6.42s/it][A[A

Evaluating:  34%|      | 11/32 [01:10<02:13,  6.38s/it][A[A

Evaluating:  38%|      | 12/32 [01:16<02:07,  6.35s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:00,  6.33s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.31s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:47,  6.31s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:40,  6.30s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:34,  6.29s/it][A[A

Evaluating:  56%|    | 18/32 [01:54<01:27,  6.28s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:21,  6.27s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.27s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.28s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:02,  6.28s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.30s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.30s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:44,  6.30s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:38,  6.43s/it][A[A

Evaluating:  84%| | 27/32 [02:51<00:32,  6.46s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.47s/it][A[A

Evaluating:  91%| | 29/32 [03:04<00:19,  6.48s/it][A[A

Evaluating:  94%|| 30/32 [03:10<00:12,  6.43s/it][A[A

Evaluating:  97%|| 31/32 [03:17<00:06,  6.40s/it][A[A

Evaluating: 100%|| 32/32 [03:18<00:00,  4.97s/it][A[AEvaluating: 100%|| 32/32 [03:18<00:00,  6.21s/it]
03/11/2022 05:42:16 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 05:42:16 - INFO - __main__ -     acc = 0.929
03/11/2022 05:42:16 - INFO - __main__ -     auc = 0.970770270973111
03/11/2022 05:42:16 - INFO - __main__ -     f1 = 0.9289840214048161
03/11/2022 05:42:16 - INFO - __main__ -     mcc = 0.8579699015839317
03/11/2022 05:42:16 - INFO - __main__ -     precision = 0.9289978234428014
03/11/2022 05:42:16 - INFO - __main__ -     recall = 0.9289720785273914
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  46%|     | 64/138 [44:40<1:40:30, 81.50s/it][A
Iteration:  47%|     | 65/138 [45:01<1:16:54, 63.22s/it][A
Iteration:  48%|     | 66/138 [45:21<1:00:23, 50.33s/it][A
Iteration:  49%|     | 67/138 [45:41<48:50, 41.27s/it]  [A
Iteration:  49%|     | 68/138 [46:01<40:41, 34.87s/it][A
Iteration:  50%|     | 69/138 [46:22<35:12, 30.62s/it][A
Iteration:  51%|     | 70/138 [46:42<31:06, 27.45s/it][A
Iteration:  51%|    | 71/138 [47:02<28:09, 25.22s/it][A
Iteration:  52%|    | 72/138 [47:22<26:01, 23.66s/it][A
Iteration:  53%|    | 73/138 [47:42<24:28, 22.59s/it][A03/11/2022 05:45:38 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 05:45:38 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 05:45:38 - INFO - __main__ -     Num examples = 1000
03/11/2022 05:45:38 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.929, "eval_f1": 0.9289840214048161, "eval_mcc": 0.8579699015839317, "eval_auc": 0.970770270973111, "eval_precision": 0.9289978234428014, "eval_recall": 0.9289720785273914, "learning_rate": 0.00011272141706924315, "loss": 0.14007770419120788, "step": 340}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:15,  6.31s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:09,  6.31s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.30s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.30s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:50,  6.31s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:44,  6.31s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.31s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:31,  6.31s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:25,  6.31s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:18,  6.31s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.32s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:06,  6.33s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:00,  6.32s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.33s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:47,  6.32s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.31s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:34,  6.30s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.29s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.28s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.28s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.27s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.29s/it][A[A

Evaluating:  72%|  | 23/32 [02:24<00:56,  6.29s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.28s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:44,  6.29s/it][A[A

Evaluating:  81%| | 26/32 [02:43<00:37,  6.29s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.29s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.29s/it][A[A

Evaluating:  91%| | 29/32 [03:02<00:18,  6.28s/it][A[A

Evaluating:  94%|| 30/32 [03:08<00:12,  6.29s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.38s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.98s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.17s/it]
03/11/2022 05:48:56 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 05:48:56 - INFO - __main__ -     acc = 0.894
03/11/2022 05:48:56 - INFO - __main__ -     auc = 0.9536109077379167
03/11/2022 05:48:56 - INFO - __main__ -     f1 = 0.8932101825098429
03/11/2022 05:48:56 - INFO - __main__ -     mcc = 0.8047241324727381
03/11/2022 05:48:56 - INFO - __main__ -     precision = 0.90946795331281
03/11/2022 05:48:56 - INFO - __main__ -     recall = 0.8953794943808986
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  54%|    | 74/138 [51:19<1:26:27, 81.06s/it][A
Iteration:  54%|    | 75/138 [51:40<1:06:07, 62.97s/it][A
Iteration:  55%|    | 76/138 [52:00<51:49, 50.16s/it]  [A
Iteration:  56%|    | 77/138 [52:20<41:48, 41.12s/it][A
Iteration:  57%|    | 78/138 [52:40<34:46, 34.77s/it][A
Iteration:  57%|    | 79/138 [53:00<29:51, 30.36s/it][A
Iteration:  58%|    | 80/138 [53:21<26:28, 27.39s/it][A
Iteration:  59%|    | 81/138 [53:42<24:08, 25.41s/it][A
Iteration:  59%|    | 82/138 [54:02<22:11, 23.78s/it][A
Iteration:  60%|    | 83/138 [54:22<20:46, 22.66s/it][A03/11/2022 05:52:18 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 05:52:18 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 05:52:18 - INFO - __main__ -     Num examples = 1000
03/11/2022 05:52:18 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.894, "eval_f1": 0.8932101825098429, "eval_mcc": 0.8047241324727381, "eval_auc": 0.9536109077379167, "eval_precision": 0.90946795331281, "eval_recall": 0.8953794943808986, "learning_rate": 0.00010950080515297907, "loss": 0.14067096523940564, "step": 350}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.27s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.27s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:01,  6.27s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.27s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.27s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.29s/it][A[A

Evaluating:  22%|       | 7/32 [00:43<02:37,  6.28s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.28s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.27s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.27s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:11,  6.27s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.28s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:59,  6.30s/it][A[A

Evaluating:  44%|     | 14/32 [01:27<01:53,  6.30s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:46,  6.29s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.29s/it][A[A

Evaluating:  53%|    | 17/32 [01:46<01:34,  6.29s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.29s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.29s/it][A[A

Evaluating:  62%|   | 20/32 [02:05<01:15,  6.29s/it][A[A

Evaluating:  66%|   | 21/32 [02:11<01:09,  6.29s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.29s/it][A[A

Evaluating:  72%|  | 23/32 [02:24<00:56,  6.29s/it][A[A

Evaluating:  75%|  | 24/32 [02:30<00:50,  6.29s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:43,  6.28s/it][A[A

Evaluating:  81%| | 26/32 [02:43<00:37,  6.29s/it][A[A

Evaluating:  84%| | 27/32 [02:49<00:31,  6.30s/it][A[A

Evaluating:  88%| | 28/32 [02:55<00:25,  6.29s/it][A[A

Evaluating:  91%| | 29/32 [03:02<00:18,  6.30s/it][A[A

Evaluating:  94%|| 30/32 [03:08<00:12,  6.30s/it][A[A

Evaluating:  97%|| 31/32 [03:14<00:06,  6.29s/it][A[A

Evaluating: 100%|| 32/32 [03:16<00:00,  4.89s/it][A[AEvaluating: 100%|| 32/32 [03:16<00:00,  6.14s/it]
03/11/2022 05:55:35 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 05:55:35 - INFO - __main__ -     acc = 0.94
03/11/2022 05:55:35 - INFO - __main__ -     auc = 0.9748630731623399
03/11/2022 05:55:35 - INFO - __main__ -     f1 = 0.9399961597542242
03/11/2022 05:55:35 - INFO - __main__ -     mcc = 0.8800600100567277
03/11/2022 05:55:35 - INFO - __main__ -     precision = 0.9399877599510398
03/11/2022 05:55:35 - INFO - __main__ -     recall = 0.9400722541618157
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  61%|    | 84/138 [57:59<1:12:47, 80.88s/it][A
Iteration:  62%|   | 85/138 [58:19<55:26, 62.77s/it]  [A
Iteration:  62%|   | 86/138 [58:39<43:19, 50.00s/it][A
Iteration:  63%|   | 87/138 [58:59<34:51, 41.01s/it][A
Iteration:  64%|   | 88/138 [59:19<28:55, 34.71s/it][A
Iteration:  64%|   | 89/138 [59:39<24:45, 30.32s/it][A
Iteration:  65%|   | 90/138 [59:59<21:45, 27.20s/it][A
Iteration:  66%|   | 91/138 [1:00:19<19:37, 25.06s/it][A
Iteration:  67%|   | 92/138 [1:00:40<18:13, 23.77s/it][A
Iteration:  67%|   | 93/138 [1:01:00<17:01, 22.71s/it][A03/11/2022 05:58:56 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 05:58:56 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 05:58:56 - INFO - __main__ -     Num examples = 1000
03/11/2022 05:58:56 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.94, "eval_f1": 0.9399961597542242, "eval_mcc": 0.8800600100567277, "eval_auc": 0.9748630731623399, "eval_precision": 0.9399877599510398, "eval_recall": 0.9400722541618157, "learning_rate": 0.00010628019323671499, "loss": 0.21146649308502674, "step": 360}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:13,  6.26s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.28s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.29s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.28s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.29s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.28s/it][A[A

Evaluating:  22%|       | 7/32 [00:43<02:36,  6.28s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.27s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.27s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:17,  6.27s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:11,  6.27s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:07,  6.35s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:01,  6.41s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:55,  6.39s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:48,  6.37s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:41,  6.34s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:34,  6.32s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.30s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.29s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.28s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.28s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.28s/it][A[A

Evaluating:  72%|  | 23/32 [02:24<00:56,  6.27s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.27s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:43,  6.27s/it][A[A

Evaluating:  81%| | 26/32 [02:43<00:37,  6.27s/it][A[A

Evaluating:  84%| | 27/32 [02:49<00:31,  6.27s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.28s/it][A[A

Evaluating:  91%| | 29/32 [03:02<00:18,  6.30s/it][A[A

Evaluating:  94%|| 30/32 [03:08<00:12,  6.29s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.28s/it][A[A

Evaluating: 100%|| 32/32 [03:16<00:00,  4.88s/it][A[AEvaluating: 100%|| 32/32 [03:16<00:00,  6.15s/it]
03/11/2022 06:02:13 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 06:02:13 - INFO - __main__ -     acc = 0.935
03/11/2022 06:02:13 - INFO - __main__ -     auc = 0.9704542090249689
03/11/2022 06:02:13 - INFO - __main__ -     f1 = 0.9349890131432212
03/11/2022 06:02:13 - INFO - __main__ -     mcc = 0.8713426806207776
03/11/2022 06:02:13 - INFO - __main__ -     precision = 0.935977564102564
03/11/2022 06:02:13 - INFO - __main__ -     recall = 0.9353653316049946
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  68%|   | 94/138 [1:04:37<59:20, 80.91s/it][A
Iteration:  69%|   | 95/138 [1:04:57<44:59, 62.78s/it][A
Iteration:  70%|   | 96/138 [1:05:18<34:59, 49.98s/it][A
Iteration:  70%|   | 97/138 [1:05:38<28:01, 41.01s/it][A
Iteration:  71%|   | 98/138 [1:05:58<23:09, 34.74s/it][A
Iteration:  72%|  | 99/138 [1:06:18<19:43, 30.34s/it][A
Iteration:  72%|  | 100/138 [1:06:38<17:15, 27.24s/it][A
Iteration:  73%|  | 101/138 [1:06:58<15:29, 25.11s/it][A
Iteration:  74%|  | 102/138 [1:07:18<14:06, 23.53s/it][A
Iteration:  75%|  | 103/138 [1:07:38<13:07, 22.50s/it][A03/11/2022 06:05:34 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 06:05:35 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 06:05:35 - INFO - __main__ -     Num examples = 1000
03/11/2022 06:05:35 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.935, "eval_f1": 0.9349890131432212, "eval_mcc": 0.8713426806207776, "eval_auc": 0.9704542090249689, "eval_precision": 0.935977564102564, "eval_recall": 0.9353653316049946, "learning_rate": 0.00010305958132045089, "loss": 0.16558888759464024, "step": 370}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.27s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.29s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.30s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.31s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:50,  6.31s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.30s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.29s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.28s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.28s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.28s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:11,  6.27s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.29s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:59,  6.29s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.28s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:46,  6.28s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.29s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:36,  6.42s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:30,  6.46s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:24,  6.47s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:17,  6.49s/it][A[A

Evaluating:  66%|   | 21/32 [02:13<01:10,  6.45s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:03,  6.39s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:57,  6.35s/it][A[A

Evaluating:  75%|  | 24/32 [02:32<00:50,  6.33s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:44,  6.31s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.30s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.29s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.30s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:18,  6.30s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.29s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.29s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.89s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.18s/it]
03/11/2022 06:08:52 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 06:08:52 - INFO - __main__ -     acc = 0.943
03/11/2022 06:08:52 - INFO - __main__ -     auc = 0.9726866465827301
03/11/2022 06:08:52 - INFO - __main__ -     f1 = 0.9429131709329891
03/11/2022 06:08:52 - INFO - __main__ -     mcc = 0.8911543463628807
03/11/2022 06:08:52 - INFO - __main__ -     precision = 0.9474310564618935
03/11/2022 06:08:52 - INFO - __main__ -     recall = 0.943730971270369
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  75%|  | 104/138 [1:11:16<46:00, 81.20s/it][A
Iteration:  76%|  | 105/138 [1:11:37<34:38, 62.97s/it][A
Iteration:  77%|  | 106/138 [1:11:58<26:52, 50.40s/it][A
Iteration:  78%|  | 107/138 [1:12:18<21:22, 41.38s/it][A
Iteration:  78%|  | 108/138 [1:12:38<17:29, 34.98s/it][A
Iteration:  79%|  | 109/138 [1:12:58<14:44, 30.48s/it][A
Iteration:  80%|  | 110/138 [1:13:18<12:45, 27.35s/it][A
Iteration:  80%|  | 111/138 [1:13:38<11:19, 25.16s/it][A
Iteration:  81%|  | 112/138 [1:13:59<10:19, 23.81s/it][A
Iteration:  82%| | 113/138 [1:14:19<09:30, 22.80s/it][A03/11/2022 06:12:15 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 06:12:15 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 06:12:15 - INFO - __main__ -     Num examples = 1000
03/11/2022 06:12:15 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.943, "eval_f1": 0.9429131709329891, "eval_mcc": 0.8911543463628807, "eval_auc": 0.9726866465827301, "eval_precision": 0.9474310564618935, "eval_recall": 0.943730971270369, "learning_rate": 9.98389694041868e-05, "loss": 0.10300495997071266, "step": 380}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:15,  6.30s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:09,  6.31s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.31s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.31s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:50,  6.30s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.30s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.30s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:31,  6.30s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.30s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:18,  6.30s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.31s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:08,  6.41s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:02,  6.46s/it][A[A

Evaluating:  44%|     | 14/32 [01:29<01:56,  6.47s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:50,  6.49s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:43,  6.44s/it][A[A

Evaluating:  53%|    | 17/32 [01:48<01:35,  6.39s/it][A[A

Evaluating:  56%|    | 18/32 [01:54<01:28,  6.35s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:22,  6.33s/it][A[A

Evaluating:  62%|   | 20/32 [02:07<01:15,  6.32s/it][A[A

Evaluating:  66%|   | 21/32 [02:13<01:10,  6.41s/it][A[A

Evaluating:  69%|   | 22/32 [02:20<01:04,  6.44s/it][A[A

Evaluating:  72%|  | 23/32 [02:26<00:57,  6.41s/it][A[A

Evaluating:  75%|  | 24/32 [02:32<00:50,  6.37s/it][A[A

Evaluating:  78%|  | 25/32 [02:39<00:44,  6.34s/it][A[A

Evaluating:  81%| | 26/32 [02:45<00:37,  6.33s/it][A[A

Evaluating:  84%| | 27/32 [02:51<00:31,  6.33s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.32s/it][A[A

Evaluating:  91%| | 29/32 [03:04<00:18,  6.30s/it][A[A

Evaluating:  94%|| 30/32 [03:10<00:12,  6.29s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.28s/it][A[A

Evaluating: 100%|| 32/32 [03:18<00:00,  4.88s/it][A[AEvaluating: 100%|| 32/32 [03:18<00:00,  6.20s/it]
03/11/2022 06:15:34 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 06:15:34 - INFO - __main__ -     acc = 0.95
03/11/2022 06:15:34 - INFO - __main__ -     auc = 0.975291157066785
03/11/2022 06:15:34 - INFO - __main__ -     f1 = 0.9499661771357437
03/11/2022 06:15:34 - INFO - __main__ -     mcc = 0.9029811428680211
03/11/2022 06:15:34 - INFO - __main__ -     precision = 0.9524327972648858
03/11/2022 06:15:34 - INFO - __main__ -     recall = 0.9505503078603406
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  83%| | 114/138 [1:17:58<32:36, 81.52s/it][A
Iteration:  83%| | 115/138 [1:18:18<24:13, 63.22s/it][A
Iteration:  84%| | 116/138 [1:18:38<18:26, 50.30s/it][A
Iteration:  85%| | 117/138 [1:18:59<14:28, 41.37s/it][A
Iteration:  86%| | 118/138 [1:19:20<11:43, 35.15s/it][A
Iteration:  86%| | 119/138 [1:19:40<09:41, 30.63s/it][A
Iteration:  87%| | 120/138 [1:20:00<08:14, 27.47s/it][A
Iteration:  88%| | 121/138 [1:20:20<07:09, 25.25s/it][A
Iteration:  88%| | 122/138 [1:20:40<06:18, 23.68s/it][A
Iteration:  89%| | 123/138 [1:21:00<05:39, 22.61s/it][A03/11/2022 06:18:56 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 06:18:56 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 06:18:56 - INFO - __main__ -     Num examples = 1000
03/11/2022 06:18:56 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.95, "eval_f1": 0.9499661771357437, "eval_mcc": 0.9029811428680211, "eval_auc": 0.975291157066785, "eval_precision": 0.9524327972648858, "eval_recall": 0.9505503078603406, "learning_rate": 9.66183574879227e-05, "loss": 0.052692406717687844, "step": 390}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:15,  6.30s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.30s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:03,  6.31s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.31s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:50,  6.30s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.29s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.29s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:31,  6.29s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.29s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.30s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.31s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:06,  6.32s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<02:00,  6.32s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.32s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:47,  6.31s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:42,  6.39s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:36,  6.45s/it][A[A

Evaluating:  56%|    | 18/32 [01:54<01:30,  6.48s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:24,  6.49s/it][A[A

Evaluating:  62%|   | 20/32 [02:07<01:17,  6.46s/it][A[A

Evaluating:  66%|   | 21/32 [02:13<01:10,  6.40s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:03,  6.36s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.33s/it][A[A

Evaluating:  75%|  | 24/32 [02:32<00:50,  6.32s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:44,  6.31s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.32s/it][A[A

Evaluating:  84%| | 27/32 [02:51<00:31,  6.30s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.29s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:18,  6.28s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.28s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.28s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.88s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.18s/it]
03/11/2022 06:22:14 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 06:22:14 - INFO - __main__ -     acc = 0.949
03/11/2022 06:22:14 - INFO - __main__ -     auc = 0.9739428928069901
03/11/2022 06:22:14 - INFO - __main__ -     f1 = 0.9489570728983074
03/11/2022 06:22:14 - INFO - __main__ -     mcc = 0.9014356730076596
03/11/2022 06:22:14 - INFO - __main__ -     precision = 0.9518463715903243
03/11/2022 06:22:14 - INFO - __main__ -     recall = 0.9495921200555308
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  90%| | 124/138 [1:24:38<18:57, 81.28s/it][A
Iteration:  91%| | 125/138 [1:24:59<13:39, 63.05s/it][A
Iteration:  91%|| 126/138 [1:25:19<10:04, 50.39s/it][A
Iteration:  92%|| 127/138 [1:25:39<07:34, 41.27s/it][A
Iteration:  93%|| 128/138 [1:25:59<05:48, 34.88s/it][A
Iteration:  93%|| 129/138 [1:26:19<04:33, 30.43s/it][A
Iteration:  94%|| 130/138 [1:26:40<03:38, 27.35s/it][A
Iteration:  95%|| 131/138 [1:27:00<02:55, 25.13s/it][A
Iteration:  96%|| 132/138 [1:27:19<02:21, 23.56s/it][A
Iteration:  96%|| 133/138 [1:27:40<01:52, 22.52s/it][A03/11/2022 06:25:36 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 06:25:36 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 06:25:36 - INFO - __main__ -     Num examples = 1000
03/11/2022 06:25:36 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.949, "eval_f1": 0.9489570728983074, "eval_mcc": 0.9014356730076596, "eval_auc": 0.9739428928069901, "eval_precision": 0.9518463715903243, "eval_recall": 0.9495921200555308, "learning_rate": 9.339774557165862e-05, "loss": 0.19317030964884907, "step": 400}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.27s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.29s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.29s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.28s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.27s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.27s/it][A[A

Evaluating:  22%|       | 7/32 [00:43<02:36,  6.27s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.28s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.29s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.31s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.31s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:06,  6.32s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<02:00,  6.33s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.33s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:47,  6.33s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:41,  6.33s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:34,  6.33s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.33s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:22,  6.32s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.32s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.36s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:04,  6.44s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:58,  6.47s/it][A[A

Evaluating:  75%|  | 24/32 [02:32<00:51,  6.48s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:45,  6.49s/it][A[A

Evaluating:  81%| | 26/32 [02:45<00:38,  6.44s/it][A[A

Evaluating:  84%| | 27/32 [02:51<00:31,  6.39s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.37s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:19,  6.34s/it][A[A

Evaluating:  94%|| 30/32 [03:10<00:12,  6.32s/it][A[A

Evaluating:  97%|| 31/32 [03:16<00:06,  6.30s/it][A[A

Evaluating: 100%|| 32/32 [03:18<00:00,  4.90s/it][A[AEvaluating: 100%|| 32/32 [03:18<00:00,  6.19s/it]
03/11/2022 06:28:54 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 06:28:54 - INFO - __main__ -     acc = 0.94
03/11/2022 06:28:54 - INFO - __main__ -     auc = 0.9743349696540522
03/11/2022 06:28:54 - INFO - __main__ -     f1 = 0.93999975999904
03/11/2022 06:28:54 - INFO - __main__ -     mcc = 0.8804809268446735
03/11/2022 06:28:54 - INFO - __main__ -     precision = 0.9402686470416415
03/11/2022 06:28:54 - INFO - __main__ -     recall = 0.9402122816071949
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  97%|| 134/138 [1:31:18<05:24, 81.25s/it][A
Iteration:  98%|| 135/138 [1:31:38<03:09, 63.05s/it][A
Iteration:  99%|| 136/138 [1:31:59<01:40, 50.19s/it][A
Iteration:  99%|| 137/138 [1:32:19<00:41, 41.17s/it][A
Iteration: 100%|| 138/138 [1:32:24<00:00, 30.52s/it][AIteration: 100%|| 138/138 [1:32:24<00:00, 40.18s/it]
Epoch:  60%|    | 3/5 [4:35:01<3:03:50, 5515.33s/it]{"eval_acc": 0.94, "eval_f1": 0.93999975999904, "eval_mcc": 0.8804809268446735, "eval_auc": 0.9743349696540522, "eval_precision": 0.9402686470416415, "eval_recall": 0.9402122816071949, "learning_rate": 9.017713365539453e-05, "loss": 0.11801699115894734, "step": 410}

Iteration:   0%|          | 0/138 [00:00<?, ?it/s][A
Iteration:   1%|          | 1/138 [00:20<46:06, 20.19s/it][A
Iteration:   1%|         | 2/138 [00:40<45:46, 20.20s/it][A
Iteration:   2%|         | 3/138 [01:00<45:23, 20.17s/it][A
Iteration:   3%|         | 4/138 [01:20<44:56, 20.12s/it][A
Iteration:   4%|         | 5/138 [01:40<44:23, 20.03s/it][A03/11/2022 06:32:02 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 06:32:02 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 06:32:02 - INFO - __main__ -     Num examples = 1000
03/11/2022 06:32:02 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:23,  6.55s/it][A[A

Evaluating:   6%|         | 2/32 [00:13<03:15,  6.51s/it][A[A

Evaluating:   9%|         | 3/32 [00:19<03:06,  6.42s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:58,  6.37s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:51,  6.33s/it][A[A

Evaluating:  19%|        | 6/32 [00:38<02:44,  6.31s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.30s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.29s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.29s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:18,  6.28s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.30s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.30s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<01:59,  6.29s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.29s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:47,  6.30s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:40,  6.29s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:34,  6.29s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:27,  6.29s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.29s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.29s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.28s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.28s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.28s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.28s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:43,  6.27s/it][A[A

Evaluating:  81%| | 26/32 [02:43<00:37,  6.27s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.29s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.38s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:19,  6.42s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.41s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.37s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.94s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.17s/it]
03/11/2022 06:35:19 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 06:35:19 - INFO - __main__ -     acc = 0.927
03/11/2022 06:35:19 - INFO - __main__ -     auc = 0.96732559581678
03/11/2022 06:35:19 - INFO - __main__ -     f1 = 0.9268243051566811
03/11/2022 06:35:19 - INFO - __main__ -     mcc = 0.8610695295152292
03/11/2022 06:35:19 - INFO - __main__ -     precision = 0.9332182902326787
03/11/2022 06:35:19 - INFO - __main__ -     recall = 0.9278678621009717
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:   4%|         | 6/138 [05:18<3:12:26, 87.47s/it][A
Iteration:   5%|         | 7/138 [05:39<2:23:12, 65.59s/it][A
Iteration:   6%|         | 8/138 [05:59<1:50:43, 51.10s/it][A
Iteration:   7%|         | 9/138 [06:19<1:28:58, 41.39s/it][A
Iteration:   7%|         | 10/138 [06:39<1:14:13, 34.79s/it][A
Iteration:   8%|         | 11/138 [06:59<1:04:07, 30.30s/it][A
Iteration:   9%|         | 12/138 [07:20<57:41, 27.47s/it]  [A
Iteration:   9%|         | 13/138 [07:40<52:34, 25.23s/it][A
Iteration:  10%|         | 14/138 [08:00<48:49, 23.62s/it][A
Iteration:  11%|         | 15/138 [08:20<46:08, 22.51s/it][A03/11/2022 06:38:41 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 06:38:41 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 06:38:41 - INFO - __main__ -     Num examples = 1000
03/11/2022 06:38:41 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.927, "eval_f1": 0.9268243051566811, "eval_mcc": 0.8610695295152292, "eval_auc": 0.96732559581678, "eval_precision": 0.9332182902326787, "eval_recall": 0.9278678621009717, "learning_rate": 8.695652173913044e-05, "loss": 0.11604729820974172, "step": 420}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:15,  6.29s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.29s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.30s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:58,  6.36s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:54,  6.45s/it][A[A

Evaluating:  19%|        | 6/32 [00:38<02:48,  6.46s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:42,  6.48s/it][A[A

Evaluating:  25%|       | 8/32 [00:51<02:35,  6.48s/it][A[A

Evaluating:  28%|       | 9/32 [00:57<02:27,  6.41s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:20,  6.38s/it][A[A

Evaluating:  34%|      | 11/32 [01:10<02:13,  6.35s/it][A[A

Evaluating:  38%|      | 12/32 [01:16<02:06,  6.33s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<01:59,  6.31s/it][A[A

Evaluating:  44%|     | 14/32 [01:29<01:53,  6.31s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:47,  6.32s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:41,  6.31s/it][A[A

Evaluating:  53%|    | 17/32 [01:48<01:34,  6.31s/it][A[A

Evaluating:  56%|    | 18/32 [01:54<01:28,  6.30s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:21,  6.30s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.29s/it][A[A

Evaluating:  66%|   | 21/32 [02:13<01:09,  6.28s/it][A[A

Evaluating:  69%|   | 22/32 [02:19<01:02,  6.27s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.27s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.27s/it][A[A

Evaluating:  78%|  | 25/32 [02:38<00:43,  6.27s/it][A[A

Evaluating:  81%| | 26/32 [02:44<00:37,  6.28s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.29s/it][A[A

Evaluating:  88%| | 28/32 [02:57<00:25,  6.29s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:18,  6.29s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.28s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.28s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.88s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.17s/it]
03/11/2022 06:41:59 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 06:41:59 - INFO - __main__ -     acc = 0.952
03/11/2022 06:41:59 - INFO - __main__ -     auc = 0.9756832339138471
03/11/2022 06:41:59 - INFO - __main__ -     f1 = 0.9519877088534665
03/11/2022 06:41:59 - INFO - __main__ -     mcc = 0.9056913970807239
03/11/2022 06:41:59 - INFO - __main__ -     precision = 0.953281142898373
03/11/2022 06:41:59 - INFO - __main__ -     recall = 0.9524106724918084
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  12%|        | 16/138 [11:58<2:45:09, 81.23s/it][A
Iteration:  12%|        | 17/138 [12:18<2:07:01, 62.99s/it][A
Iteration:  13%|        | 18/138 [12:38<1:40:15, 50.13s/it][A
Iteration:  14%|        | 19/138 [12:59<1:21:35, 41.14s/it][A
Iteration:  14%|        | 20/138 [13:19<1:08:41, 34.93s/it][A
Iteration:  15%|        | 21/138 [13:39<59:30, 30.52s/it]  [A
Iteration:  16%|        | 22/138 [13:59<52:59, 27.41s/it][A
Iteration:  17%|        | 23/138 [14:19<48:18, 25.20s/it][A
Iteration:  17%|        | 24/138 [14:39<44:52, 23.62s/it][A
Iteration:  18%|        | 25/138 [14:59<42:23, 22.51s/it][A03/11/2022 06:45:20 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 06:45:20 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 06:45:20 - INFO - __main__ -     Num examples = 1000
03/11/2022 06:45:20 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.952, "eval_f1": 0.9519877088534665, "eval_mcc": 0.9056913970807239, "eval_auc": 0.9756832339138471, "eval_precision": 0.953281142898373, "eval_recall": 0.9524106724918084, "learning_rate": 8.373590982286635e-05, "loss": 0.08476591883227229, "step": 430}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.27s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.30s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.29s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.28s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.27s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:42,  6.27s/it][A[A

Evaluating:  22%|       | 7/32 [00:43<02:36,  6.27s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.27s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.27s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:20,  6.38s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:14,  6.42s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:07,  6.39s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:00,  6.35s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.33s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:47,  6.32s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:40,  6.31s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:34,  6.30s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.30s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.29s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.28s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.27s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.27s/it][A[A

Evaluating:  72%|  | 23/32 [02:24<00:56,  6.27s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.27s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:43,  6.28s/it][A[A

Evaluating:  81%| | 26/32 [02:43<00:37,  6.28s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.27s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.38s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:19,  6.42s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.40s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.36s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.94s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.17s/it]
03/11/2022 06:48:38 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 06:48:38 - INFO - __main__ -     acc = 0.945
03/11/2022 06:48:38 - INFO - __main__ -     auc = 0.9746870386595773
03/11/2022 06:48:38 - INFO - __main__ -     f1 = 0.9449325423643964
03/11/2022 06:48:38 - INFO - __main__ -     mcc = 0.8944244636754085
03/11/2022 06:48:38 - INFO - __main__ -     precision = 0.9487544111248973
03/11/2022 06:48:38 - INFO - __main__ -     recall = 0.9456753523690644
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  19%|        | 26/138 [18:37<2:31:13, 81.01s/it][A
Iteration:  20%|        | 27/138 [18:57<1:56:17, 62.86s/it][A
Iteration:  20%|        | 28/138 [19:17<1:31:41, 50.01s/it][A
Iteration:  21%|        | 29/138 [19:38<1:14:35, 41.06s/it][A
Iteration:  22%|       | 30/138 [19:58<1:02:32, 34.74s/it][A
Iteration:  22%|       | 31/138 [20:18<54:07, 30.35s/it]  [A
Iteration:  23%|       | 32/138 [20:39<48:36, 27.51s/it][A
Iteration:  24%|       | 33/138 [20:59<44:18, 25.32s/it][A
Iteration:  25%|       | 34/138 [21:19<41:08, 23.74s/it][A
Iteration:  25%|       | 35/138 [21:39<38:45, 22.58s/it][A03/11/2022 06:52:00 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 06:52:00 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 06:52:00 - INFO - __main__ -     Num examples = 1000
03/11/2022 06:52:00 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.945, "eval_f1": 0.9449325423643964, "eval_mcc": 0.8944244636754085, "eval_auc": 0.9746870386595773, "eval_precision": 0.9487544111248973, "eval_recall": 0.9456753523690644, "learning_rate": 8.051529790660226e-05, "loss": 0.08202689790632575, "step": 440}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.27s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.28s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.28s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.31s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:52,  6.40s/it][A[A

Evaluating:  19%|        | 6/32 [00:38<02:46,  6.42s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:39,  6.36s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:31,  6.33s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:25,  6.31s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:18,  6.30s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.29s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.28s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<01:59,  6.27s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:52,  6.26s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:46,  6.26s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.26s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:34,  6.28s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:27,  6.28s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.28s/it][A[A

Evaluating:  62%|   | 20/32 [02:05<01:15,  6.28s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:09,  6.28s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.28s/it][A[A

Evaluating:  72%|  | 23/32 [02:24<00:56,  6.28s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.28s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:44,  6.29s/it][A[A

Evaluating:  81%| | 26/32 [02:43<00:37,  6.29s/it][A[A

Evaluating:  84%| | 27/32 [02:49<00:31,  6.29s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.29s/it][A[A

Evaluating:  91%| | 29/32 [03:02<00:18,  6.29s/it][A[A

Evaluating:  94%|| 30/32 [03:08<00:12,  6.30s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.29s/it][A[A

Evaluating: 100%|| 32/32 [03:16<00:00,  4.89s/it][A[AEvaluating: 100%|| 32/32 [03:16<00:00,  6.15s/it]
03/11/2022 06:55:16 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 06:55:16 - INFO - __main__ -     acc = 0.95
03/11/2022 06:55:16 - INFO - __main__ -     auc = 0.9754111805913959
03/11/2022 06:55:16 - INFO - __main__ -     f1 = 0.9499901980788235
03/11/2022 06:55:16 - INFO - __main__ -     mcc = 0.9014717196045287
03/11/2022 06:55:16 - INFO - __main__ -     precision = 0.9510897222700845
03/11/2022 06:55:16 - INFO - __main__ -     recall = 0.9503822749258855
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  26%|       | 36/138 [25:15<2:17:25, 80.84s/it][A
Iteration:  27%|       | 37/138 [25:36<1:45:37, 62.74s/it][A
Iteration:  28%|       | 38/138 [25:56<1:23:11, 49.91s/it][A
Iteration:  28%|       | 39/138 [26:16<1:07:35, 40.96s/it][A
Iteration:  29%|       | 40/138 [26:36<56:37, 34.67s/it]  [A
Iteration:  30%|       | 41/138 [26:56<48:51, 30.22s/it][A
Iteration:  30%|       | 42/138 [27:16<43:23, 27.12s/it][A
Iteration:  31%|       | 43/138 [27:36<39:31, 24.96s/it][A
Iteration:  32%|      | 44/138 [27:56<36:45, 23.46s/it][A
Iteration:  33%|      | 45/138 [28:15<34:40, 22.38s/it][A03/11/2022 06:58:36 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 06:58:36 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 06:58:36 - INFO - __main__ -     Num examples = 1000
03/11/2022 06:58:36 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.95, "eval_f1": 0.9499901980788235, "eval_mcc": 0.9014717196045287, "eval_auc": 0.9754111805913959, "eval_precision": 0.9510897222700845, "eval_recall": 0.9503822749258855, "learning_rate": 7.729468599033817e-05, "loss": 0.13711785646155478, "step": 450}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:15,  6.29s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.28s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:01,  6.27s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.26s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:48,  6.25s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:42,  6.26s/it][A[A

Evaluating:  22%|       | 7/32 [00:43<02:36,  6.26s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.26s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.27s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.27s/it][A[A

Evaluating:  34%|      | 11/32 [01:08<02:11,  6.27s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.27s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:58,  6.26s/it][A[A

Evaluating:  44%|     | 14/32 [01:27<01:52,  6.26s/it][A[A

Evaluating:  47%|     | 15/32 [01:33<01:46,  6.26s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.26s/it][A[A

Evaluating:  53%|    | 17/32 [01:46<01:34,  6.28s/it][A[A

Evaluating:  56%|    | 18/32 [01:52<01:27,  6.28s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.28s/it][A[A

Evaluating:  62%|   | 20/32 [02:05<01:15,  6.27s/it][A[A

Evaluating:  66%|   | 21/32 [02:11<01:08,  6.26s/it][A[A

Evaluating:  69%|   | 22/32 [02:17<01:02,  6.26s/it][A[A

Evaluating:  72%|  | 23/32 [02:24<00:56,  6.26s/it][A[A

Evaluating:  75%|  | 24/32 [02:30<00:50,  6.26s/it][A[A

Evaluating:  78%|  | 25/32 [02:36<00:43,  6.28s/it][A[A

Evaluating:  81%| | 26/32 [02:42<00:37,  6.27s/it][A[A

Evaluating:  84%| | 27/32 [02:49<00:31,  6.27s/it][A[A

Evaluating:  88%| | 28/32 [02:55<00:25,  6.26s/it][A[A

Evaluating:  91%| | 29/32 [03:01<00:18,  6.26s/it][A[A

Evaluating:  94%|| 30/32 [03:08<00:12,  6.27s/it][A[A

Evaluating:  97%|| 31/32 [03:14<00:06,  6.27s/it][A[A

Evaluating: 100%|| 32/32 [03:15<00:00,  4.87s/it][A[AEvaluating: 100%|| 32/32 [03:15<00:00,  6.12s/it]
03/11/2022 07:01:52 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 07:01:52 - INFO - __main__ -     acc = 0.95
03/11/2022 07:01:52 - INFO - __main__ -     auc = 0.9732287528355558
03/11/2022 07:01:52 - INFO - __main__ -     f1 = 0.9499607692430865
03/11/2022 07:01:52 - INFO - __main__ -     mcc = 0.9032840726432523
03/11/2022 07:01:52 - INFO - __main__ -     precision = 0.9527082705255754
03/11/2022 07:01:52 - INFO - __main__ -     recall = 0.9505783133494164
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  33%|      | 46/138 [31:51<2:03:17, 80.41s/it][A
Iteration:  34%|      | 47/138 [32:12<1:34:39, 62.41s/it][A
Iteration:  35%|      | 48/138 [32:32<1:14:33, 49.71s/it][A
Iteration:  36%|      | 49/138 [32:54<1:01:32, 41.49s/it][A
Iteration:  36%|      | 50/138 [33:14<51:22, 35.02s/it]  [A
Iteration:  37%|      | 51/138 [33:34<44:13, 30.50s/it][A
Iteration:  38%|      | 52/138 [33:54<39:11, 27.34s/it][A
Iteration:  38%|      | 53/138 [34:14<35:35, 25.12s/it][A
Iteration:  39%|      | 54/138 [34:34<33:00, 23.57s/it][A
Iteration:  40%|      | 55/138 [34:54<31:04, 22.46s/it][A03/11/2022 07:05:15 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 07:05:15 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 07:05:15 - INFO - __main__ -     Num examples = 1000
03/11/2022 07:05:15 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.95, "eval_f1": 0.9499607692430865, "eval_mcc": 0.9032840726432523, "eval_auc": 0.9732287528355558, "eval_precision": 0.9527082705255754, "eval_recall": 0.9505783133494164, "learning_rate": 7.407407407407407e-05, "loss": 0.1280875954311341, "step": 460}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:24,  6.61s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:06,  6.23s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<02:55,  6.06s/it][A[A

Evaluating:  12%|        | 4/32 [00:24<02:47,  5.98s/it][A[A

Evaluating:  16%|        | 5/32 [00:30<02:39,  5.92s/it][A[A

Evaluating:  19%|        | 6/32 [00:35<02:33,  5.90s/it][A[A

Evaluating:  22%|       | 7/32 [00:41<02:27,  5.88s/it][A[A

Evaluating:  25%|       | 8/32 [00:47<02:20,  5.87s/it][A[A

Evaluating:  28%|       | 9/32 [00:53<02:14,  5.87s/it][A[A

Evaluating:  31%|      | 10/32 [00:59<02:08,  5.86s/it][A[A

Evaluating:  34%|      | 11/32 [01:05<02:02,  5.85s/it][A[A

Evaluating:  38%|      | 12/32 [01:10<01:56,  5.84s/it][A[A

Evaluating:  41%|      | 13/32 [01:16<01:50,  5.84s/it][A[A

Evaluating:  44%|     | 14/32 [01:22<01:45,  5.84s/it][A[A

Evaluating:  47%|     | 15/32 [01:28<01:39,  5.84s/it][A[A

Evaluating:  50%|     | 16/32 [01:34<01:33,  5.85s/it][A[A

Evaluating:  53%|    | 17/32 [01:40<01:27,  5.86s/it][A[A

Evaluating:  56%|    | 18/32 [01:46<01:22,  5.86s/it][A[A

Evaluating:  59%|    | 19/32 [01:52<01:16,  5.87s/it][A[A

Evaluating:  62%|   | 20/32 [01:57<01:10,  5.86s/it][A[A

Evaluating:  66%|   | 21/32 [02:03<01:04,  5.85s/it][A[A

Evaluating:  69%|   | 22/32 [02:09<00:58,  5.85s/it][A[A

Evaluating:  72%|  | 23/32 [02:15<00:52,  5.85s/it][A[A

Evaluating:  75%|  | 24/32 [02:21<00:46,  5.84s/it][A[A

Evaluating:  78%|  | 25/32 [02:27<00:40,  5.84s/it][A[A

Evaluating:  81%| | 26/32 [02:32<00:35,  5.84s/it][A[A

Evaluating:  84%| | 27/32 [02:38<00:29,  5.82s/it][A[A

Evaluating:  88%| | 28/32 [02:44<00:23,  5.82s/it][A[A

Evaluating:  91%| | 29/32 [02:50<00:17,  5.82s/it][A[A

Evaluating:  94%|| 30/32 [02:57<00:12,  6.09s/it][A[A

Evaluating:  97%|| 31/32 [03:03<00:06,  6.18s/it][A[A

Evaluating: 100%|| 32/32 [03:05<00:00,  4.82s/it][A[AEvaluating: 100%|| 32/32 [03:05<00:00,  5.78s/it]
03/11/2022 07:08:20 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 07:08:20 - INFO - __main__ -     acc = 0.952
03/11/2022 07:08:20 - INFO - __main__ -     auc = 0.9772075326764046
03/11/2022 07:08:20 - INFO - __main__ -     f1 = 0.9519807923169268
03/11/2022 07:08:20 - INFO - __main__ -     mcc = 0.906166834924617
03/11/2022 07:08:20 - INFO - __main__ -     precision = 0.9537009920929431
03/11/2022 07:08:20 - INFO - __main__ -     recall = 0.9524666834699601
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  41%|      | 56/138 [38:19<1:45:49, 77.44s/it][A
Iteration:  41%|     | 57/138 [38:39<1:21:18, 60.23s/it][A
Iteration:  42%|     | 58/138 [38:58<1:03:41, 47.77s/it][A
Iteration:  43%|     | 59/138 [39:17<51:21, 39.00s/it]  [A
Iteration:  43%|     | 60/138 [39:35<42:43, 32.87s/it][A
Iteration:  44%|     | 61/138 [39:54<36:40, 28.58s/it][A
Iteration:  45%|     | 62/138 [40:13<32:25, 25.60s/it][A
Iteration:  46%|     | 63/138 [40:32<29:48, 23.84s/it][A
Iteration:  46%|     | 64/138 [40:51<27:27, 22.26s/it][A
Iteration:  47%|     | 65/138 [41:09<25:42, 21.13s/it][A03/11/2022 07:11:29 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 07:11:29 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 07:11:29 - INFO - __main__ -     Num examples = 1000
03/11/2022 07:11:29 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.952, "eval_f1": 0.9519807923169268, "eval_mcc": 0.906166834924617, "eval_auc": 0.9772075326764046, "eval_precision": 0.9537009920929431, "eval_recall": 0.9524666834699601, "learning_rate": 7.085346215781e-05, "loss": 0.05794556336477399, "step": 470}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:05<02:59,  5.80s/it][A[A

Evaluating:   6%|         | 2/32 [00:11<02:54,  5.82s/it][A[A

Evaluating:   9%|         | 3/32 [00:17<02:48,  5.82s/it][A[A

Evaluating:  12%|        | 4/32 [00:23<02:43,  5.82s/it][A[A

Evaluating:  16%|        | 5/32 [00:29<02:37,  5.82s/it][A[A

Evaluating:  19%|        | 6/32 [00:34<02:31,  5.82s/it][A[A

Evaluating:  22%|       | 7/32 [00:40<02:26,  5.85s/it][A[A

Evaluating:  25%|       | 8/32 [00:46<02:20,  5.84s/it][A[A

Evaluating:  28%|       | 9/32 [00:52<02:14,  5.83s/it][A[A

Evaluating:  31%|      | 10/32 [00:58<02:08,  5.83s/it][A[A

Evaluating:  34%|      | 11/32 [01:04<02:02,  5.82s/it][A[A

Evaluating:  38%|      | 12/32 [01:09<01:56,  5.81s/it][A[A

Evaluating:  41%|      | 13/32 [01:15<01:50,  5.82s/it][A[A

Evaluating:  44%|     | 14/32 [01:21<01:44,  5.82s/it][A[A

Evaluating:  47%|     | 15/32 [01:27<01:38,  5.82s/it][A[A

Evaluating:  50%|     | 16/32 [01:33<01:33,  5.82s/it][A[A

Evaluating:  53%|    | 17/32 [01:39<01:27,  5.82s/it][A[A

Evaluating:  56%|    | 18/32 [01:44<01:21,  5.82s/it][A[A

Evaluating:  59%|    | 19/32 [01:50<01:15,  5.81s/it][A[A

Evaluating:  62%|   | 20/32 [01:56<01:09,  5.82s/it][A[A

Evaluating:  66%|   | 21/32 [02:02<01:04,  5.82s/it][A[A

Evaluating:  69%|   | 22/32 [02:08<00:58,  5.82s/it][A[A

Evaluating:  72%|  | 23/32 [02:13<00:52,  5.83s/it][A[A

Evaluating:  75%|  | 24/32 [02:19<00:46,  5.83s/it][A[A

Evaluating:  78%|  | 25/32 [02:25<00:40,  5.83s/it][A[A

Evaluating:  81%| | 26/32 [02:31<00:34,  5.83s/it][A[A

Evaluating:  84%| | 27/32 [02:37<00:29,  5.83s/it][A[A

Evaluating:  88%| | 28/32 [02:43<00:23,  5.82s/it][A[A

Evaluating:  91%| | 29/32 [02:48<00:17,  5.84s/it][A[A

Evaluating:  94%|| 30/32 [02:55<00:12,  6.04s/it][A[A

Evaluating:  97%|| 31/32 [03:01<00:06,  6.13s/it][A[A

Evaluating: 100%|| 32/32 [03:03<00:00,  4.75s/it][A[AEvaluating: 100%|| 32/32 [03:03<00:00,  5.73s/it]
03/11/2022 07:14:32 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 07:14:32 - INFO - __main__ -     acc = 0.955
03/11/2022 07:14:32 - INFO - __main__ -     auc = 0.9777476385371533
03/11/2022 07:14:32 - INFO - __main__ -     f1 = 0.9549977948919497
03/11/2022 07:14:32 - INFO - __main__ -     mcc = 0.9108383501754967
03/11/2022 07:14:32 - INFO - __main__ -     precision = 0.9555531536724793
03/11/2022 07:14:32 - INFO - __main__ -     recall = 0.9552852359062376
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  48%|     | 66/138 [44:31<1:30:26, 75.36s/it][A
Iteration:  49%|     | 67/138 [44:50<1:09:12, 58.48s/it][A
Iteration:  49%|     | 68/138 [45:09<54:17, 46.54s/it]  [A
Iteration:  50%|     | 69/138 [45:28<43:52, 38.15s/it][A
Iteration:  51%|     | 70/138 [45:46<36:33, 32.26s/it][A
Iteration:  51%|    | 71/138 [46:05<31:27, 28.17s/it][A
Iteration:  52%|    | 72/138 [46:23<27:48, 25.28s/it][A
Iteration:  53%|    | 73/138 [46:42<25:12, 23.26s/it][A
Iteration:  54%|    | 74/138 [47:00<23:20, 21.88s/it][A
Iteration:  54%|    | 75/138 [47:19<21:57, 20.91s/it][A03/11/2022 07:17:40 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 07:17:40 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 07:17:40 - INFO - __main__ -     Num examples = 1000
03/11/2022 07:17:40 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.955, "eval_f1": 0.9549977948919497, "eval_mcc": 0.9108383501754967, "eval_auc": 0.9777476385371533, "eval_precision": 0.9555531536724793, "eval_recall": 0.9552852359062376, "learning_rate": 6.76328502415459e-05, "loss": 0.04958220585249364, "step": 480}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:05<03:00,  5.83s/it][A[A

Evaluating:   6%|         | 2/32 [00:11<02:54,  5.83s/it][A[A

Evaluating:   9%|         | 3/32 [00:17<02:48,  5.83s/it][A[A

Evaluating:  12%|        | 4/32 [00:23<02:43,  5.82s/it][A[A

Evaluating:  16%|        | 5/32 [00:29<02:37,  5.82s/it][A[A

Evaluating:  19%|        | 6/32 [00:34<02:31,  5.82s/it][A[A

Evaluating:  22%|       | 7/32 [00:40<02:25,  5.82s/it][A[A

Evaluating:  25%|       | 8/32 [00:46<02:19,  5.82s/it][A[A

Evaluating:  28%|       | 9/32 [00:52<02:13,  5.82s/it][A[A

Evaluating:  31%|      | 10/32 [00:58<02:07,  5.81s/it][A[A

Evaluating:  34%|      | 11/32 [01:03<02:01,  5.81s/it][A[A

Evaluating:  38%|      | 12/32 [01:09<01:56,  5.83s/it][A[A

Evaluating:  41%|      | 13/32 [01:15<01:50,  5.83s/it][A[A

Evaluating:  44%|     | 14/32 [01:21<01:45,  5.83s/it][A[A

Evaluating:  47%|     | 15/32 [01:27<01:39,  5.83s/it][A[A

Evaluating:  50%|     | 16/32 [01:33<01:33,  5.83s/it][A[A

Evaluating:  53%|    | 17/32 [01:38<01:27,  5.82s/it][A[A

Evaluating:  56%|    | 18/32 [01:44<01:21,  5.81s/it][A[A

Evaluating:  59%|    | 19/32 [01:50<01:15,  5.81s/it][A[A

Evaluating:  62%|   | 20/32 [01:56<01:09,  5.81s/it][A[A

Evaluating:  66%|   | 21/32 [02:02<01:03,  5.82s/it][A[A

Evaluating:  69%|   | 22/32 [02:08<00:58,  5.81s/it][A[A

Evaluating:  72%|  | 23/32 [02:13<00:52,  5.81s/it][A[A

Evaluating:  75%|  | 24/32 [02:19<00:46,  5.81s/it][A[A

Evaluating:  78%|  | 25/32 [02:25<00:40,  5.82s/it][A[A

Evaluating:  81%| | 26/32 [02:31<00:34,  5.83s/it][A[A

Evaluating:  84%| | 27/32 [02:37<00:29,  5.83s/it][A[A

Evaluating:  88%| | 28/32 [02:42<00:23,  5.82s/it][A[A

Evaluating:  91%| | 29/32 [02:48<00:17,  5.83s/it][A[A

Evaluating:  94%|| 30/32 [02:54<00:11,  5.83s/it][A[A

Evaluating:  97%|| 31/32 [03:00<00:05,  5.84s/it][A[A

Evaluating: 100%|| 32/32 [03:02<00:00,  4.54s/it][A[AEvaluating: 100%|| 32/32 [03:02<00:00,  5.69s/it]
03/11/2022 07:20:42 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 07:20:42 - INFO - __main__ -     acc = 0.952
03/11/2022 07:20:42 - INFO - __main__ -     auc = 0.9786478149717345
03/11/2022 07:20:42 - INFO - __main__ -     f1 = 0.9519905901556704
03/11/2022 07:20:42 - INFO - __main__ -     mcc = 0.9054756446828309
03/11/2022 07:20:42 - INFO - __main__ -     precision = 0.9530932565044739
03/11/2022 07:20:42 - INFO - __main__ -     recall = 0.9523826670027324
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  55%|    | 76/138 [50:41<1:17:44, 75.24s/it][A
Iteration:  56%|    | 77/138 [51:00<59:22, 58.40s/it]  [A
Iteration:  57%|    | 78/138 [51:19<46:30, 46.51s/it][A
Iteration:  57%|    | 79/138 [51:39<37:52, 38.52s/it][A
Iteration:  58%|    | 80/138 [51:57<31:27, 32.55s/it][A
Iteration:  59%|    | 81/138 [52:16<26:55, 28.35s/it][A
Iteration:  59%|    | 82/138 [52:35<23:43, 25.42s/it][A
Iteration:  60%|    | 83/138 [52:53<21:27, 23.40s/it][A
Iteration:  61%|    | 84/138 [53:12<19:45, 21.96s/it][A
Iteration:  62%|   | 85/138 [53:30<18:29, 20.94s/it][A03/11/2022 07:23:51 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 07:23:51 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 07:23:51 - INFO - __main__ -     Num examples = 1000
03/11/2022 07:23:51 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.952, "eval_f1": 0.9519905901556704, "eval_mcc": 0.9054756446828309, "eval_auc": 0.9786478149717345, "eval_precision": 0.9530932565044739, "eval_recall": 0.9523826670027324, "learning_rate": 6.44122383252818e-05, "loss": 0.10743435341864824, "step": 490}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:05<03:00,  5.83s/it][A[A

Evaluating:   6%|         | 2/32 [00:11<02:55,  5.83s/it][A[A

Evaluating:   9%|         | 3/32 [00:17<02:49,  5.84s/it][A[A

Evaluating:  12%|        | 4/32 [00:23<02:43,  5.84s/it][A[A

Evaluating:  16%|        | 5/32 [00:29<02:37,  5.82s/it][A[A

Evaluating:  19%|        | 6/32 [00:34<02:31,  5.81s/it][A[A

Evaluating:  22%|       | 7/32 [00:40<02:25,  5.80s/it][A[A

Evaluating:  25%|       | 8/32 [00:46<02:19,  5.81s/it][A[A

Evaluating:  28%|       | 9/32 [00:52<02:13,  5.81s/it][A[A

Evaluating:  31%|      | 10/32 [00:58<02:08,  5.82s/it][A[A

Evaluating:  34%|      | 11/32 [01:04<02:02,  5.82s/it][A[A

Evaluating:  38%|      | 12/32 [01:09<01:56,  5.82s/it][A[A

Evaluating:  41%|      | 13/32 [01:15<01:50,  5.81s/it][A[A

Evaluating:  44%|     | 14/32 [01:21<01:44,  5.81s/it][A[A

Evaluating:  47%|     | 15/32 [01:27<01:39,  5.83s/it][A[A

Evaluating:  50%|     | 16/32 [01:33<01:33,  5.84s/it][A[A

Evaluating:  53%|    | 17/32 [01:39<01:27,  5.84s/it][A[A

Evaluating:  56%|    | 18/32 [01:44<01:21,  5.83s/it][A[A

Evaluating:  59%|    | 19/32 [01:50<01:15,  5.83s/it][A[A

Evaluating:  62%|   | 20/32 [01:56<01:10,  5.84s/it][A[A

Evaluating:  66%|   | 21/32 [02:02<01:04,  5.84s/it][A[A

Evaluating:  69%|   | 22/32 [02:08<00:58,  5.85s/it][A[A

Evaluating:  72%|  | 23/32 [02:14<00:52,  5.85s/it][A[A

Evaluating:  75%|  | 24/32 [02:19<00:46,  5.85s/it][A[A

Evaluating:  78%|  | 25/32 [02:25<00:40,  5.85s/it][A[A

Evaluating:  81%| | 26/32 [02:31<00:35,  5.85s/it][A[A

Evaluating:  84%| | 27/32 [02:37<00:29,  5.84s/it][A[A

Evaluating:  88%| | 28/32 [02:43<00:23,  5.84s/it][A[A

Evaluating:  91%| | 29/32 [02:49<00:17,  5.85s/it][A[A

Evaluating:  94%|| 30/32 [02:55<00:11,  5.85s/it][A[A

Evaluating:  97%|| 31/32 [03:00<00:05,  5.85s/it][A[A

Evaluating: 100%|| 32/32 [03:02<00:00,  4.55s/it][A[AEvaluating: 100%|| 32/32 [03:02<00:00,  5.70s/it]
03/11/2022 07:26:54 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 07:26:54 - INFO - __main__ -     acc = 0.95
03/11/2022 07:26:54 - INFO - __main__ -     auc = 0.9795539925825462
03/11/2022 07:26:54 - INFO - __main__ -     f1 = 0.949987196722361
03/11/2022 07:26:54 - INFO - __main__ -     mcc = 0.9016867658795853
03/11/2022 07:26:54 - INFO - __main__ -     precision = 0.9512769019244722
03/11/2022 07:26:54 - INFO - __main__ -     recall = 0.9504102804149613
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  62%|   | 86/138 [56:53<1:05:18, 75.35s/it][A
Iteration:  63%|   | 87/138 [57:12<49:43, 58.51s/it]  [A
Iteration:  64%|   | 88/138 [57:31<38:49, 46.59s/it][A
Iteration:  64%|   | 89/138 [57:49<31:11, 38.19s/it][A
Iteration:  65%|   | 90/138 [58:08<25:51, 32.32s/it][A
Iteration:  66%|   | 91/138 [58:27<22:05, 28.20s/it][A
Iteration:  67%|   | 92/138 [58:46<19:41, 25.68s/it][A
Iteration:  67%|   | 93/138 [59:06<17:56, 23.92s/it][A
Iteration:  68%|   | 94/138 [59:25<16:22, 22.34s/it][A
Iteration:  69%|   | 95/138 [59:43<15:12, 21.23s/it][A03/11/2022 07:30:03 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 07:30:03 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 07:30:03 - INFO - __main__ -     Num examples = 1000
03/11/2022 07:30:03 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.95, "eval_f1": 0.949987196722361, "eval_mcc": 0.9016867658795853, "eval_auc": 0.9795539925825462, "eval_precision": 0.9512769019244722, "eval_recall": 0.9504102804149613, "learning_rate": 6.119162640901772e-05, "loss": 0.11501377900131046, "step": 500}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:05<02:59,  5.80s/it][A[A

Evaluating:   6%|         | 2/32 [00:11<02:54,  5.81s/it][A[A

Evaluating:   9%|         | 3/32 [00:17<02:48,  5.82s/it][A[A

Evaluating:  12%|        | 4/32 [00:23<02:43,  5.84s/it][A[A

Evaluating:  16%|        | 5/32 [00:29<02:37,  5.84s/it][A[A

Evaluating:  19%|        | 6/32 [00:35<02:31,  5.85s/it][A[A

Evaluating:  22%|       | 7/32 [00:41<02:27,  5.91s/it][A[A

Evaluating:  25%|       | 8/32 [00:47<02:26,  6.10s/it][A[A

Evaluating:  28%|       | 9/32 [00:53<02:21,  6.17s/it][A[A

Evaluating:  31%|      | 10/32 [00:59<02:13,  6.07s/it][A[A

Evaluating:  34%|      | 11/32 [01:05<02:06,  6.01s/it][A[A

Evaluating:  38%|      | 12/32 [01:11<01:59,  5.96s/it][A[A

Evaluating:  41%|      | 13/32 [01:17<01:52,  5.92s/it][A[A

Evaluating:  44%|     | 14/32 [01:23<01:46,  5.89s/it][A[A

Evaluating:  47%|     | 15/32 [01:28<01:39,  5.87s/it][A[A

Evaluating:  50%|     | 16/32 [01:34<01:33,  5.87s/it][A[A

Evaluating:  53%|    | 17/32 [01:40<01:27,  5.85s/it][A[A

Evaluating:  56%|    | 18/32 [01:46<01:21,  5.85s/it][A[A

Evaluating:  59%|    | 19/32 [01:52<01:16,  5.85s/it][A[A

Evaluating:  62%|   | 20/32 [01:58<01:10,  5.85s/it][A[A

Evaluating:  66%|   | 21/32 [02:03<01:04,  5.85s/it][A[A

Evaluating:  69%|   | 22/32 [02:09<00:58,  5.86s/it][A[A

Evaluating:  72%|  | 23/32 [02:15<00:52,  5.86s/it][A[A

Evaluating:  75%|  | 24/32 [02:21<00:46,  5.86s/it][A[A

Evaluating:  78%|  | 25/32 [02:27<00:40,  5.84s/it][A[A

Evaluating:  81%| | 26/32 [02:33<00:34,  5.83s/it][A[A

Evaluating:  84%| | 27/32 [02:39<00:29,  5.83s/it][A[A

Evaluating:  88%| | 28/32 [02:44<00:23,  5.86s/it][A[A

Evaluating:  91%| | 29/32 [02:51<00:18,  6.06s/it][A[A

Evaluating:  94%|| 30/32 [02:57<00:12,  6.16s/it][A[A

Evaluating:  97%|| 31/32 [03:03<00:06,  6.07s/it][A[A

Evaluating: 100%|| 32/32 [03:05<00:00,  4.70s/it][A[AEvaluating: 100%|| 32/32 [03:05<00:00,  5.79s/it]
03/11/2022 07:33:08 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 07:33:08 - INFO - __main__ -     acc = 0.944
03/11/2022 07:33:08 - INFO - __main__ -     auc = 0.978287744397902
03/11/2022 07:33:08 - INFO - __main__ -     f1 = 0.9439351890785749
03/11/2022 07:33:08 - INFO - __main__ -     mcc = 0.8922399194624376
03/11/2022 07:33:08 - INFO - __main__ -     precision = 0.9475835518023188
03/11/2022 07:33:08 - INFO - __main__ -     recall = 0.9446611535861029
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  70%|   | 96/138 [1:03:07<53:11, 75.98s/it][A
Iteration:  70%|   | 97/138 [1:03:26<40:14, 58.90s/it][A
Iteration:  71%|   | 98/138 [1:03:45<31:14, 46.85s/it][A
Iteration:  72%|  | 99/138 [1:04:04<24:57, 38.40s/it][A
Iteration:  72%|  | 100/138 [1:04:22<20:33, 32.47s/it][A
Iteration:  73%|  | 101/138 [1:04:41<17:28, 28.34s/it][A
Iteration:  74%|  | 102/138 [1:05:02<15:36, 26.01s/it][A
Iteration:  75%|  | 103/138 [1:05:21<13:56, 23.90s/it][A
Iteration:  75%|  | 104/138 [1:05:39<12:37, 22.27s/it][A
Iteration:  76%|  | 105/138 [1:05:58<11:38, 21.17s/it][A03/11/2022 07:36:17 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 07:36:17 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 07:36:17 - INFO - __main__ -     Num examples = 1000
03/11/2022 07:36:17 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.944, "eval_f1": 0.9439351890785749, "eval_mcc": 0.8922399194624376, "eval_auc": 0.978287744397902, "eval_precision": 0.9475835518023188, "eval_recall": 0.9446611535861029, "learning_rate": 5.797101449275363e-05, "loss": 0.06942168176174164, "step": 510}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:05<03:00,  5.82s/it][A[A

Evaluating:   6%|         | 2/32 [00:11<02:54,  5.81s/it][A[A

Evaluating:   9%|         | 3/32 [00:17<02:48,  5.83s/it][A[A

Evaluating:  12%|        | 4/32 [00:23<02:42,  5.82s/it][A[A

Evaluating:  16%|        | 5/32 [00:29<02:37,  5.82s/it][A[A

Evaluating:  19%|        | 6/32 [00:35<02:37,  6.05s/it][A[A

Evaluating:  22%|       | 7/32 [00:41<02:33,  6.15s/it][A[A

Evaluating:  25%|       | 8/32 [00:47<02:25,  6.06s/it][A[A

Evaluating:  28%|       | 9/32 [00:53<02:17,  5.99s/it][A[A

Evaluating:  31%|      | 10/32 [00:59<02:10,  5.95s/it][A[A

Evaluating:  34%|      | 11/32 [01:05<02:04,  5.91s/it][A[A

Evaluating:  38%|      | 12/32 [01:11<01:57,  5.88s/it][A[A

Evaluating:  41%|      | 13/32 [01:16<01:51,  5.86s/it][A[A

Evaluating:  44%|     | 14/32 [01:22<01:44,  5.83s/it][A[A

Evaluating:  47%|     | 15/32 [01:28<01:39,  5.83s/it][A[A

Evaluating:  50%|     | 16/32 [01:34<01:34,  5.90s/it][A[A

Evaluating:  53%|    | 17/32 [01:41<01:31,  6.12s/it][A[A

Evaluating:  56%|    | 18/32 [01:47<01:26,  6.20s/it][A[A

Evaluating:  59%|    | 19/32 [01:53<01:21,  6.25s/it][A[A

Evaluating:  62%|   | 20/32 [02:00<01:14,  6.23s/it][A[A

Evaluating:  66%|   | 21/32 [02:05<01:07,  6.10s/it][A[A

Evaluating:  69%|   | 22/32 [02:11<01:00,  6.02s/it][A[A

Evaluating:  72%|  | 23/32 [02:17<00:53,  5.96s/it][A[A

Evaluating:  75%|  | 24/32 [02:23<00:47,  5.92s/it][A[A

Evaluating:  78%|  | 25/32 [02:29<00:41,  5.89s/it][A[A

Evaluating:  81%| | 26/32 [02:35<00:35,  5.87s/it][A[A

Evaluating:  84%| | 27/32 [02:40<00:29,  5.85s/it][A[A

Evaluating:  88%| | 28/32 [02:46<00:23,  5.84s/it][A[A

Evaluating:  91%| | 29/32 [02:52<00:17,  5.84s/it][A[A

Evaluating:  94%|| 30/32 [02:58<00:11,  5.84s/it][A[A

Evaluating:  97%|| 31/32 [03:04<00:05,  5.84s/it][A[A

Evaluating: 100%|| 32/32 [03:05<00:00,  4.54s/it][A[AEvaluating: 100%|| 32/32 [03:05<00:00,  5.80s/it]
03/11/2022 07:39:23 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 07:39:23 - INFO - __main__ -     acc = 0.953
03/11/2022 07:39:23 - INFO - __main__ -     auc = 0.9785718000728143
03/11/2022 07:39:23 - INFO - __main__ -     f1 = 0.9529894226200897
03/11/2022 07:39:23 - INFO - __main__ -     mcc = 0.9075838282025641
03/11/2022 07:39:23 - INFO - __main__ -     precision = 0.9541873066256272
03/11/2022 07:39:23 - INFO - __main__ -     recall = 0.9533968657856939
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  77%|  | 106/138 [1:09:22<40:34, 76.09s/it][A
Iteration:  78%|  | 107/138 [1:09:41<30:27, 58.95s/it][A
Iteration:  78%|  | 108/138 [1:10:00<23:31, 47.07s/it][A
Iteration:  79%|  | 109/138 [1:10:19<18:42, 38.72s/it][A
Iteration:  80%|  | 110/138 [1:10:38<15:15, 32.71s/it][A
Iteration:  80%|  | 111/138 [1:10:57<12:48, 28.45s/it][A
Iteration:  81%|  | 112/138 [1:11:15<11:02, 25.48s/it][A
Iteration:  82%| | 113/138 [1:11:34<09:44, 23.38s/it][A
Iteration:  83%| | 114/138 [1:11:52<08:47, 21.96s/it][A
Iteration:  83%| | 115/138 [1:12:11<08:02, 20.97s/it][A03/11/2022 07:42:31 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 07:42:31 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 07:42:31 - INFO - __main__ -     Num examples = 1000
03/11/2022 07:42:31 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.953, "eval_f1": 0.9529894226200897, "eval_mcc": 0.9075838282025641, "eval_auc": 0.9785718000728143, "eval_precision": 0.9541873066256272, "eval_recall": 0.9533968657856939, "learning_rate": 5.4750402576489535e-05, "loss": 0.08515830310061574, "step": 520}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:05<02:59,  5.78s/it][A[A

Evaluating:   6%|         | 2/32 [00:11<02:54,  5.81s/it][A[A

Evaluating:   9%|         | 3/32 [00:17<02:48,  5.81s/it][A[A

Evaluating:  12%|        | 4/32 [00:23<02:42,  5.80s/it][A[A

Evaluating:  16%|        | 5/32 [00:28<02:36,  5.80s/it][A[A

Evaluating:  19%|        | 6/32 [00:34<02:30,  5.80s/it][A[A

Evaluating:  22%|       | 7/32 [00:40<02:25,  5.81s/it][A[A

Evaluating:  25%|       | 8/32 [00:46<02:19,  5.82s/it][A[A

Evaluating:  28%|       | 9/32 [00:52<02:13,  5.82s/it][A[A

Evaluating:  31%|      | 10/32 [00:58<02:07,  5.82s/it][A[A

Evaluating:  34%|      | 11/32 [01:03<02:02,  5.83s/it][A[A

Evaluating:  38%|      | 12/32 [01:09<01:56,  5.83s/it][A[A

Evaluating:  41%|      | 13/32 [01:15<01:50,  5.84s/it][A[A

Evaluating:  44%|     | 14/32 [01:21<01:45,  5.84s/it][A[A

Evaluating:  47%|     | 15/32 [01:27<01:39,  5.84s/it][A[A

Evaluating:  50%|     | 16/32 [01:33<01:36,  6.00s/it][A[A

Evaluating:  53%|    | 17/32 [01:40<01:31,  6.11s/it][A[A

Evaluating:  56%|    | 18/32 [01:46<01:25,  6.09s/it][A[A

Evaluating:  59%|    | 19/32 [01:51<01:17,  6.00s/it][A[A

Evaluating:  62%|   | 20/32 [01:57<01:11,  5.96s/it][A[A

Evaluating:  66%|   | 21/32 [02:03<01:05,  5.92s/it][A[A

Evaluating:  69%|   | 22/32 [02:09<00:58,  5.89s/it][A[A

Evaluating:  72%|  | 23/32 [02:15<00:52,  5.87s/it][A[A

Evaluating:  75%|  | 24/32 [02:21<00:46,  5.86s/it][A[A

Evaluating:  78%|  | 25/32 [02:26<00:40,  5.85s/it][A[A

Evaluating:  81%| | 26/32 [02:32<00:35,  5.84s/it][A[A

Evaluating:  84%| | 27/32 [02:38<00:29,  5.84s/it][A[A

Evaluating:  88%| | 28/32 [02:44<00:23,  5.84s/it][A[A

Evaluating:  91%| | 29/32 [02:50<00:17,  5.83s/it][A[A

Evaluating:  94%|| 30/32 [02:56<00:11,  5.83s/it][A[A

Evaluating:  97%|| 31/32 [03:01<00:05,  5.82s/it][A[A

Evaluating: 100%|| 32/32 [03:03<00:00,  4.54s/it][A[AEvaluating: 100%|| 32/32 [03:03<00:00,  5.73s/it]
03/11/2022 07:45:34 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 07:45:34 - INFO - __main__ -     acc = 0.949
03/11/2022 07:45:34 - INFO - __main__ -     auc = 0.9811243003628711
03/11/2022 07:45:34 - INFO - __main__ -     f1 = 0.9489987249681242
03/11/2022 07:45:34 - INFO - __main__ -     mcc = 0.8981512076659106
03/11/2022 07:45:34 - INFO - __main__ -     precision = 0.949035184562953
03/11/2022 07:45:34 - INFO - __main__ -     recall = 0.9491160267412413
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  84%| | 116/138 [1:15:33<27:36, 75.28s/it][A
Iteration:  85%| | 117/138 [1:15:52<20:26, 58.41s/it][A
Iteration:  86%| | 118/138 [1:16:11<15:30, 46.52s/it][A
Iteration:  86%| | 119/138 [1:16:29<12:04, 38.14s/it][A
Iteration:  87%| | 120/138 [1:16:48<09:41, 32.28s/it][A
Iteration:  88%| | 121/138 [1:17:07<08:02, 28.36s/it][A
Iteration:  88%| | 122/138 [1:17:28<06:55, 25.96s/it][A
Iteration:  89%| | 123/138 [1:17:46<05:56, 23.77s/it][A
Iteration:  90%| | 124/138 [1:18:05<05:11, 22.23s/it][A
Iteration:  91%| | 125/138 [1:18:23<04:34, 21.12s/it][A03/11/2022 07:48:43 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 07:48:43 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 07:48:43 - INFO - __main__ -     Num examples = 1000
03/11/2022 07:48:43 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.949, "eval_f1": 0.9489987249681242, "eval_mcc": 0.8981512076659106, "eval_auc": 0.9811243003628711, "eval_precision": 0.949035184562953, "eval_recall": 0.9491160267412413, "learning_rate": 5.152979066022544e-05, "loss": 0.08542197365313768, "step": 530}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:05<03:00,  5.81s/it][A[A

Evaluating:   6%|         | 2/32 [00:11<02:54,  5.82s/it][A[A

Evaluating:   9%|         | 3/32 [00:17<02:48,  5.81s/it][A[A

Evaluating:  12%|        | 4/32 [00:23<02:42,  5.81s/it][A[A

Evaluating:  16%|        | 5/32 [00:29<02:36,  5.80s/it][A[A

Evaluating:  19%|        | 6/32 [00:34<02:31,  5.81s/it][A[A

Evaluating:  22%|       | 7/32 [00:40<02:25,  5.82s/it][A[A

Evaluating:  25%|       | 8/32 [00:46<02:19,  5.82s/it][A[A

Evaluating:  28%|       | 9/32 [00:52<02:13,  5.82s/it][A[A

Evaluating:  31%|      | 10/32 [00:58<02:07,  5.81s/it][A[A

Evaluating:  34%|      | 11/32 [01:03<02:02,  5.81s/it][A[A

Evaluating:  38%|      | 12/32 [01:09<01:56,  5.81s/it][A[A

Evaluating:  41%|      | 13/32 [01:15<01:50,  5.81s/it][A[A

Evaluating:  44%|     | 14/32 [01:21<01:44,  5.81s/it][A[A

Evaluating:  47%|     | 15/32 [01:27<01:38,  5.81s/it][A[A

Evaluating:  50%|     | 16/32 [01:33<01:33,  5.82s/it][A[A

Evaluating:  53%|    | 17/32 [01:38<01:27,  5.82s/it][A[A

Evaluating:  56%|    | 18/32 [01:44<01:21,  5.82s/it][A[A

Evaluating:  59%|    | 19/32 [01:50<01:15,  5.81s/it][A[A

Evaluating:  62%|   | 20/32 [01:56<01:09,  5.81s/it][A[A

Evaluating:  66%|   | 21/32 [02:02<01:03,  5.82s/it][A[A

Evaluating:  69%|   | 22/32 [02:07<00:58,  5.81s/it][A[A

Evaluating:  72%|  | 23/32 [02:13<00:52,  5.82s/it][A[A

Evaluating:  75%|  | 24/32 [02:19<00:46,  5.82s/it][A[A

Evaluating:  78%|  | 25/32 [02:25<00:40,  5.83s/it][A[A

Evaluating:  81%| | 26/32 [02:31<00:35,  5.99s/it][A[A

Evaluating:  84%| | 27/32 [02:38<00:30,  6.15s/it][A[A

Evaluating:  88%| | 28/32 [02:44<00:24,  6.22s/it][A[A

Evaluating:  91%| | 29/32 [02:51<00:18,  6.27s/it][A[A

Evaluating:  94%|| 30/32 [02:57<00:12,  6.20s/it][A[A

Evaluating:  97%|| 31/32 [03:02<00:06,  6.08s/it][A[A

Evaluating: 100%|| 32/32 [03:04<00:00,  4.71s/it][A[AEvaluating: 100%|| 32/32 [03:04<00:00,  5.76s/it]
03/11/2022 07:51:47 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 07:51:47 - INFO - __main__ -     acc = 0.94
03/11/2022 07:51:47 - INFO - __main__ -     auc = 0.9772875483594784
03/11/2022 07:51:47 - INFO - __main__ -     f1 = 0.9399594125628925
03/11/2022 07:51:47 - INFO - __main__ -     mcc = 0.8829354754672633
03/11/2022 07:51:47 - INFO - __main__ -     precision = 0.9423890466873701
03/11/2022 07:51:47 - INFO - __main__ -     recall = 0.9405483474761054
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  91%|| 126/138 [1:21:46<15:07, 75.64s/it][A
Iteration:  92%|| 127/138 [1:22:05<10:45, 58.67s/it][A
Iteration:  93%|| 128/138 [1:22:24<07:46, 46.68s/it][A
Iteration:  93%|| 129/138 [1:22:42<05:44, 38.23s/it][A
Iteration:  94%|| 130/138 [1:23:01<04:18, 32.31s/it][A
Iteration:  95%|| 131/138 [1:23:20<03:18, 28.35s/it][A
Iteration:  96%|| 132/138 [1:23:40<02:35, 25.91s/it][A
Iteration:  96%|| 133/138 [1:23:59<01:58, 23.72s/it][A
Iteration:  97%|| 134/138 [1:24:17<01:28, 22.15s/it][A
Iteration:  98%|| 135/138 [1:24:36<01:03, 21.08s/it][A03/11/2022 07:54:55 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 07:54:56 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 07:54:56 - INFO - __main__ -     Num examples = 1000
03/11/2022 07:54:56 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.94, "eval_f1": 0.9399594125628925, "eval_mcc": 0.8829354754672633, "eval_auc": 0.9772875483594784, "eval_precision": 0.9423890466873701, "eval_recall": 0.9405483474761054, "learning_rate": 4.830917874396135e-05, "loss": 0.05964793507009745, "step": 540}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:05<02:59,  5.80s/it][A[A

Evaluating:   6%|         | 2/32 [00:11<02:54,  5.82s/it][A[A

Evaluating:   9%|         | 3/32 [00:17<02:48,  5.81s/it][A[A

Evaluating:  12%|        | 4/32 [00:23<02:42,  5.81s/it][A[A

Evaluating:  16%|        | 5/32 [00:29<02:36,  5.81s/it][A[A

Evaluating:  19%|        | 6/32 [00:34<02:30,  5.80s/it][A[A

Evaluating:  22%|       | 7/32 [00:40<02:24,  5.80s/it][A[A

Evaluating:  25%|       | 8/32 [00:46<02:19,  5.80s/it][A[A

Evaluating:  28%|       | 9/32 [00:52<02:13,  5.81s/it][A[A

Evaluating:  31%|      | 10/32 [00:58<02:08,  5.82s/it][A[A

Evaluating:  34%|      | 11/32 [01:03<02:02,  5.82s/it][A[A

Evaluating:  38%|      | 12/32 [01:09<01:56,  5.83s/it][A[A

Evaluating:  41%|      | 13/32 [01:15<01:50,  5.83s/it][A[A

Evaluating:  44%|     | 14/32 [01:21<01:47,  5.99s/it][A[A

Evaluating:  47%|     | 15/32 [01:28<01:43,  6.10s/it][A[A

Evaluating:  50%|     | 16/32 [01:34<01:37,  6.09s/it][A[A

Evaluating:  53%|    | 17/32 [01:40<01:30,  6.01s/it][A[A

Evaluating:  56%|    | 18/32 [01:46<01:23,  5.96s/it][A[A

Evaluating:  59%|    | 19/32 [01:51<01:16,  5.91s/it][A[A

Evaluating:  62%|   | 20/32 [01:57<01:10,  5.88s/it][A[A

Evaluating:  66%|   | 21/32 [02:03<01:04,  5.87s/it][A[A

Evaluating:  69%|   | 22/32 [02:09<00:58,  5.87s/it][A[A

Evaluating:  72%|  | 23/32 [02:15<00:52,  5.87s/it][A[A

Evaluating:  75%|  | 24/32 [02:21<00:46,  5.86s/it][A[A

Evaluating:  78%|  | 25/32 [02:26<00:40,  5.85s/it][A[A

Evaluating:  81%| | 26/32 [02:32<00:35,  5.85s/it][A[A

Evaluating:  84%| | 27/32 [02:38<00:29,  5.84s/it][A[A

Evaluating:  88%| | 28/32 [02:44<00:23,  5.84s/it][A[A

Evaluating:  91%| | 29/32 [02:50<00:17,  5.84s/it][A[A

Evaluating:  94%|| 30/32 [02:56<00:11,  5.84s/it][A[A

Evaluating:  97%|| 31/32 [03:01<00:05,  5.83s/it][A[A

Evaluating: 100%|| 32/32 [03:03<00:00,  4.53s/it][A[AEvaluating: 100%|| 32/32 [03:03<00:00,  5.73s/it]
03/11/2022 07:57:59 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 07:57:59 - INFO - __main__ -     acc = 0.942
03/11/2022 07:57:59 - INFO - __main__ -     auc = 0.9745870190557349
03/11/2022 07:57:59 - INFO - __main__ -     f1 = 0.9419477529776799
03/11/2022 07:57:59 - INFO - __main__ -     mcc = 0.8875562111727759
03/11/2022 07:57:59 - INFO - __main__ -     precision = 0.9449545712320668
03/11/2022 07:57:59 - INFO - __main__ -     recall = 0.9426047505311042
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  99%|| 136/138 [1:27:58<02:30, 75.34s/it][A
Iteration:  99%|| 137/138 [1:28:17<00:58, 58.50s/it][A
Iteration: 100%|| 138/138 [1:28:23<00:00, 42.66s/it][AIteration: 100%|| 138/138 [1:28:23<00:00, 38.43s/it]
Epoch:  80%|  | 4/5 [6:03:24<1:30:31, 5431.63s/it]{"eval_acc": 0.942, "eval_f1": 0.9419477529776799, "eval_mcc": 0.8875562111727759, "eval_auc": 0.9745870190557349, "eval_precision": 0.9449545712320668, "eval_recall": 0.9426047505311042, "learning_rate": 4.5088566827697266e-05, "loss": 0.03891062543261796, "step": 550}

Iteration:   0%|          | 0/138 [00:00<?, ?it/s][A
Iteration:   1%|          | 1/138 [00:20<46:11, 20.23s/it][A
Iteration:   1%|         | 2/138 [00:38<43:40, 19.27s/it][A
Iteration:   2%|         | 3/138 [00:57<42:43, 18.99s/it][A
Iteration:   3%|         | 4/138 [01:16<42:05, 18.85s/it][A
Iteration:   4%|         | 5/138 [01:34<41:30, 18.73s/it][A
Iteration:   4%|         | 6/138 [01:53<41:01, 18.65s/it][A
Iteration:   5%|         | 7/138 [02:12<41:32, 19.03s/it][A03/11/2022 08:00:55 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 08:00:56 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 08:00:56 - INFO - __main__ -     Num examples = 1000
03/11/2022 08:00:56 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:05<03:00,  5.82s/it][A[A

Evaluating:   6%|         | 2/32 [00:11<02:55,  5.84s/it][A[A

Evaluating:   9%|         | 3/32 [00:17<02:49,  5.86s/it][A[A

Evaluating:  12%|        | 4/32 [00:23<02:43,  5.84s/it][A[A

Evaluating:  16%|        | 5/32 [00:29<02:37,  5.83s/it][A[A

Evaluating:  19%|        | 6/32 [00:34<02:31,  5.82s/it][A[A

Evaluating:  22%|       | 7/32 [00:40<02:25,  5.83s/it][A[A

Evaluating:  25%|       | 8/32 [00:46<02:19,  5.82s/it][A[A

Evaluating:  28%|       | 9/32 [00:52<02:13,  5.82s/it][A[A

Evaluating:  31%|      | 10/32 [00:58<02:08,  5.82s/it][A[A

Evaluating:  34%|      | 11/32 [01:04<02:02,  5.83s/it][A[A

Evaluating:  38%|      | 12/32 [01:09<01:56,  5.84s/it][A[A

Evaluating:  41%|      | 13/32 [01:15<01:51,  5.84s/it][A[A

Evaluating:  44%|     | 14/32 [01:21<01:45,  5.84s/it][A[A

Evaluating:  47%|     | 15/32 [01:27<01:39,  5.83s/it][A[A

Evaluating:  50%|     | 16/32 [01:33<01:35,  5.97s/it][A[A

Evaluating:  53%|    | 17/32 [01:40<01:32,  6.14s/it][A[A

Evaluating:  56%|    | 18/32 [01:46<01:27,  6.22s/it][A[A

Evaluating:  59%|    | 19/32 [01:53<01:21,  6.26s/it][A[A

Evaluating:  62%|   | 20/32 [01:59<01:14,  6.22s/it][A[A

Evaluating:  66%|   | 21/32 [02:05<01:07,  6.10s/it][A[A

Evaluating:  69%|   | 22/32 [02:10<01:00,  6.02s/it][A[A

Evaluating:  72%|  | 23/32 [02:16<00:53,  5.97s/it][A[A

Evaluating:  75%|  | 24/32 [02:22<00:47,  5.92s/it][A[A

Evaluating:  78%|  | 25/32 [02:28<00:41,  5.88s/it][A[A

Evaluating:  81%| | 26/32 [02:34<00:35,  5.87s/it][A[A

Evaluating:  84%| | 27/32 [02:39<00:29,  5.86s/it][A[A

Evaluating:  88%| | 28/32 [02:45<00:23,  5.84s/it][A[A

Evaluating:  91%| | 29/32 [02:51<00:17,  5.85s/it][A[A

Evaluating:  94%|| 30/32 [02:57<00:11,  5.84s/it][A[A

Evaluating:  97%|| 31/32 [03:03<00:05,  5.83s/it][A[A

Evaluating: 100%|| 32/32 [03:04<00:00,  4.53s/it][A[AEvaluating: 100%|| 32/32 [03:04<00:00,  5.77s/it]
03/11/2022 08:04:00 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 08:04:00 - INFO - __main__ -     acc = 0.951
03/11/2022 08:04:00 - INFO - __main__ -     auc = 0.9807762321414998
03/11/2022 08:04:00 - INFO - __main__ -     f1 = 0.950999950999951
03/11/2022 08:04:00 - INFO - __main__ -     mcc = 0.9023213949421767
03/11/2022 08:04:00 - INFO - __main__ -     precision = 0.951148965451025
03/11/2022 08:04:00 - INFO - __main__ -     recall = 0.9511724297962401
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:   6%|         | 8/138 [05:36<2:48:29, 77.77s/it][A
Iteration:   7%|         | 9/138 [05:55<2:07:44, 59.41s/it][A
Iteration:   7%|         | 10/138 [06:14<1:39:53, 46.82s/it][A
Iteration:   8%|         | 11/138 [06:32<1:20:52, 38.21s/it][A
Iteration:   9%|         | 12/138 [06:51<1:07:46, 32.27s/it][A
Iteration:   9%|         | 13/138 [07:10<58:35, 28.12s/it]  [A
Iteration:  10%|         | 14/138 [07:28<52:09, 25.24s/it][A
Iteration:  11%|         | 15/138 [07:47<47:39, 23.25s/it][A
Iteration:  12%|        | 16/138 [08:06<44:28, 21.88s/it][A
Iteration:  12%|        | 17/138 [08:24<42:07, 20.89s/it][A03/11/2022 08:07:07 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 08:07:07 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 08:07:07 - INFO - __main__ -     Num examples = 1000
03/11/2022 08:07:07 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.951, "eval_f1": 0.950999950999951, "eval_mcc": 0.9023213949421767, "eval_auc": 0.9807762321414998, "eval_precision": 0.951148965451025, "eval_recall": 0.9511724297962401, "learning_rate": 4.1867954911433174e-05, "loss": 0.07101221535122022, "step": 560}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:05<02:59,  5.80s/it][A[A

Evaluating:   6%|         | 2/32 [00:11<02:54,  5.81s/it][A[A

Evaluating:   9%|         | 3/32 [00:17<02:48,  5.82s/it][A[A

Evaluating:  12%|        | 4/32 [00:23<02:42,  5.82s/it][A[A

Evaluating:  16%|        | 5/32 [00:29<02:40,  5.95s/it][A[A

Evaluating:  19%|        | 6/32 [00:35<02:40,  6.15s/it][A[A

Evaluating:  22%|       | 7/32 [00:42<02:35,  6.24s/it][A[A

Evaluating:  25%|       | 8/32 [00:48<02:30,  6.27s/it][A[A

Evaluating:  28%|       | 9/32 [00:54<02:23,  6.23s/it][A[A

Evaluating:  31%|      | 10/32 [01:00<02:14,  6.10s/it][A[A

Evaluating:  34%|      | 11/32 [01:06<02:06,  6.01s/it][A[A

Evaluating:  38%|      | 12/32 [01:12<01:59,  5.96s/it][A[A

Evaluating:  41%|      | 13/32 [01:18<01:52,  5.91s/it][A[A

Evaluating:  44%|     | 14/32 [01:23<01:45,  5.88s/it][A[A

Evaluating:  47%|     | 15/32 [01:29<01:39,  5.86s/it][A[A

Evaluating:  50%|     | 16/32 [01:35<01:33,  5.85s/it][A[A

Evaluating:  53%|    | 17/32 [01:41<01:27,  5.84s/it][A[A

Evaluating:  56%|    | 18/32 [01:47<01:21,  5.83s/it][A[A

Evaluating:  59%|    | 19/32 [01:53<01:15,  5.82s/it][A[A

Evaluating:  62%|   | 20/32 [01:58<01:09,  5.82s/it][A[A

Evaluating:  66%|   | 21/32 [02:04<01:04,  5.82s/it][A[A

Evaluating:  69%|   | 22/32 [02:10<00:58,  5.82s/it][A[A

Evaluating:  72%|  | 23/32 [02:16<00:52,  5.82s/it][A[A

Evaluating:  75%|  | 24/32 [02:22<00:46,  5.82s/it][A[A

Evaluating:  78%|  | 25/32 [02:27<00:40,  5.81s/it][A[A

Evaluating:  81%| | 26/32 [02:33<00:34,  5.83s/it][A[A

Evaluating:  84%| | 27/32 [02:40<00:30,  6.03s/it][A[A

Evaluating:  88%| | 28/32 [02:46<00:24,  6.13s/it][A[A

Evaluating:  91%| | 29/32 [02:52<00:18,  6.04s/it][A[A

Evaluating:  94%|| 30/32 [02:58<00:11,  5.98s/it][A[A

Evaluating:  97%|| 31/32 [03:04<00:05,  5.93s/it][A[A

Evaluating: 100%|| 32/32 [03:05<00:00,  4.60s/it][A[AEvaluating: 100%|| 32/32 [03:05<00:00,  5.80s/it]
03/11/2022 08:10:13 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 08:10:13 - INFO - __main__ -     acc = 0.951
03/11/2022 08:10:13 - INFO - __main__ -     auc = 0.9811563066361007
03/11/2022 08:10:13 - INFO - __main__ -     f1 = 0.950999950999951
03/11/2022 08:10:13 - INFO - __main__ -     mcc = 0.9023213949421767
03/11/2022 08:10:13 - INFO - __main__ -     precision = 0.951148965451025
03/11/2022 08:10:13 - INFO - __main__ -     recall = 0.9511724297962401
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  13%|        | 18/138 [11:48<2:32:02, 76.02s/it][A
Iteration:  14%|        | 19/138 [12:07<1:56:49, 58.90s/it][A
Iteration:  14%|        | 20/138 [12:26<1:32:08, 46.85s/it][A
Iteration:  15%|        | 21/138 [12:45<1:14:47, 38.35s/it][A
Iteration:  16%|        | 22/138 [13:03<1:02:42, 32.44s/it][A
Iteration:  17%|        | 23/138 [13:22<54:14, 28.30s/it]  [A
Iteration:  17%|        | 24/138 [13:41<48:13, 25.38s/it][A
Iteration:  18%|        | 25/138 [13:59<43:55, 23.32s/it][A
Iteration:  19%|        | 26/138 [14:18<40:51, 21.88s/it][A
Iteration:  20%|        | 27/138 [14:36<38:41, 20.91s/it][A03/11/2022 08:13:19 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 08:13:19 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 08:13:19 - INFO - __main__ -     Num examples = 1000
03/11/2022 08:13:19 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.951, "eval_f1": 0.950999950999951, "eval_mcc": 0.9023213949421767, "eval_auc": 0.9811563066361007, "eval_precision": 0.951148965451025, "eval_recall": 0.9511724297962401, "learning_rate": 3.864734299516908e-05, "loss": 0.07754035919206217, "step": 570}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:05<02:59,  5.79s/it][A[A

Evaluating:   6%|         | 2/32 [00:11<02:54,  5.80s/it][A[A

Evaluating:   9%|         | 3/32 [00:17<02:48,  5.80s/it][A[A

Evaluating:  12%|        | 4/32 [00:23<02:42,  5.80s/it][A[A

Evaluating:  16%|        | 5/32 [00:29<02:41,  5.97s/it][A[A

Evaluating:  19%|        | 6/32 [00:35<02:39,  6.15s/it][A[A

Evaluating:  22%|       | 7/32 [00:42<02:35,  6.21s/it][A[A

Evaluating:  25%|       | 8/32 [00:48<02:30,  6.25s/it][A[A

Evaluating:  28%|       | 9/32 [00:54<02:22,  6.18s/it][A[A

Evaluating:  31%|      | 10/32 [01:00<02:13,  6.07s/it][A[A

Evaluating:  34%|      | 11/32 [01:06<02:06,  6.01s/it][A[A

Evaluating:  38%|      | 12/32 [01:12<01:59,  5.96s/it][A[A

Evaluating:  41%|      | 13/32 [01:18<01:52,  5.92s/it][A[A

Evaluating:  44%|     | 14/32 [01:23<01:45,  5.89s/it][A[A

Evaluating:  47%|     | 15/32 [01:29<01:39,  5.86s/it][A[A

Evaluating:  50%|     | 16/32 [01:35<01:33,  5.85s/it][A[A

Evaluating:  53%|    | 17/32 [01:41<01:27,  5.83s/it][A[A

Evaluating:  56%|    | 18/32 [01:47<01:21,  5.83s/it][A[A

Evaluating:  59%|    | 19/32 [01:52<01:15,  5.81s/it][A[A

Evaluating:  62%|   | 20/32 [01:58<01:09,  5.81s/it][A[A

Evaluating:  66%|   | 21/32 [02:04<01:03,  5.81s/it][A[A

Evaluating:  69%|   | 22/32 [02:10<00:58,  5.81s/it][A[A

Evaluating:  72%|  | 23/32 [02:16<00:52,  5.82s/it][A[A

Evaluating:  75%|  | 24/32 [02:21<00:46,  5.83s/it][A[A

Evaluating:  78%|  | 25/32 [02:27<00:40,  5.84s/it][A[A

Evaluating:  81%| | 26/32 [02:33<00:35,  5.84s/it][A[A

Evaluating:  84%| | 27/32 [02:39<00:29,  5.83s/it][A[A

Evaluating:  88%| | 28/32 [02:45<00:23,  5.82s/it][A[A

Evaluating:  91%| | 29/32 [02:51<00:17,  5.82s/it][A[A

Evaluating:  94%|| 30/32 [02:56<00:11,  5.82s/it][A[A

Evaluating:  97%|| 31/32 [03:02<00:05,  5.82s/it][A[A

Evaluating: 100%|| 32/32 [03:04<00:00,  4.53s/it][A[AEvaluating: 100%|| 32/32 [03:04<00:00,  5.76s/it]
03/11/2022 08:16:24 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 08:16:24 - INFO - __main__ -     acc = 0.948
03/11/2022 08:16:24 - INFO - __main__ -     auc = 0.9804421666646663
03/11/2022 08:16:24 - INFO - __main__ -     f1 = 0.9479648242211736
03/11/2022 08:16:24 - INFO - __main__ -     mcc = 0.8989720093878696
03/11/2022 08:16:24 - INFO - __main__ -     precision = 0.9504240471493828
03/11/2022 08:16:24 - INFO - __main__ -     recall = 0.9485499157834936
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  20%|        | 28/138 [17:59<2:18:26, 75.51s/it][A
Iteration:  21%|        | 29/138 [18:18<1:46:26, 58.59s/it][A
Iteration:  22%|       | 30/138 [18:37<1:23:55, 46.63s/it][A
Iteration:  22%|       | 31/138 [18:56<1:08:11, 38.24s/it][A
Iteration:  23%|       | 32/138 [19:14<57:04, 32.30s/it]  [A
Iteration:  24%|       | 33/138 [19:33<49:19, 28.19s/it][A
Iteration:  25%|       | 34/138 [19:51<43:51, 25.30s/it][A
Iteration:  25%|       | 35/138 [20:10<39:58, 23.29s/it][A
Iteration:  26%|       | 36/138 [20:29<37:12, 21.88s/it][A
Iteration:  27%|       | 37/138 [20:49<36:03, 21.42s/it][A03/11/2022 08:19:32 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 08:19:32 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 08:19:32 - INFO - __main__ -     Num examples = 1000
03/11/2022 08:19:32 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.948, "eval_f1": 0.9479648242211736, "eval_mcc": 0.8989720093878696, "eval_auc": 0.9804421666646663, "eval_precision": 0.9504240471493828, "eval_recall": 0.9485499157834936, "learning_rate": 3.5426731078905e-05, "loss": 0.09792913178098388, "step": 580}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:05<03:00,  5.83s/it][A[A

Evaluating:   6%|         | 2/32 [00:11<02:54,  5.83s/it][A[A

Evaluating:   9%|         | 3/32 [00:17<02:48,  5.82s/it][A[A

Evaluating:  12%|        | 4/32 [00:23<02:42,  5.82s/it][A[A

Evaluating:  16%|        | 5/32 [00:29<02:36,  5.81s/it][A[A

Evaluating:  19%|        | 6/32 [00:34<02:30,  5.81s/it][A[A

Evaluating:  22%|       | 7/32 [00:40<02:25,  5.80s/it][A[A

Evaluating:  25%|       | 8/32 [00:46<02:19,  5.81s/it][A[A

Evaluating:  28%|       | 9/32 [00:52<02:13,  5.81s/it][A[A

Evaluating:  31%|      | 10/32 [00:58<02:07,  5.81s/it][A[A

Evaluating:  34%|      | 11/32 [01:03<02:02,  5.81s/it][A[A

Evaluating:  38%|      | 12/32 [01:09<01:56,  5.82s/it][A[A

Evaluating:  41%|      | 13/32 [01:15<01:50,  5.81s/it][A[A

Evaluating:  44%|     | 14/32 [01:21<01:44,  5.82s/it][A[A

Evaluating:  47%|     | 15/32 [01:27<01:38,  5.82s/it][A[A

Evaluating:  50%|     | 16/32 [01:33<01:33,  5.83s/it][A[A

Evaluating:  53%|    | 17/32 [01:38<01:27,  5.84s/it][A[A

Evaluating:  56%|    | 18/32 [01:44<01:21,  5.83s/it][A[A

Evaluating:  59%|    | 19/32 [01:50<01:15,  5.82s/it][A[A

Evaluating:  62%|   | 20/32 [01:56<01:09,  5.83s/it][A[A

Evaluating:  66%|   | 21/32 [02:02<01:03,  5.82s/it][A[A

Evaluating:  69%|   | 22/32 [02:08<00:58,  5.82s/it][A[A

Evaluating:  72%|  | 23/32 [02:13<00:52,  5.82s/it][A[A

Evaluating:  75%|  | 24/32 [02:19<00:46,  5.83s/it][A[A

Evaluating:  78%|  | 25/32 [02:25<00:40,  5.83s/it][A[A

Evaluating:  81%| | 26/32 [02:31<00:34,  5.82s/it][A[A

Evaluating:  84%| | 27/32 [02:37<00:29,  5.81s/it][A[A

Evaluating:  88%| | 28/32 [02:42<00:23,  5.82s/it][A[A

Evaluating:  91%| | 29/32 [02:48<00:17,  5.83s/it][A[A

Evaluating:  94%|| 30/32 [02:54<00:11,  5.83s/it][A[A

Evaluating:  97%|| 31/32 [03:00<00:05,  5.82s/it][A[A

Evaluating: 100%|| 32/32 [03:01<00:00,  4.53s/it][A[AEvaluating: 100%|| 32/32 [03:01<00:00,  5.69s/it]
03/11/2022 08:22:34 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 08:22:34 - INFO - __main__ -     acc = 0.949
03/11/2022 08:22:34 - INFO - __main__ -     auc = 0.9797520313981541
03/11/2022 08:22:34 - INFO - __main__ -     f1 = 0.9489681050656661
03/11/2022 08:22:34 - INFO - __main__ -     mcc = 0.9008308995000823
03/11/2022 08:22:34 - INFO - __main__ -     precision = 0.9512965105070368
03/11/2022 08:22:34 - INFO - __main__ -     recall = 0.9495361090773792
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  28%|       | 38/138 [24:10<2:05:26, 75.27s/it][A
Iteration:  28%|       | 39/138 [24:29<1:36:23, 58.42s/it][A
Iteration:  29%|       | 40/138 [24:50<1:17:15, 47.30s/it][A
Iteration:  30%|       | 41/138 [25:11<1:03:38, 39.37s/it][A
Iteration:  30%|       | 42/138 [25:30<53:21, 33.35s/it]  [A
Iteration:  31%|       | 43/138 [25:50<46:28, 29.36s/it][A
Iteration:  32%|      | 44/138 [26:11<41:45, 26.65s/it][A
Iteration:  33%|      | 45/138 [26:31<38:27, 24.81s/it][A
Iteration:  33%|      | 46/138 [26:50<35:08, 22.92s/it][A
Iteration:  34%|      | 47/138 [27:08<32:44, 21.58s/it][A03/11/2022 08:25:51 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 08:25:51 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 08:25:51 - INFO - __main__ -     Num examples = 1000
03/11/2022 08:25:51 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.949, "eval_f1": 0.9489681050656661, "eval_mcc": 0.9008308995000823, "eval_auc": 0.9797520313981541, "eval_precision": 0.9512965105070368, "eval_recall": 0.9495361090773792, "learning_rate": 3.22061191626409e-05, "loss": 0.06528661211486905, "step": 590}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:05<02:59,  5.79s/it][A[A

Evaluating:   6%|         | 2/32 [00:11<02:53,  5.80s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<02:59,  6.18s/it][A[A

Evaluating:  12%|        | 4/32 [00:24<02:55,  6.28s/it][A[A

Evaluating:  16%|        | 5/32 [00:30<02:50,  6.30s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:44,  6.32s/it][A[A

Evaluating:  22%|       | 7/32 [00:43<02:34,  6.17s/it][A[A

Evaluating:  25%|       | 8/32 [00:49<02:25,  6.05s/it][A[A

Evaluating:  28%|       | 9/32 [00:54<02:17,  5.98s/it][A[A

Evaluating:  31%|      | 10/32 [01:00<02:10,  5.93s/it][A[A

Evaluating:  34%|      | 11/32 [01:06<02:03,  5.90s/it][A[A

Evaluating:  38%|      | 12/32 [01:12<01:57,  5.88s/it][A[A

Evaluating:  41%|      | 13/32 [01:18<01:51,  5.88s/it][A[A

Evaluating:  44%|     | 14/32 [01:24<01:45,  5.86s/it][A[A

Evaluating:  47%|     | 15/32 [01:29<01:39,  5.85s/it][A[A

Evaluating:  50%|     | 16/32 [01:35<01:33,  5.83s/it][A[A

Evaluating:  53%|    | 17/32 [01:41<01:27,  5.83s/it][A[A

Evaluating:  56%|    | 18/32 [01:47<01:21,  5.84s/it][A[A

Evaluating:  59%|    | 19/32 [01:53<01:15,  5.83s/it][A[A

Evaluating:  62%|   | 20/32 [01:58<01:10,  5.83s/it][A[A

Evaluating:  66%|   | 21/32 [02:04<01:04,  5.83s/it][A[A

Evaluating:  69%|   | 22/32 [02:10<00:58,  5.82s/it][A[A

Evaluating:  72%|  | 23/32 [02:16<00:52,  5.82s/it][A[A

Evaluating:  75%|  | 24/32 [02:22<00:46,  5.81s/it][A[A

Evaluating:  78%|  | 25/32 [02:28<00:40,  5.81s/it][A[A

Evaluating:  81%| | 26/32 [02:33<00:34,  5.82s/it][A[A

Evaluating:  84%| | 27/32 [02:39<00:29,  5.81s/it][A[A

Evaluating:  88%| | 28/32 [02:45<00:23,  5.81s/it][A[A

Evaluating:  91%| | 29/32 [02:51<00:17,  5.81s/it][A[A

Evaluating:  94%|| 30/32 [02:57<00:11,  5.81s/it][A[A

Evaluating:  97%|| 31/32 [03:02<00:05,  5.82s/it][A[A

Evaluating: 100%|| 32/32 [03:04<00:00,  4.53s/it][A[AEvaluating: 100%|| 32/32 [03:04<00:00,  5.76s/it]
03/11/2022 08:28:56 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 08:28:56 - INFO - __main__ -     acc = 0.953
03/11/2022 08:28:56 - INFO - __main__ -     auc = 0.981676408576081
03/11/2022 08:28:56 - INFO - __main__ -     f1 = 0.9529920556574061
03/11/2022 08:28:56 - INFO - __main__ -     mcc = 0.9073750465718298
03/11/2022 08:28:56 - INFO - __main__ -     precision = 0.9540064102564103
03/11/2022 08:28:56 - INFO - __main__ -     recall = 0.9533688602966182
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  35%|      | 48/138 [30:31<1:54:01, 76.02s/it][A
Iteration:  36%|      | 49/138 [30:50<1:27:22, 58.90s/it][A
Iteration:  36%|      | 50/138 [31:09<1:08:38, 46.81s/it][A
Iteration:  37%|      | 51/138 [31:27<55:34, 38.33s/it]  [A
Iteration:  38%|      | 52/138 [31:46<46:32, 32.47s/it][A
Iteration:  38%|      | 53/138 [32:07<40:57, 28.91s/it][A
Iteration:  39%|      | 54/138 [32:26<36:28, 26.06s/it][A
Iteration:  40%|      | 55/138 [32:47<33:45, 24.40s/it][A
Iteration:  41%|      | 56/138 [33:07<31:33, 23.09s/it][A
Iteration:  41%|     | 57/138 [33:27<29:50, 22.11s/it][A03/11/2022 08:32:11 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 08:32:11 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 08:32:11 - INFO - __main__ -     Num examples = 1000
03/11/2022 08:32:11 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.953, "eval_f1": 0.9529920556574061, "eval_mcc": 0.9073750465718298, "eval_auc": 0.981676408576081, "eval_precision": 0.9540064102564103, "eval_recall": 0.9533688602966182, "learning_rate": 2.8985507246376814e-05, "loss": 0.05527186719700694, "step": 600}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:15,  6.31s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:15,  6.50s/it][A[A

Evaluating:   9%|         | 3/32 [00:19<03:08,  6.51s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:59,  6.41s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:51,  6.35s/it][A[A

Evaluating:  19%|        | 6/32 [00:38<02:44,  6.32s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:37,  6.30s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:31,  6.29s/it][A[A

Evaluating:  28%|       | 9/32 [00:57<02:24,  6.30s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:18,  6.29s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:11,  6.28s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.29s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<01:59,  6.28s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.32s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:47,  6.34s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:41,  6.36s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:35,  6.38s/it][A[A

Evaluating:  56%|    | 18/32 [01:54<01:29,  6.40s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:23,  6.42s/it][A[A

Evaluating:  62%|   | 20/32 [02:07<01:18,  6.52s/it][A[A

Evaluating:  66%|   | 21/32 [02:14<01:12,  6.55s/it][A[A

Evaluating:  69%|   | 22/32 [02:20<01:05,  6.51s/it][A[A

Evaluating:  72%|  | 23/32 [02:26<00:57,  6.43s/it][A[A

Evaluating:  75%|  | 24/32 [02:33<00:51,  6.38s/it][A[A

Evaluating:  78%|  | 25/32 [02:39<00:44,  6.34s/it][A[A

Evaluating:  81%| | 26/32 [02:45<00:37,  6.31s/it][A[A

Evaluating:  84%| | 27/32 [02:51<00:31,  6.30s/it][A[A

Evaluating:  88%| | 28/32 [02:58<00:25,  6.29s/it][A[A

Evaluating:  91%| | 29/32 [03:04<00:18,  6.31s/it][A[A

Evaluating:  94%|| 30/32 [03:10<00:12,  6.39s/it][A[A

Evaluating:  97%|| 31/32 [03:17<00:06,  6.40s/it][A[A

Evaluating: 100%|| 32/32 [03:19<00:00,  4.97s/it][A[AEvaluating: 100%|| 32/32 [03:19<00:00,  6.22s/it]
03/11/2022 08:35:30 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 08:35:30 - INFO - __main__ -     acc = 0.954
03/11/2022 08:35:30 - INFO - __main__ -     auc = 0.9800080815839906
03/11/2022 08:35:30 - INFO - __main__ -     f1 = 0.9539688829648842
03/11/2022 08:35:30 - INFO - __main__ -     mcc = 0.9109994098283243
03/11/2022 08:35:30 - INFO - __main__ -     precision = 0.956450297495892
03/11/2022 08:35:30 - INFO - __main__ -     recall = 0.9545510920140348
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  42%|     | 58/138 [37:05<1:48:11, 81.15s/it][A
Iteration:  43%|     | 59/138 [37:26<1:22:53, 62.95s/it][A
Iteration:  43%|     | 60/138 [37:46<1:05:06, 50.08s/it][A
Iteration:  44%|     | 61/138 [38:06<52:44, 41.09s/it]  [A
Iteration:  45%|     | 62/138 [38:26<44:00, 34.74s/it][A
Iteration:  46%|     | 63/138 [38:46<37:51, 30.28s/it][A
Iteration:  46%|     | 64/138 [39:06<33:41, 27.32s/it][A
Iteration:  47%|     | 65/138 [39:26<30:34, 25.13s/it][A
Iteration:  48%|     | 66/138 [39:46<28:12, 23.51s/it][A
Iteration:  49%|     | 67/138 [40:06<26:32, 22.42s/it][A03/11/2022 08:38:50 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 08:38:50 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 08:38:50 - INFO - __main__ -     Num examples = 1000
03/11/2022 08:38:50 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.954, "eval_f1": 0.9539688829648842, "eval_mcc": 0.9109994098283243, "eval_auc": 0.9800080815839906, "eval_precision": 0.956450297495892, "eval_recall": 0.9545510920140348, "learning_rate": 2.576489533011272e-05, "loss": 0.033474739221855995, "step": 610}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:13,  6.23s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:07,  6.27s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.30s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:56,  6.30s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.27s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:45,  6.37s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:40,  6.40s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:32,  6.37s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:25,  6.33s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:18,  6.31s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:12,  6.30s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.29s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:59,  6.27s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:52,  6.26s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:46,  6.26s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.27s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:34,  6.27s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:27,  6.27s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.27s/it][A[A

Evaluating:  62%|   | 20/32 [02:05<01:15,  6.26s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:08,  6.25s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.25s/it][A[A

Evaluating:  72%|  | 23/32 [02:24<00:56,  6.25s/it][A[A

Evaluating:  75%|  | 24/32 [02:30<00:50,  6.26s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:43,  6.28s/it][A[A

Evaluating:  81%| | 26/32 [02:43<00:38,  6.40s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:32,  6.43s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.46s/it][A[A

Evaluating:  91%| | 29/32 [03:03<00:19,  6.46s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.40s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.36s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.94s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.17s/it]
03/11/2022 08:42:08 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 08:42:08 - INFO - __main__ -     acc = 0.948
03/11/2022 08:42:08 - INFO - __main__ -     auc = 0.9769114746490313
03/11/2022 08:42:08 - INFO - __main__ -     f1 = 0.9479466974181563
03/11/2022 08:42:08 - INFO - __main__ -     mcc = 0.8999216661962913
03/11/2022 08:42:08 - INFO - __main__ -     precision = 0.9512916584500098
03/11/2022 08:42:08 - INFO - __main__ -     recall = 0.9486339322507211
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  49%|     | 68/138 [43:43<1:34:23, 80.91s/it][A
Iteration:  50%|     | 69/138 [44:04<1:12:10, 62.76s/it][A
Iteration:  51%|     | 70/138 [44:24<56:35, 49.94s/it]  [A
Iteration:  51%|    | 71/138 [44:44<45:41, 40.92s/it][A
Iteration:  52%|    | 72/138 [45:04<38:05, 34.62s/it][A
Iteration:  53%|    | 73/138 [45:24<32:53, 30.36s/it][A
Iteration:  54%|    | 74/138 [45:44<29:03, 27.24s/it][A
Iteration:  54%|    | 75/138 [46:04<26:22, 25.11s/it][A
Iteration:  55%|    | 76/138 [46:24<24:27, 23.66s/it][A
Iteration:  56%|    | 77/138 [46:44<22:53, 22.51s/it][A03/11/2022 08:45:29 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 08:45:29 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 08:45:29 - INFO - __main__ -     Num examples = 1000
03/11/2022 08:45:29 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.948, "eval_f1": 0.9479466974181563, "eval_mcc": 0.8999216661962913, "eval_auc": 0.9769114746490313, "eval_precision": 0.9512916584500098, "eval_recall": 0.9486339322507211, "learning_rate": 2.2544283413848633e-05, "loss": 0.07621134961955249, "step": 620}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:13,  6.23s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.27s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:01,  6.27s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.27s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:48,  6.26s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:42,  6.25s/it][A[A

Evaluating:  22%|       | 7/32 [00:43<02:36,  6.25s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:29,  6.25s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:23,  6.26s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:18,  6.30s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:14,  6.41s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:08,  6.43s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:02,  6.45s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:55,  6.44s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:48,  6.38s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:41,  6.37s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:35,  6.35s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.32s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:21,  6.30s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.28s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:08,  6.27s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.26s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.26s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.26s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:43,  6.27s/it][A[A

Evaluating:  81%| | 26/32 [02:43<00:37,  6.27s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.26s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.27s/it][A[A

Evaluating:  91%| | 29/32 [03:02<00:18,  6.27s/it][A[A

Evaluating:  94%|| 30/32 [03:09<00:12,  6.30s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.40s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.98s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.17s/it]
03/11/2022 08:48:46 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 08:48:46 - INFO - __main__ -     acc = 0.955
03/11/2022 08:48:46 - INFO - __main__ -     auc = 0.9817204172017715
03/11/2022 08:48:46 - INFO - __main__ -     f1 = 0.9549801462444938
03/11/2022 08:48:46 - INFO - __main__ -     mcc = 0.9123048872874879
03/11/2022 08:48:46 - INFO - __main__ -     precision = 0.9568246019517206
03/11/2022 08:48:46 - INFO - __main__ -     recall = 0.9554812743297687
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  57%|    | 78/138 [50:22<1:21:01, 81.02s/it][A
Iteration:  57%|    | 79/138 [50:43<1:01:59, 63.05s/it][A
Iteration:  58%|    | 80/138 [51:03<48:28, 50.15s/it]  [A
Iteration:  59%|    | 81/138 [51:23<39:04, 41.14s/it][A
Iteration:  59%|    | 82/138 [51:43<32:26, 34.76s/it][A
Iteration:  60%|    | 83/138 [52:03<27:46, 30.31s/it][A
Iteration:  61%|    | 84/138 [52:23<24:36, 27.35s/it][A
Iteration:  62%|   | 85/138 [52:44<22:24, 25.37s/it][A
Iteration:  62%|   | 86/138 [53:04<20:34, 23.74s/it][A
Iteration:  63%|   | 87/138 [53:24<19:12, 22.60s/it][A03/11/2022 08:52:08 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 08:52:09 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 08:52:09 - INFO - __main__ -     Num examples = 1000
03/11/2022 08:52:09 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.955, "eval_f1": 0.9549801462444938, "eval_mcc": 0.9123048872874879, "eval_auc": 0.9817204172017715, "eval_precision": 0.9568246019517206, "eval_recall": 0.9554812743297687, "learning_rate": 1.932367149758454e-05, "loss": 0.03553010243922472, "step": 630}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:14,  6.27s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:07,  6.26s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:01,  6.26s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.25s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:50,  6.30s/it][A[A

Evaluating:  19%|        | 6/32 [00:38<02:46,  6.41s/it][A[A

Evaluating:  22%|       | 7/32 [00:44<02:40,  6.43s/it][A[A

Evaluating:  25%|       | 8/32 [00:51<02:35,  6.47s/it][A[A

Evaluating:  28%|       | 9/32 [00:57<02:28,  6.46s/it][A[A

Evaluating:  31%|      | 10/32 [01:03<02:20,  6.39s/it][A[A

Evaluating:  34%|      | 11/32 [01:09<02:13,  6.35s/it][A[A

Evaluating:  38%|      | 12/32 [01:16<02:06,  6.33s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<01:59,  6.31s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:53,  6.29s/it][A[A

Evaluating:  47%|     | 15/32 [01:35<01:46,  6.28s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:40,  6.28s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:34,  6.28s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:27,  6.27s/it][A[A

Evaluating:  59%|    | 19/32 [02:00<01:21,  6.25s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.25s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:08,  6.24s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.25s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.25s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.27s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:43,  6.27s/it][A[A

Evaluating:  81%| | 26/32 [02:43<00:37,  6.28s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.27s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.27s/it][A[A

Evaluating:  91%| | 29/32 [03:02<00:18,  6.27s/it][A[A

Evaluating:  94%|| 30/32 [03:08<00:12,  6.26s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.26s/it][A[A

Evaluating: 100%|| 32/32 [03:16<00:00,  4.87s/it][A[AEvaluating: 100%|| 32/32 [03:16<00:00,  6.15s/it]
03/11/2022 08:55:25 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 08:55:25 - INFO - __main__ -     acc = 0.958
03/11/2022 08:55:25 - INFO - __main__ -     auc = 0.9829226528399566
03/11/2022 08:55:25 - INFO - __main__ -     f1 = 0.957995799579958
03/11/2022 08:55:25 - INFO - __main__ -     mcc = 0.9170960626592976
03/11/2022 08:55:25 - INFO - __main__ -     precision = 0.9587683361966433
03/11/2022 08:55:25 - INFO - __main__ -     recall = 0.9583278322551221
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  64%|   | 88/138 [57:01<1:07:27, 80.95s/it][A
Iteration:  64%|   | 89/138 [57:22<51:17, 62.80s/it]  [A
Iteration:  65%|   | 90/138 [57:43<40:14, 50.31s/it][A
Iteration:  66%|   | 91/138 [58:03<32:20, 41.29s/it][A
Iteration:  67%|   | 92/138 [58:23<26:42, 34.84s/it][A
Iteration:  67%|   | 93/138 [58:43<22:46, 30.36s/it][A
Iteration:  68%|   | 94/138 [59:03<19:59, 27.27s/it][A
Iteration:  69%|   | 95/138 [59:23<17:58, 25.09s/it][A
Iteration:  70%|   | 96/138 [59:43<16:30, 23.58s/it][A
Iteration:  70%|   | 97/138 [1:00:03<15:22, 22.49s/it][A03/11/2022 08:58:47 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 08:58:47 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 08:58:47 - INFO - __main__ -     Num examples = 1000
03/11/2022 08:58:47 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.958, "eval_f1": 0.957995799579958, "eval_mcc": 0.9170960626592976, "eval_auc": 0.9829226528399566, "eval_precision": 0.9587683361966433, "eval_recall": 0.9583278322551221, "learning_rate": 1.610305958132045e-05, "loss": 0.05076543381437659, "step": 640}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:13,  6.24s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:07,  6.25s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:01,  6.25s/it][A[A

Evaluating:  12%|        | 4/32 [00:24<02:54,  6.25s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:48,  6.24s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:42,  6.24s/it][A[A

Evaluating:  22%|       | 7/32 [00:43<02:36,  6.26s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.26s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:23,  6.26s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:17,  6.25s/it][A[A

Evaluating:  34%|      | 11/32 [01:08<02:12,  6.31s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:08,  6.41s/it][A[A

Evaluating:  41%|      | 13/32 [01:22<02:02,  6.44s/it][A[A

Evaluating:  44%|     | 14/32 [01:28<01:56,  6.45s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:49,  6.44s/it][A[A

Evaluating:  50%|     | 16/32 [01:41<01:42,  6.39s/it][A[A

Evaluating:  53%|    | 17/32 [01:47<01:35,  6.35s/it][A[A

Evaluating:  56%|    | 18/32 [01:53<01:28,  6.32s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.29s/it][A[A

Evaluating:  62%|   | 20/32 [02:06<01:15,  6.28s/it][A[A

Evaluating:  66%|   | 21/32 [02:12<01:08,  6.27s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.26s/it][A[A

Evaluating:  72%|  | 23/32 [02:25<00:56,  6.28s/it][A[A

Evaluating:  75%|  | 24/32 [02:31<00:50,  6.27s/it][A[A

Evaluating:  78%|  | 25/32 [02:37<00:43,  6.27s/it][A[A

Evaluating:  81%| | 26/32 [02:43<00:37,  6.27s/it][A[A

Evaluating:  84%| | 27/32 [02:50<00:31,  6.27s/it][A[A

Evaluating:  88%| | 28/32 [02:56<00:25,  6.27s/it][A[A

Evaluating:  91%| | 29/32 [03:02<00:18,  6.26s/it][A[A

Evaluating:  94%|| 30/32 [03:08<00:12,  6.27s/it][A[A

Evaluating:  97%|| 31/32 [03:15<00:06,  6.35s/it][A[A

Evaluating: 100%|| 32/32 [03:17<00:00,  4.96s/it][A[AEvaluating: 100%|| 32/32 [03:17<00:00,  6.16s/it]
03/11/2022 09:02:04 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 09:02:04 - INFO - __main__ -     acc = 0.958
03/11/2022 09:02:04 - INFO - __main__ -     auc = 0.983060679893259
03/11/2022 09:02:04 - INFO - __main__ -     f1 = 0.9579917663862116
03/11/2022 09:02:04 - INFO - __main__ -     mcc = 0.9174874199177379
03/11/2022 09:02:04 - INFO - __main__ -     precision = 0.9591038592076423
03/11/2022 09:02:04 - INFO - __main__ -     recall = 0.9583838432332736
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  71%|   | 98/138 [1:03:40<53:57, 80.93s/it][A
Iteration:  72%|  | 99/138 [1:04:01<40:57, 63.01s/it][A
Iteration:  72%|  | 100/138 [1:04:21<31:44, 50.11s/it][A
Iteration:  73%|  | 101/138 [1:04:41<25:20, 41.10s/it][A
Iteration:  74%|  | 102/138 [1:05:01<20:51, 34.76s/it][A
Iteration:  75%|  | 103/138 [1:05:21<17:41, 30.33s/it][A
Iteration:  75%|  | 104/138 [1:05:42<15:31, 27.40s/it][A
Iteration:  76%|  | 105/138 [1:06:02<13:53, 25.26s/it][A
Iteration:  77%|  | 106/138 [1:06:22<12:35, 23.62s/it][A
Iteration:  78%|  | 107/138 [1:06:42<11:40, 22.59s/it][A03/11/2022 09:05:27 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 09:05:27 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 09:05:27 - INFO - __main__ -     Num examples = 1000
03/11/2022 09:05:27 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.958, "eval_f1": 0.9579917663862116, "eval_mcc": 0.9174874199177379, "eval_auc": 0.983060679893259, "eval_precision": 0.9591038592076423, "eval_recall": 0.9583838432332736, "learning_rate": 1.288244766505636e-05, "loss": 0.03809525412507355, "step": 650}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:15,  6.31s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:08,  6.28s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<03:02,  6.28s/it][A[A

Evaluating:  12%|        | 4/32 [00:25<02:55,  6.28s/it][A[A

Evaluating:  16%|        | 5/32 [00:31<02:49,  6.27s/it][A[A

Evaluating:  19%|        | 6/32 [00:37<02:43,  6.28s/it][A[A

Evaluating:  22%|       | 7/32 [00:43<02:36,  6.28s/it][A[A

Evaluating:  25%|       | 8/32 [00:50<02:30,  6.27s/it][A[A

Evaluating:  28%|       | 9/32 [00:56<02:24,  6.27s/it][A[A

Evaluating:  31%|      | 10/32 [01:02<02:17,  6.26s/it][A[A

Evaluating:  34%|      | 11/32 [01:08<02:11,  6.26s/it][A[A

Evaluating:  38%|      | 12/32 [01:15<02:05,  6.26s/it][A[A

Evaluating:  41%|      | 13/32 [01:21<01:58,  6.26s/it][A[A

Evaluating:  44%|     | 14/32 [01:27<01:52,  6.27s/it][A[A

Evaluating:  47%|     | 15/32 [01:34<01:46,  6.27s/it][A[A

Evaluating:  50%|     | 16/32 [01:40<01:40,  6.28s/it][A[A

Evaluating:  53%|    | 17/32 [01:46<01:34,  6.29s/it][A[A

Evaluating:  56%|    | 18/32 [01:52<01:28,  6.29s/it][A[A

Evaluating:  59%|    | 19/32 [01:59<01:21,  6.27s/it][A[A

Evaluating:  62%|   | 20/32 [02:05<01:15,  6.27s/it][A[A

Evaluating:  66%|   | 21/32 [02:11<01:08,  6.26s/it][A[A

Evaluating:  69%|   | 22/32 [02:18<01:02,  6.28s/it][A[A

Evaluating:  72%|  | 23/32 [02:24<00:56,  6.27s/it][A[A

Evaluating:  75%|  | 24/32 [02:30<00:50,  6.27s/it][A[A

Evaluating:  78%|  | 25/32 [02:36<00:43,  6.27s/it][A[A

Evaluating:  81%| | 26/32 [02:43<00:37,  6.26s/it][A[A

Evaluating:  84%| | 27/32 [02:49<00:31,  6.26s/it][A[A

Evaluating:  88%| | 28/32 [02:55<00:25,  6.26s/it][A[A

Evaluating:  91%| | 29/32 [03:01<00:18,  6.28s/it][A[A

Evaluating:  94%|| 30/32 [03:08<00:12,  6.28s/it][A[A

Evaluating:  97%|| 31/32 [03:14<00:06,  6.28s/it][A[A

Evaluating: 100%|| 32/32 [03:16<00:00,  4.88s/it][A[AEvaluating: 100%|| 32/32 [03:16<00:00,  6.13s/it]
03/11/2022 09:08:43 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 09:08:43 - INFO - __main__ -     acc = 0.957
03/11/2022 09:08:43 - INFO - __main__ -     auc = 0.9830206720517223
03/11/2022 09:08:43 - INFO - __main__ -     f1 = 0.9569903228226351
03/11/2022 09:08:43 - INFO - __main__ -     mcc = 0.9155923683404179
03/11/2022 09:08:43 - INFO - __main__ -     precision = 0.9581950656470928
03/11/2022 09:08:43 - INFO - __main__ -     recall = 0.9573976499393881
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  78%|  | 108/138 [1:10:19<40:26, 80.88s/it][A
Iteration:  79%|  | 109/138 [1:10:39<30:19, 62.75s/it][A
Iteration:  80%|  | 110/138 [1:10:59<23:18, 49.94s/it][A
Iteration:  80%|  | 111/138 [1:11:19<18:26, 40.98s/it][A
Iteration:  81%|  | 112/138 [1:11:39<15:00, 34.64s/it][A
Iteration:  82%| | 113/138 [1:11:59<12:36, 30.26s/it][A
Iteration:  83%| | 114/138 [1:12:19<10:50, 27.12s/it][A
Iteration:  83%| | 115/138 [1:12:38<09:23, 24.52s/it][A
Iteration:  84%| | 116/138 [1:12:57<08:28, 23.12s/it][A
Iteration:  85%| | 117/138 [1:13:17<07:44, 22.12s/it][A03/11/2022 09:12:00 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 09:12:00 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 09:12:00 - INFO - __main__ -     Num examples = 1000
03/11/2022 09:12:00 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.957, "eval_f1": 0.9569903228226351, "eval_mcc": 0.9155923683404179, "eval_auc": 0.9830206720517223, "eval_precision": 0.9581950656470928, "eval_recall": 0.9573976499393881, "learning_rate": 9.66183574879227e-06, "loss": 0.0925903080496937, "step": 660}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:05<03:00,  5.84s/it][A[A

Evaluating:   6%|         | 2/32 [00:11<02:55,  5.86s/it][A[A

Evaluating:   9%|         | 3/32 [00:17<02:49,  5.84s/it][A[A

Evaluating:  12%|        | 4/32 [00:23<02:43,  5.84s/it][A[A

Evaluating:  16%|        | 5/32 [00:29<02:37,  5.84s/it][A[A

Evaluating:  19%|        | 6/32 [00:35<02:31,  5.83s/it][A[A

Evaluating:  22%|       | 7/32 [00:40<02:25,  5.81s/it][A[A

Evaluating:  25%|       | 8/32 [00:46<02:19,  5.79s/it][A[A

Evaluating:  28%|       | 9/32 [00:52<02:13,  5.79s/it][A[A

Evaluating:  31%|      | 10/32 [00:58<02:07,  5.80s/it][A[A

Evaluating:  34%|      | 11/32 [01:03<02:01,  5.81s/it][A[A

Evaluating:  38%|      | 12/32 [01:09<01:56,  5.81s/it][A[A

Evaluating:  41%|      | 13/32 [01:15<01:50,  5.80s/it][A[A

Evaluating:  44%|     | 14/32 [01:21<01:47,  5.98s/it][A[A

Evaluating:  47%|     | 15/32 [01:28<01:43,  6.10s/it][A[A

Evaluating:  50%|     | 16/32 [01:34<01:37,  6.07s/it][A[A

Evaluating:  53%|    | 17/32 [01:40<01:30,  6.01s/it][A[A

Evaluating:  56%|    | 18/32 [01:46<01:23,  5.97s/it][A[A

Evaluating:  59%|    | 19/32 [01:51<01:17,  5.93s/it][A[A

Evaluating:  62%|   | 20/32 [01:57<01:10,  5.91s/it][A[A

Evaluating:  66%|   | 21/32 [02:03<01:04,  5.88s/it][A[A

Evaluating:  69%|   | 22/32 [02:09<00:58,  5.87s/it][A[A

Evaluating:  72%|  | 23/32 [02:15<00:52,  5.86s/it][A[A

Evaluating:  75%|  | 24/32 [02:21<00:46,  5.85s/it][A[A

Evaluating:  78%|  | 25/32 [02:26<00:40,  5.84s/it][A[A

Evaluating:  81%| | 26/32 [02:32<00:35,  5.83s/it][A[A

Evaluating:  84%| | 27/32 [02:38<00:29,  5.82s/it][A[A

Evaluating:  88%| | 28/32 [02:44<00:23,  5.81s/it][A[A

Evaluating:  91%| | 29/32 [02:50<00:17,  5.82s/it][A[A

Evaluating:  94%|| 30/32 [02:56<00:11,  5.83s/it][A[A

Evaluating:  97%|| 31/32 [03:01<00:05,  5.83s/it][A[A

Evaluating: 100%|| 32/32 [03:03<00:00,  4.54s/it][A[AEvaluating: 100%|| 32/32 [03:03<00:00,  5.73s/it]
03/11/2022 09:15:03 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 09:15:03 - INFO - __main__ -     acc = 0.956
03/11/2022 09:15:03 - INFO - __main__ -     auc = 0.9832167104752533
03/11/2022 09:15:03 - INFO - __main__ -     f1 = 0.955995599559956
03/11/2022 09:15:03 - INFO - __main__ -     mcc = 0.9130933563684211
03/11/2022 09:15:03 - INFO - __main__ -     precision = 0.9567660215208782
03/11/2022 09:15:03 - INFO - __main__ -     recall = 0.9563274401782749
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  86%| | 118/138 [1:16:39<25:21, 76.06s/it][A
Iteration:  86%| | 119/138 [1:16:58<18:40, 58.96s/it][A
Iteration:  87%| | 120/138 [1:17:17<14:03, 46.88s/it][A
Iteration:  88%| | 121/138 [1:17:36<10:53, 38.45s/it][A
Iteration:  88%| | 122/138 [1:17:54<08:40, 32.53s/it][A
Iteration:  89%| | 123/138 [1:18:13<07:05, 28.34s/it][A
Iteration:  90%| | 124/138 [1:18:31<05:55, 25.39s/it][A
Iteration:  91%| | 125/138 [1:18:50<05:03, 23.38s/it][A
Iteration:  91%|| 126/138 [1:19:09<04:23, 21.99s/it][A
Iteration:  92%|| 127/138 [1:19:28<03:51, 21.01s/it][A03/11/2022 09:18:11 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 09:18:11 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 09:18:11 - INFO - __main__ -     Num examples = 1000
03/11/2022 09:18:11 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.956, "eval_f1": 0.955995599559956, "eval_mcc": 0.9130933563684211, "eval_auc": 0.9832167104752533, "eval_precision": 0.9567660215208782, "eval_recall": 0.9563274401782749, "learning_rate": 6.44122383252818e-06, "loss": 0.033603790868073705, "step": 670}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:05<02:59,  5.81s/it][A[A

Evaluating:   6%|         | 2/32 [00:11<02:54,  5.81s/it][A[A

Evaluating:   9%|         | 3/32 [00:18<02:57,  6.12s/it][A[A

Evaluating:  12%|        | 4/32 [00:24<02:55,  6.26s/it][A[A

Evaluating:  16%|        | 5/32 [00:30<02:47,  6.22s/it][A[A

Evaluating:  19%|        | 6/32 [00:36<02:38,  6.09s/it][A[A

Evaluating:  22%|       | 7/32 [00:42<02:30,  6.00s/it][A[A

Evaluating:  25%|       | 8/32 [00:48<02:22,  5.94s/it][A[A

Evaluating:  28%|       | 9/32 [00:54<02:15,  5.90s/it][A[A

Evaluating:  31%|      | 10/32 [00:59<02:09,  5.87s/it][A[A

Evaluating:  34%|      | 11/32 [01:05<02:02,  5.85s/it][A[A

Evaluating:  38%|      | 12/32 [01:11<01:56,  5.84s/it][A[A

Evaluating:  41%|      | 13/32 [01:17<01:50,  5.83s/it][A[A

Evaluating:  44%|     | 14/32 [01:23<01:44,  5.83s/it][A[A

Evaluating:  47%|     | 15/32 [01:28<01:38,  5.82s/it][A[A

Evaluating:  50%|     | 16/32 [01:34<01:33,  5.81s/it][A[A

Evaluating:  53%|    | 17/32 [01:40<01:27,  5.82s/it][A[A

Evaluating:  56%|    | 18/32 [01:46<01:21,  5.83s/it][A[A

Evaluating:  59%|    | 19/32 [01:52<01:15,  5.82s/it][A[A

Evaluating:  62%|   | 20/32 [01:57<01:09,  5.82s/it][A[A

Evaluating:  66%|   | 21/32 [02:03<01:03,  5.82s/it][A[A

Evaluating:  69%|   | 22/32 [02:09<00:58,  5.81s/it][A[A

Evaluating:  72%|  | 23/32 [02:15<00:52,  5.82s/it][A[A

Evaluating:  75%|  | 24/32 [02:21<00:47,  5.90s/it][A[A

Evaluating:  78%|  | 25/32 [02:28<00:42,  6.10s/it][A[A

Evaluating:  81%| | 26/32 [02:34<00:37,  6.19s/it][A[A

Evaluating:  84%| | 27/32 [02:40<00:31,  6.25s/it][A[A

Evaluating:  88%| | 28/32 [02:47<00:24,  6.23s/it][A[A

Evaluating:  91%| | 29/32 [02:52<00:18,  6.10s/it][A[A

Evaluating:  94%|| 30/32 [02:58<00:12,  6.02s/it][A[A

Evaluating:  97%|| 31/32 [03:04<00:05,  5.96s/it][A[A

Evaluating: 100%|| 32/32 [03:06<00:00,  4.63s/it][A[AEvaluating: 100%|| 32/32 [03:06<00:00,  5.81s/it]
03/11/2022 09:21:17 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 09:21:17 - INFO - __main__ -     acc = 0.955
03/11/2022 09:21:17 - INFO - __main__ -     auc = 0.9831046885189498
03/11/2022 09:21:17 - INFO - __main__ -     f1 = 0.9549945543410753
03/11/2022 09:21:17 - INFO - __main__ -     mcc = 0.9111838810947571
03/11/2022 09:21:17 - INFO - __main__ -     precision = 0.9558427722328138
03/11/2022 09:21:17 - INFO - __main__ -     recall = 0.9553412468843894
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration:  93%|| 128/138 [1:22:52<12:41, 76.12s/it][A
Iteration:  93%|| 129/138 [1:23:11<08:51, 59.03s/it][A
Iteration:  94%|| 130/138 [1:23:30<06:15, 46.89s/it][A
Iteration:  95%|| 131/138 [1:23:49<04:29, 38.46s/it][A
Iteration:  96%|| 132/138 [1:24:07<03:15, 32.51s/it][A
Iteration:  96%|| 133/138 [1:24:28<02:24, 28.94s/it][A
Iteration:  97%|| 134/138 [1:24:48<01:44, 26.24s/it][A
Iteration:  98%|| 135/138 [1:25:08<01:13, 24.38s/it][A
Iteration:  99%|| 136/138 [1:25:28<00:46, 23.10s/it][A
Iteration:  99%|| 137/138 [1:25:48<00:22, 22.17s/it][A03/11/2022 09:24:18 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 09:24:18 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 09:24:18 - INFO - __main__ -     Num examples = 1000
03/11/2022 09:24:18 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.955, "eval_f1": 0.9549945543410753, "eval_mcc": 0.9111838810947571, "eval_auc": 0.9831046885189498, "eval_precision": 0.9558427722328138, "eval_recall": 0.9553412468843894, "learning_rate": 3.22061191626409e-06, "loss": 0.04741672025993467, "step": 680}


Evaluating:   0%|          | 0/32 [00:00<?, ?it/s][A[A

Evaluating:   3%|         | 1/32 [00:06<03:13,  6.25s/it][A[A

Evaluating:   6%|         | 2/32 [00:12<03:13,  6.43s/it][A[A

Evaluating:   9%|         | 3/32 [00:19<03:08,  6.50s/it][A[A

Evaluating:  12%|        | 4/32 [00:26<03:03,  6.57s/it][A[A

Evaluating:  16%|        | 5/32 [00:32<02:56,  6.52s/it][A[A

Evaluating:  19%|        | 6/32 [00:39<02:49,  6.52s/it][A[A

Evaluating:  22%|       | 7/32 [00:45<02:42,  6.48s/it][A[A

Evaluating:  25%|       | 8/32 [00:51<02:35,  6.47s/it][A[A

Evaluating:  28%|       | 9/32 [00:58<02:28,  6.45s/it][A[A

Evaluating:  31%|      | 10/32 [01:04<02:21,  6.44s/it][A[A

Evaluating:  34%|      | 11/32 [01:11<02:15,  6.43s/it][A[A

Evaluating:  38%|      | 12/32 [01:17<02:08,  6.43s/it][A[A

Evaluating:  41%|      | 13/32 [01:23<02:01,  6.41s/it][A[A

Evaluating:  44%|     | 14/32 [01:30<01:55,  6.41s/it][A[A

Evaluating:  47%|     | 15/32 [01:36<01:48,  6.41s/it][A[A

Evaluating:  50%|     | 16/32 [01:43<01:42,  6.41s/it][A[A

Evaluating:  53%|    | 17/32 [01:49<01:36,  6.41s/it][A[A

Evaluating:  56%|    | 18/32 [01:55<01:29,  6.39s/it][A[A

Evaluating:  59%|    | 19/32 [02:02<01:22,  6.35s/it][A[A

Evaluating:  62%|   | 20/32 [02:08<01:16,  6.33s/it][A[A

Evaluating:  66%|   | 21/32 [02:14<01:09,  6.32s/it][A[A

Evaluating:  69%|   | 22/32 [02:20<01:02,  6.30s/it][A[A

Evaluating:  72%|  | 23/32 [02:27<00:56,  6.29s/it][A[A

Evaluating:  75%|  | 24/32 [02:33<00:50,  6.30s/it][A[A

Evaluating:  78%|  | 25/32 [02:39<00:44,  6.29s/it][A[A

Evaluating:  81%| | 26/32 [02:46<00:37,  6.29s/it][A[A

Evaluating:  84%| | 27/32 [02:52<00:31,  6.29s/it][A[A

Evaluating:  88%| | 28/32 [02:58<00:25,  6.28s/it][A[A

Evaluating:  91%| | 29/32 [03:04<00:18,  6.27s/it][A[A

Evaluating:  94%|| 30/32 [03:11<00:12,  6.27s/it][A[A

Evaluating:  97%|| 31/32 [03:17<00:06,  6.27s/it][A[A

Evaluating: 100%|| 32/32 [03:19<00:00,  4.88s/it][A[AEvaluating: 100%|| 32/32 [03:19<00:00,  6.22s/it]
03/11/2022 09:27:37 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 09:27:37 - INFO - __main__ -     acc = 0.956
03/11/2022 09:27:37 - INFO - __main__ -     auc = 0.9831046885189497
03/11/2022 09:27:37 - INFO - __main__ -     f1 = 0.9559936630874846
03/11/2022 09:27:37 - INFO - __main__ -     mcc = 0.9132810684269117
03/11/2022 09:27:37 - INFO - __main__ -     precision = 0.9569258008564367
03/11/2022 09:27:37 - INFO - __main__ -     recall = 0.9563554456673509
/home/mexposit/miniconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "

Iteration: 100%|| 138/138 [1:29:13<00:00, 76.88s/it][AIteration: 100%|| 138/138 [1:29:13<00:00, 38.79s/it]
Epoch: 100%|| 5/5 [7:32:38<00:00, 5403.35s/it]  Epoch: 100%|| 5/5 [7:32:38<00:00, 5431.62s/it]
03/11/2022 09:27:37 - INFO - __main__ -    global_step = 690, average loss = 0.2021088479178708
03/11/2022 09:27:37 - INFO - __main__ -   Saving model checkpoint to /home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand
03/11/2022 09:27:37 - INFO - transformers.configuration_utils -   Configuration saved in /home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand/config.json
03/11/2022 09:27:38 - INFO - transformers.modeling_utils -   Model weights saved in /home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand/pytorch_model.bin
03/11/2022 09:27:38 - INFO - transformers.configuration_utils -   loading configuration file /home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand/config.json
03/11/2022 09:27:38 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "do_sample": false,
  "eos_token_ids": 0,
  "finetuning_task": "dnaprom",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "num_return_sequences": 1,
  "num_rnn_layer": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "rnn": "lstm",
  "rnn_dropout": 0.0,
  "rnn_hidden": 768,
  "split": 0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 4101
}

03/11/2022 09:27:38 - INFO - transformers.modeling_utils -   loading weights file /home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand/pytorch_model.bin
03/11/2022 09:27:40 - INFO - transformers.tokenization_utils -   Model name '/home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand' not found in model shortcut name list (dna3, dna4, dna5, dna6). Assuming '/home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand' is a path, a model identifier, or url to a directory containing tokenizer files.
03/11/2022 09:27:40 - INFO - transformers.tokenization_utils -   Didn't find file /home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand/added_tokens.json. We won't load it.
03/11/2022 09:27:40 - INFO - transformers.tokenization_utils -   loading file /home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand/vocab.txt
03/11/2022 09:27:40 - INFO - transformers.tokenization_utils -   loading file None
03/11/2022 09:27:40 - INFO - transformers.tokenization_utils -   loading file /home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand/special_tokens_map.json
03/11/2022 09:27:40 - INFO - transformers.tokenization_utils -   loading file /home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand/tokenizer_config.json
03/11/2022 09:27:40 - INFO - transformers.tokenization_utils -   Model name '/home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand' not found in model shortcut name list (dna3, dna4, dna5, dna6). Assuming '/home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand' is a path, a model identifier, or url to a directory containing tokenizer files.
03/11/2022 09:27:40 - INFO - transformers.tokenization_utils -   Didn't find file /home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand/added_tokens.json. We won't load it.
03/11/2022 09:27:40 - INFO - transformers.tokenization_utils -   loading file /home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand/vocab.txt
03/11/2022 09:27:40 - INFO - transformers.tokenization_utils -   loading file None
03/11/2022 09:27:40 - INFO - transformers.tokenization_utils -   loading file /home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand/special_tokens_map.json
03/11/2022 09:27:40 - INFO - transformers.tokenization_utils -   loading file /home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand/tokenizer_config.json
03/11/2022 09:27:40 - INFO - __main__ -   Evaluate the following checkpoints: ['/home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand']
03/11/2022 09:27:40 - INFO - transformers.configuration_utils -   loading configuration file /home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand/config.json
03/11/2022 09:27:40 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "do_sample": false,
  "eos_token_ids": 0,
  "finetuning_task": "dnaprom",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "num_return_sequences": 1,
  "num_rnn_layer": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "rnn": "lstm",
  "rnn_dropout": 0.0,
  "rnn_hidden": 768,
  "split": 0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 4101
}

03/11/2022 09:27:40 - INFO - transformers.modeling_utils -   loading weights file /home/mexposit/cg/gea/transformers/2_geainit/model/ft_2binrand/pytorch_model.bin
03/11/2022 09:27:42 - INFO - __main__ -   Loading features from cached file /home/mexposit/cg/gea/transformers/2_geainit/in_data/cached_dev_6-new-12w-0_100_dnaprom
03/11/2022 09:27:42 - INFO - __main__ -   ***** Running evaluation  *****
03/11/2022 09:27:42 - INFO - __main__ -     Num examples = 1000
03/11/2022 09:27:42 - INFO - __main__ -     Batch size = 32
{"eval_acc": 0.956, "eval_f1": 0.9559936630874846, "eval_mcc": 0.9132810684269117, "eval_auc": 0.9831046885189497, "eval_precision": 0.9569258008564367, "eval_recall": 0.9563554456673509, "learning_rate": 0.0, "loss": 0.06764436480589212, "step": 690}
============================================================
<class 'transformers.tokenization_dna.DNATokenizer'>
============================================================
<class 'transformers.tokenization_dna.DNATokenizer'>
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|         | 1/32 [00:06<03:24,  6.60s/it]Evaluating:   6%|         | 2/32 [00:13<03:17,  6.58s/it]Evaluating:   9%|         | 3/32 [00:19<03:10,  6.56s/it]Evaluating:  12%|        | 4/32 [00:26<03:02,  6.52s/it]Evaluating:  16%|        | 5/32 [00:32<02:53,  6.42s/it]Evaluating:  19%|        | 6/32 [00:38<02:45,  6.36s/it]Evaluating:  22%|       | 7/32 [00:44<02:38,  6.32s/it]Evaluating:  25%|       | 8/32 [00:51<02:31,  6.30s/it]Evaluating:  28%|       | 9/32 [00:57<02:24,  6.30s/it]Evaluating:  31%|      | 10/32 [01:03<02:18,  6.29s/it]Evaluating:  34%|      | 11/32 [01:09<02:11,  6.28s/it]Evaluating:  38%|      | 12/32 [01:16<02:05,  6.28s/it]Evaluating:  41%|      | 13/32 [01:22<01:59,  6.27s/it]Evaluating:  44%|     | 14/32 [01:28<01:52,  6.27s/it]Evaluating:  47%|     | 15/32 [01:35<01:46,  6.26s/it]Evaluating:  50%|     | 16/32 [01:41<01:40,  6.27s/it]Evaluating:  53%|    | 17/32 [01:47<01:34,  6.28s/it]Evaluating:  56%|    | 18/32 [01:53<01:27,  6.27s/it]Evaluating:  59%|    | 19/32 [02:00<01:22,  6.35s/it]Evaluating:  62%|   | 20/32 [02:06<01:17,  6.42s/it]Evaluating:  66%|   | 21/32 [02:13<01:10,  6.43s/it]Evaluating:  69%|   | 22/32 [02:19<01:04,  6.45s/it]Evaluating:  72%|  | 23/32 [02:26<00:57,  6.42s/it]Evaluating:  75%|  | 24/32 [02:32<00:51,  6.39s/it]Evaluating:  78%|  | 25/32 [02:38<00:44,  6.35s/it]Evaluating:  81%| | 26/32 [02:45<00:37,  6.33s/it]Evaluating:  84%| | 27/32 [02:51<00:31,  6.31s/it]Evaluating:  88%| | 28/32 [02:57<00:25,  6.29s/it]Evaluating:  91%| | 29/32 [03:03<00:18,  6.28s/it]Evaluating:  94%|| 30/32 [03:10<00:12,  6.28s/it]Evaluating:  97%|| 31/32 [03:16<00:06,  6.28s/it]Evaluating: 100%|| 32/32 [03:18<00:00,  4.89s/it]Evaluating: 100%|| 32/32 [03:18<00:00,  6.19s/it]
03/11/2022 09:31:00 - INFO - __main__ -   ***** Eval results  *****
03/11/2022 09:31:00 - INFO - __main__ -     acc = 0.956
03/11/2022 09:31:00 - INFO - __main__ -     auc = 0.9831046885189497
03/11/2022 09:31:00 - INFO - __main__ -     f1 = 0.9559936630874846
03/11/2022 09:31:00 - INFO - __main__ -     mcc = 0.9132810684269117
03/11/2022 09:31:00 - INFO - __main__ -     precision = 0.9569258008564367
03/11/2022 09:31:00 - INFO - __main__ -     recall = 0.9563554456673509
